<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[以匠人之心，Java 11 正式发布]]></title>
    <url>%2Farchives%2F7c18665b.html</url>
    <content type="text"><![CDATA[背景Java 11 已于 2018 年 9 月 25 日正式发布，Java 开发团队为了加快的版本迭代、跟进社区反馈，Java 的版本发布周期调整为每六个月一次——即每半年发布一个大版本，每个季度发布一个中间特性版本，并且做出不会跳票的承诺。通过这样的方式，Java 开发团队能够将一些重要特性尽早的合并到 Java Release 版本中，以便快速得到开发者的反馈，避免出现类似 Java 9 发布时的两次延期的情况。 本文主要针对 Java 9 - Java 11 中的新特性展开介绍，让您快速了解 Java 9 - Java 11 带来的变化。 特性介绍按照官方介绍，新的版本发布周期将会严格按照时间节点，于每年的 3 月和 9 月发布，Java 11 发布的时间节点也正好处于 Java 8 免费更新到期的前夕。与 Java 9 和 Java 10 这两个被称为 “功能性的版本” 不同，Java 11 仅将提供长期支持服务（LTS, Long-Term-Support），还将作为 Java 平台的默认支持版本，并且会提供技术支持直至 2023 年 9 月，对应的补丁和安全警告等支持将持续至 2026 年。 REPL（JShell）@since 9REPL（Read Eval Print Loop）意为交互式的编程环境。JShell 是 Java 9 新增的一个交互式的编程环境工具。它允许你无需使用类或者方法包装来执行 Java 语句。它与 Python 的解释器类似，可以直接 输入表达式并查看其执行结果。JShell 它提供了一个交互式 shell，用于快速原型、调试、学习 Java 及 Java API，所有这些都不需要 public static void main 方法，也不需要在执行之前编译代码。 1、执行 JSHELL 1234$ jshell| 欢迎使用 JShell -- 版本 13.0.1| 要大致了解该版本, 请键入: /help introjshell&gt; 2、查看 JShell 命令：输入 /help 可以查看 JShell 相关的命令 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061jshell&gt; /help| 键入 Java 语言表达式, 语句或声明。| 或者键入以下命令之一:| /list [&lt;名称或 id&gt;|-all|-start]| 列出您键入的源| /edit &lt;名称或 id&gt;| 编辑源条目| /drop &lt;名称或 id&gt;| 删除源条目| /save [-all|-history|-start] &lt;文件&gt;| 将片段源保存到文件| /open &lt;file&gt;| 打开文件作为源输入| /vars [&lt;名称或 id&gt;|-all|-start]| 列出已声明变量及其值| /methods [&lt;名称或 id&gt;|-all|-start]| 列出已声明方法及其签名| /types [&lt;名称或 id&gt;|-all|-start]| 列出类型声明| /imports| 列出导入的项| /exit [&lt;integer-expression-snippet&gt;]| 退出 jshell 工具| /env [-class-path &lt;路径&gt;] [-module-path &lt;路径&gt;] [-add-modules &lt;模块&gt;] ...| 查看或更改评估上下文| /reset [-class-path &lt;路径&gt;] [-module-path &lt;路径&gt;] [-add-modules &lt;模块&gt;]...| 重置 jshell 工具| /reload [-restore] [-quiet] [-class-path &lt;路径&gt;] [-module-path &lt;路径&gt;]...| 重置和重放相关历史记录 -- 当前历史记录或上一个历史记录 (-restore)| /history [-all]| 您键入的内容的历史记录| /help [&lt;command&gt;|&lt;subject&gt;]| 获取有关使用 jshell 工具的信息| /set editor|start|feedback|mode|prompt|truncation|format ...| 设置配置信息| /? [&lt;command&gt;|&lt;subject&gt;]| 获取有关使用 jshell 工具的信息| /!| 重新运行上一个片段 -- 请参阅 /help rerun| /&lt;id&gt;| 按 ID 或 ID 范围重新运行片段 -- 参见 /help rerun| /-&lt;n&gt;| 重新运行以前的第 n 个片段 -- 请参阅 /help rerun|| 有关详细信息, 请键入 '/help', 后跟| 命令或主题的名称。| 例如 '/help /list' 或 '/help intro'。主题:|| intro| jshell 工具的简介| keys| 类似 readline 的输入编辑的说明| id| 片段 ID 以及如何使用它们的说明| shortcuts| 片段和命令输入提示, 信息访问以及| 自动代码生成的按键说明| context| /env /reload 和 /reset 的评估上下文选项的说明| rerun| 重新评估以前输入片段的方法的说明 3、JShell 执行使用 12345678910111213141516171819202122// JShell 执行计算jshell&gt; 3 + 1$3 ==&gt; 4// JShell 定义变量jshell&gt; var uuid = UUID.randomUUID();uuid ==&gt; f543a51c-8166-49c3-a0d0-c06ed5993f71// 输出的表达式jshell&gt; System.out.println("Hello World!");Hello World!// JShell 创建函数jshell&gt; public String say(String s) &#123; ...&gt; return s; ...&gt; &#125;| 已创建 方法 say(String)// JShell 使用函数jshell&gt; say("Hello World!");$8 ==&gt; "Hello World!" 4、退出 JShell：输入 /exit 命令退出 jshell 12jshell&gt; /exit| 再见 局部变量类型推断 @since 10从 Java 10 开始，便引入了局部变量类型推断这一关键特性。类型推断允许使用关键字 var 作为局部变量的类型而不是实际类型，编译器根据分配给变量的值推断出类型。这一改进简化了代码编写、节省了开发者的工作时间，因为不再需要显式声明局部变量的类型，而是可以使用关键字 var，且不会使源代码过于复杂。var 是 Java10 中新增的局部类型变量推断。它会根据后面的值来推断变量的类型，所以 var 必须要初始化。var 其实就是 Java 10 增加的一种语法糖而已，在编译期间会自动推断实际类型，其编译后的字节码和实际类型一致。 但是在 Java 10 中，还有下面几个限制： 只能用于局部变量上 声明时必须初始化 不能用作方法参数 不能在 Lambda 表达式中使用 12345678910111213141516// var 定义局部变量[等同于 int a = 1;]var a = 1; // var 接收方法返回[等同于 String result = this.getResult();]var result = this.getResult();// var 循环中定义局部变量for (var i = 0; i &lt; 5; i++) &#123; System.out.println(i);&#125;// var 结合泛型[等同于 List&lt;String&gt; list = new ArrayList&lt;&gt;();]var list = new ArrayList&lt;String&gt;();// var 结合泛型[&lt;&gt;里默认会是Object]var list2 = new ArrayList&lt;&gt;(); 用于 Lambda 参数的局部变量语法 @since 11Java 11 与 Java 10 的不同之处在于允许开发者在 Lambda 表达式中使用 var 进行参数声明。乍一看，这一举措似乎有点多余，因为在写代码过程中可以省略 Lambda 参数的类型，并通过类型推断确定它们。但是，添加上类型定义同时使用 @Nonnull 和 @Nullable 等类型注释还是很有用的，既能保持与局部变量的一致写法，也不丢失代码简洁。 12@Nonnull var x = new Foo();(@Nonnull var x, @Nullable var y) -&gt; x.process(y) 局部变量类型推断优缺点 （1）优点：简化代码 1234567CopyOnWriteArrayList list1 = new CopyOnWriteArrayList();ConcurrentModificationException cme1 = new ConcurrentModificationException();DefaultServiceUnavailableRetryStrategy strategy1 = new DefaultServiceUnavailableRetryStrategy();var list2 = new CopyOnWriteArrayList &lt;&gt;();var cme2 = new ConcurrentModificationException ();var strategy2 = new DefaultServiceUnavailableRetryStrategy (); 从以上代码可以看出，很长的定义类型会显得代码很冗长，使用 var 大大简化了代码编写，同时类型统一显得代码很对齐。 （2）缺点：掩盖类型 1var token = new JsonParserDelegate(parser).currentToken(); 看以上代码，不进去看返回结果类型，谁知道返回的类型是什么？所以这种情况最好别使用 var，而使用具体的抽象类、接口或者实例类型。 私有接口方法 @since 9在 Java 8之前，接口可以有常量变量和抽象方法。我们不能在接口中提供方法实现。如果我们要提供抽象方法和非抽象方法（方法与实现）的组合，那么我们就得使用抽象类。 在 Java 8 接口引入了一些新功能——默认方法和静态方法。我们可以在Java SE 8的接口中编写方法实现，仅仅需要使用 default 关键字来定义它们。 Java 9 不仅像 Java 8 一样支持接口默认方法，同时还支持私有方法。在 Java 9 中，一个接口中能定义如下几种变量/方法： 常量 抽象方法 默认方法 静态方法 私有方法 私有静态方法 12345678910public interface Employee &#123; private Long createEmployeeID() &#123; // Method implementation goes here. &#125; private static void displayEmployeeDetails() &#123; // Method implementation goes here. &#125;&#125; 字符串增强 @since 11Java 11 增加了一系列的字符串处理方法，如以下所示： 1234567891011121314151617181920212223// 判断字符串是否为空白boolean blank = " ".isBlank();System.out.println(blank); // true// 去除首尾空格String strip = " Hello Java11 ".strip();System.out.println(strip); // "Hello Java11"// 去除尾部空格String stripTrailing = " Hello Java11 ".stripTrailing();System.out.println(stripTrailing); // " Hello Java11"// 去除首部空格String stripLeading = " Hello Java11 ".stripLeading();System.out.println(stripLeading); // "Hello Java11 "// 复制字符串String repeat = "Java11".repeat(3);System.out.println(repeat); // "Java11Java11Java11"// 行数统计long count = "A\nB\nC".lines().count();System.out.println(count); // 3 集合工厂方法 @since 9自 Java 9 开始，Jdk 里面为集合（List/ Set/ Map）都添加了 of 和 copyOf 方法，它们两个都用来创建不可变的集合，来看下它们的使用和区别。 12345678// 不可变集合 Listvar list = List.of("Java", "Python", "C");// 不可变集合 Setvar set = Set.of("Java", "Python", "C");// 不可变集合 Mapvar map = Map.of("Java", 1, "Python", 2, "C", 3); 注意：使用 of 创建的集合为不可变集合，不能进行添加、删除、替换、排序等操作，不然会报 java.lang.UnsupportedOperationException 异常，使用 Set.of() 不能出现重复元素、Map.of() 不能出现重复 key，否则回报 java.lang.IllegalArgumentException。 使用 copyOf 方法创建不可变集合 @since 10123456789// 不可变集合 Listvar immutableList = List.of("Java", "Python", "C");var copyImmutableList = List.copyOf(immutableList);System.out.println(immutableList == copyImmutableList); // true// 正常集合 List：用的 new 创建的集合，不属于不可变 AbstractImmutableList 类的子类，所以 copyOf 方法又创建了一个新的实例，所以为falsevar list = new ArrayList&lt;String&gt;();var copy = List.copyOf(list);System.out.println(list == copy); // false 来看下它们的源码： 123456789101112131415161718192021222324static &lt;E&gt; List&lt;E&gt; of(E... elements) &#123; switch (elements.length) &#123; // implicit null check of elements case 0: return ImmutableCollections.emptyList(); case 1: return new ImmutableCollections.List12&lt;&gt;(elements[0]); case 2: return new ImmutableCollections.List12&lt;&gt;(elements[0], elements[1]); default: return new ImmutableCollections.ListN&lt;&gt;(elements); &#125;&#125;static &lt;E&gt; List&lt;E&gt; copyOf(Collection&lt;? extends E&gt; coll) &#123; return ImmutableCollections.listCopy(coll);&#125;static &lt;E&gt; List&lt;E&gt; listCopy(Collection&lt;? extends E&gt; coll) &#123; if (coll instanceof AbstractImmutableList &amp;&amp; coll.getClass() != SubList.class) &#123; return (List&lt;E&gt;)coll; &#125; else &#123; return (List&lt;E&gt;)List.of(coll.toArray()); &#125;&#125; 可以看出 copyOf 方法会先判断来源集合是不是 AbstractImmutableList 类型的，如果是，就直接返回，如果不是，则调用 of 创建一个新的集合。 Stream 增强 @since 9Stream 是 Java 8 中的新特性，Java 9 开始对 Stream 增加了以下 4 个新方法。 （1) 增加单个参数构造方法，可为 null，此方法可以接收 null 来创建一个空流 12long count = Stream.ofNullable(null).count();System.out.println(count); // 0 （2）增加 takeWhile 和 dropWhile 方法 1234567891011// 从开始计算，当 n &lt; 3 时就截止：此方法根据 Predicate 接口来判断如果为 true 就取出来生成一个新的流，只要碰到 false 就终止，不管后边的元素是否符合条件。List&lt;Integer&gt; collect = Stream.of(1, 2, 3, 2, 1) .takeWhile(n -&gt; n &lt; 3) .collect(Collectors.toList());System.out.println(collect.toString()); // [1, 2]// 一旦 n &lt; 3 不成立就开始计算：此方法根据 Predicate 接口来判断如果为 true 就丢弃，只要碰到 false 就来生成一个新的流，不管后边的元素是否符合条件。List&lt;Integer&gt; collect1 = Stream.of(1, 2, 3, 2, 1) .dropWhile(n -&gt; n &lt; 3) .collect(Collectors.toList());System.out.println(collect1.toString()); // [3, 2, 1] （3）iterate 重载 以前使用 iterate 方法生成无限流需要配合 limit 进行截断 1234List&lt;Integer&gt; collect2 = Stream.iterate(1, i -&gt; i + 1) .limit(5) .collect(Collectors.toList());System.out.println(collect2.toString()); // [1, 2, 3, 4, 5] 现在重载后这个方法增加了个判断参数 123List&lt;Integer&gt; collect3 = Stream.iterate(1, i -&gt; i &lt;= 5, i -&gt; i + 1) .collect(Collectors.toList());System.out.println(collect3.toString()); // [1, 2, 3, 4, 5] Optional 增强 @since 9Opthonal 也增加了几个非常酷的方法，现在可以很方便的将一个 Optional 转换成一个 Stream，或者当一个空 Optional 时给它一个替代的。 （1）stream()：stream 方法的作用就是将 Optional 转为一个 Stream，如果该 Optional 中包含值，那么就返回包含这个值的 Stream，否则返回一个空的 Stream（Stream.empty()）。 1234567// 返回Optional值的流Stream&lt;String&gt; stream = Optional.of("Java 11").stream();stream.forEach(System.out::println); // Java 11// 返回空流Stream&lt;Object&gt; stream2 = Optional.empty().stream();stream2.forEach(System.out::println); // （2）ifPresentOrElse(Consumer&lt; ? super T&gt; action, Runnable emptyAction)：ifPresentOrElse 方法的改进就是有了 else，接受两个参数 Consumer 和 Runnable。ifPresentOrElse 方法的用途是，如果一个 Optional 包含值，则对其包含的值调用函数 action，即 action.accept(value)，这与 ifPresent 一致；与 ifPresent 方法的区别在于，ifPresentOrElse 还有第二个参数 emptyAction —— 如果 Optional 不包含值，那么 ifPresentOrElse 便会调用 emptyAction，即 emptyAction.run()。 1234567Optional&lt;Integer&gt; optional = Optional.of(1);optional.ifPresentOrElse(x -&gt; System.out.println("Value: " + x), () -&gt; System.out.println("Not Present.")); // Value: 1optional = Optional.empty();optional.ifPresentOrElse(x -&gt; System.out.println("Value: " + x), () -&gt; System.out.println("Not Present.")); //Not Present. （3）or(Supplier&lt; ? extends Optional&lt; ? extends T&gt;&gt; supplier)：如果值存在，返回 Optional 指定的值，否则返回一个预设的值。 1234567Optional&lt;String&gt; optional1 = Optional.of("Mahesh");Supplier&lt;Optional&lt;String&gt;&gt; supplierString = () -&gt; Optional.of("Not Present");optional1 = optional1.or(supplierString);optional1.ifPresent(x -&gt; System.out.println("Value: " + x)); // Value: Maheshoptional1 = Optional.empty();optional1 = optional1.or(supplierString);optional1.ifPresent(x -&gt; System.out.println("Value: " + x)); // Value: Not Present 标准 HTTP Client 升级 @since 11Java 11 对 Java 9 中引入并在 Java 10 中进行了更新的 Http Client API 进行了标准化，在前两个版本中进行孵化的同时，Http Client 几乎被完全重写，并且现在完全支持异步非阻塞。 Java 11 中的新 Http Client API，提供了对 HTTP/2 等业界前沿标准的支持，同时也向下兼容 HTTP/1.1，精简而又友好的 API 接口，与主流开源 API（如：Apache HttpClient、Jetty、OkHttp 等）类似甚至拥有更高的性能。与此同时它是 Java 在 Reactive-Stream 方面的第一个生产实践，其中广泛使用了 Java Flow API，终于让 Java 标准 HTTP 类库在扩展能力等方面，满足了现代互联网的需求，是一个难得的现代 Http/2 Client API 标准的实现，Java 工程师终于可以摆脱老旧的 HttpURLConnection 了。 同步请求会阻止当前线程 12345678var request = HttpRequest.newBuilder() .uri(URI.create("https://www.baidu.com/")) .GET() .build();var client = HttpClient.newHttpClient();// 同步HttpResponse&lt;String&gt; response = client.send(request, HttpResponse.BodyHandlers.ofString());System.out.println(response.body()); 异步请求不会阻止当前线程，而是返回 CompletableFuture 来进行异步操作 1234567891011var client = HttpClient.newHttpClient();var request = HttpRequest.newBuilder() .uri(URI.create("https://www.baidu.com/")) // 省略.GET()，因为它是默认的请求方法。 // .GET() .build();// 异步client.sendAsync(request, HttpResponse.BodyHandlers.ofString()) .thenApply(HttpResponse::body) .thenAccept(System.out::println) .join(); 简化启动单个源代码文件的方法 @since 11Java 11 版本中最令人兴奋的功能之一是增强 Java 启动器，使之能够运行单一文件的 Java 源代码。此功能允许使用 Java 解释器直接执行 Java 源代码。源代码在内存中编译，然后由解释器执行。唯一的约束在于所有相关的类必须定义在同一个 Java 文件中。 如今单文件程序在编写小实用程序时很常见，特别是脚本语言领域。从中开发者可以省去用 Java 编译程序等不必要工作，以及减少新手的入门障碍。 举个例子，写一个类文件 HelloWorld.java 123456public class HelloWorld &#123; public static void main(String[] args) &#123; System.out.println("Hello World!"); &#125;&#125; 以前简化启动单个源代码文件需要这样运行 123$ javac HelloWorld.java$ java HelloWorldHello World 现在只需要这样12$ java HelloWorld.javaHello World 更多新特性 Flow API for reactive programming Java Module System Application Class Data Sharing Dynamic Class-File Constants Java REPL (JShell) Flight Recorder Unicode 10 G1: Full Parallel Garbage Collector ZGC: Scalable Low-Latency Garbage Collector Epsilon: No-Op Garbage Collector Deprecate the Nashorn JavaScript Engine … 参考博文[1]. Java 11 正式发布，这 8 个逆天新特性教你写出更牛逼的代码[2]. Java 11 新特性介绍]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java11</tag>
        <tag>Java9</tag>
        <tag>Java10</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[了不起的消息队列（二）：啊哈！RabbitMQ]]></title>
    <url>%2Farchives%2Ff99da47b.html</url>
    <content type="text"><![CDATA[背景在本系列的前一篇博文中，笔者对消息队列的概述、特点等进行讲解，然后对消息队列使用场景进行分析，最后对市面上比较常见的消息队列产品进行技术对比。 经过上一篇博客介绍，相信大家对消息队列已经有了一个大致了解。RabbitMQ 是 MQ 产品的典型代表，是一款基于 AMQP 协议可复用的企业消息系统。业务上，可以实现服务提供者和消费者之间的数据解耦，提供高可用性的消息传输机制，在实际生产中应用相当广泛。本文意在介绍 RabbitMQ 的基本原理，包括 RabbitMQ 基本框架、概念、通信过程等，介绍一下 RabbitMQ 安装教程，最后介绍一下 RabbitMQ 在项目中实际应用场景。 RabbitMQ 介绍RabbitMQ 是采用 Erlang 语言实现的 AMQP[1] 协议的消息中间件，最初起源于金融系统，用于在分布式系统中存储转发消息。RabbitMQ 发展到今天，被越来越多的人认可，这和它在可靠性、可用性、扩展性、功能丰富等方面的卓越表现是分不开的。RabbitMQ 实现了 AQMP 协议，AQMP 协议定义了消息路由规则和方式。生产端通过路由规则发送消息到不同 queue，消费端根据 queue 名称消费消息。此外 RabbitMQ 是向消费端推送消息，订阅关系和消费状态保存在服务端。 相关概念通常我们谈到消息队列时会有三个概念：生产者、队列、消费者，RabbitMQ 在这个基本概念之上，多做了一层抽象，在生产者和队列之间，加入了交换器（Exchange）。这样生产者和队列就没有直接联系，转而变成发生产者把消息给交换器，交换器根据调度策略再把消息再给队列。因此在 RabbitMQ 的消息传递模型中，他的核心思想是生产者永远不会将任何消息直接发送到队列上，甚至不知道消息是否被传递到任何队列。生产者向 Exchanges 发送消息。 Exchanges 负责生产者消息的接收，将消息推送到队列。Exchanges 通过 exchange type 指定的类型明确要如何处理消息，比如附加到特定队列或者所有队列，或者将消息丢弃。 Message：消息，它由消息头和消息体组成。消息体是不透明的，而消息头则由一系列的可选属性组成，这些属性包括 routing-key（路由键）、 priority（相对于其他消息的优先权）、 delivery-mode（指出该消息可能需要持久性存储）等。 Publisher：消息生产者，也是一个向交换器发布消息的客户端应用程序。 Exchange：交换器，用来接收生产者发送的消息并将这些消息路由给服务器中的队列。Exchange 有 4 种类型： direct（默认）、 fanout、topic 和 headers ，不同类型的 Exchange 转发消息的策略有所区别。 Queue：消息队列，用来保存消息直到发送给消费者。它是消息的容器，也是消息的终点。一个消息可投入一个或多个队列。消息一直在队列里面，等待消费者连接到这个队列将其取走。 Binding：绑定，用于消息队列和交换器之间的关联。一个绑定就是基于路由键将交换器和消息队列连接起来的路由规则，所以可以将交换器理解成一个由绑定构成的路由表。Exchange 和 Queue 的绑定可以是多对多的关系。 Connection：网络连接，比如一个 TCP 连接。 Channel：信道，多路复用连接中的一条独立的双向数据流通道。信道是建立在真实的 TCP 连接内的虚拟连接， AMQP 命令都是通过信道发出去的，不管是发布消息、订阅队列还是接收消息，这些动作都是通过信道完成。因为对于操作系统来说建立和销毁 TCP 都是非常昂贵的开销，所以引入了信道的概念，以复用一条 TCP 连接。 Consumer：消息消费者，表示一个从消息队列中取得消息的客户端应用程序。 Virtual Host：虚拟主机，表示一批交换器、消息队列和相关对象。虚拟主机是共享相同的身份认证和加密环境的独立服务器域。每个 vhost 本质上就是一个 mini 版的 RabbitMQ 服务器，拥有自己的队列、交换器、绑定和权限机制。 vhost 是 AMQP 概念的基础，必须在连接时指定，RabbitMQ 默认的 vhost 是 / 。 Exchange交换器，用来接收生产者发送的消息并将这些消息路由给服务器中的队列。当我们发送一条消息时，首先会发给交换器（exchange），交换器根据规则（路由键：routing_key）将会确定消息投递到那个队列（queue）。交换机不存储消息，如果没有 Queue Binding 到 Exchange 的话，它会直接丢弃掉 Publisher 发送过来的消息；在启用 ack 模式后，交换机找不到队列会返回错误。Exchange 有 4 种类型： direct（默认）、fanout、topic 和 headers ，不同类型的 Exchange 转发消息的策略有所区别。 1、Direct Exchange Direct Exchange：直接交换器，Direct Exchange 是 RabbitMQ 默认的交换机模式，也是最简单的交换机模式，根据 ROUTING_KEY 全文匹配去寻找队列。其工作方式类似于单播，Exchange 会将消息发送完全匹配 ROUTING_KEY 的 Queue。 Direct 模式，可以使用 RabbitMQ 自带的 Exchange：default Exchange 。所以不需要将 Exchange 进行任何绑定（binding）操作 。消息传递时，ROUTING_KEY 必须完全匹配，才会被队列接收，否则该消息会被抛弃。 2、Fanout Exchange Fanout Exchange：扇形交换器，所有发送到 Fanout Exchange 的消息都会被转发到与该 Exchange 绑定 （Binding） 的所有 Queue 上。 Fanout 模式，Fanout Exchange 不需要处理 ROUTING_KEY。只需要简单的将队列绑定到 Exchange 上，这样发送到 Exchange 的消息都会被转发到与该交换机绑定的所有队列上。类似子网广播，每台子网内的主机都获得了一份复制的消息。所以，Fanout Exchange 转发消息是最快的。 3、Topic Exchange Topic Exchange：主题交换器，工作方式类似于组播，Exchange 会将消息转发和 ROUTING_KEY 匹配模式相同的所有队列。 Topic 模式，Exchange 将 ROUTING_KEY 和某 Topic 进行模糊匹配。此时队列需要绑定一个 Topic，可以使用通配符进行模糊匹配，符号 “#” 匹配一个或多个词，符号 “” 匹配不多不少一个词。因此 “log.#” 能够匹配到“log.info.oa”，但是“log.” 只会匹配到“log.error”。所以，Topic Exchange 使用非常灵活。 4、Headers Exchange Headers Exchange：首部交换器和扇形交换器都不需要路由键 ROUTING_KEY，首部交换器和主题交换机有点相似，但是不同于主题交换机的路由是基于路由键，头交换机的路由值基于消息的 header 数据，主题交换机路由键只有是字符串，而头交换机可以是整型和哈希值。 Headers 模式，Headers 是一个键值对，可以定义成 Hashtable。发送者在发送的时候定义一些键值对，接收者也可以再绑定时候传入一些键值对，两者匹配的话，则对应的队列就可以收到消息。匹配有两种方式 all 和 any。这两种方式是在接收端必须要用键值 “x-mactch” 来定义。all 代表定义的多个键值对都要满足，而 any 则代码只要满足一个就可以了。fanout，direct，topic exchange 的 ROUTING_KEY 都需要要字符串形式的，而 headers exchange 则没有这个要求，因为键值对的值可以是任何类型。 RabbitMQ 安装以及环境配置本文统一使用软件包管理器的方式安装 RabbitMQ，减少环境变量的配置，更加方便快捷。RabbitMQ 官网也有详细的安装教程，感兴趣的同学，可以参考下。Downloading and Installing RabbitMQ Linux 安装 RabbitMQCentOS7 中使用 yum 安装 RabbitMQ 的方法，RabbitMQ 是采用 Erlang 语言实现，因此安装 RabbitMQ 前，需要先安装 Erlang。直接用 yum install erlang 安装的版本是 R16B-03.18.el7，不满足要求，为此，RabbitMQ 贴心提供了一个 erlang.repo，将以下内容添加到 /etc/yum.repos.d/rabbitmq-erlang.repo。 1234567891011121314151617181920212223242526272829303132333435363738394041# 1. 将 erlang 新版本源添加到 /etc/yum.repos.d/rabbitmq-erlang.repo# In /etc/yum.repos.d/rabbitmq_erlang.repo[rabbitmq_erlang]name=rabbitmq_erlangbaseurl=https://packagecloud.io/rabbitmq/erlang/el/7/$basearchrepo_gpgcheck=1gpgcheck=1enabled=1# PackageCloud's repository key and RabbitMQ package signing keygpgkey=https://packagecloud.io/rabbitmq/erlang/gpgkey https://dl.bintray.com/rabbitmq/Keys/rabbitmq-release-signing-key.ascsslverify=1sslcacert=/etc/pki/tls/certs/ca-bundle.crtmetadata_expire=300[rabbitmq_erlang-source]name=rabbitmq_erlang-sourcebaseurl=https://packagecloud.io/rabbitmq/erlang/el/7/SRPMSrepo_gpgcheck=1gpgcheck=0enabled=1# PackageCloud's repository key and RabbitMQ package signing keygpgkey=https://packagecloud.io/rabbitmq/erlang/gpgkey https://dl.bintray.com/rabbitmq/Keys/rabbitmq-release-signing-key.ascsslverify=1sslcacert=/etc/pki/tls/certs/ca-bundle.crtmetadata_expire=300# 2. 安装 erlang$ yum install erlang -y# 3. 将 RabbitMQ 新版本源添加到 /etc/yum.repos.d/rabbitmq-erlang.repo[bintray-rabbitmq-server]name=bintray-rabbitmq-rpmbaseurl=https://dl.bintray.com/rabbitmq/rpm/rabbitmq-server/v3.8.x/el/7/gpgcheck=0repo_gpgcheck=0enabled=1# 4. 安装 RabbitMQ$ yum install rabbitmq-server -y Mac 安装 RabbitMQMac 中使用 brew 安装 RabbitMQ 的方法 12# 1. 使用 RabbitMQ 安装$ brew install rabbitmq Windows 安装 RabbitMQWindows 中使用 choco 安装 RabbitMQ 的方法 12# 1. 使用 RabbitMQ 安装$ choco install rabbitmq 开启 rabbitmq_management 以便通过浏览器访问控制台12345678# 1. 开启 rabbitmq_management 以便通过浏览器访问控制台$ rabbitmq-plugins enable rabbitmq_management# 2. 将 RabbitMQ 加入开机自启动$ systemctl enable rabbitmq-server.service# 3. 立即启动 RabbitMQ$ systemctl start rabbitmq-server.service 管理控制台的地址默认为 http://server-name:15672 （将其中 server-name 替换为你自己的 ip 地址）。RabbitMQ 默认有个 guest 账号，密码也为 guest，但是如果不是从 RabbitMQ 所在机器上试图用这个账号登陆管理控制台的话，会报错误：“User can only log in via localhost”。RabbitMQ 3.0 开始禁止使用 guest/guest 权限通过除 localhost 外的访问。因此，我们需要添加一个超级管理员。 12345# 1. 添加一个账户吧，用户名 admin 密码 admin$ rabbitmqctl add_user admin admin# 2. 该用户赋予超级管理员的角色$ rabbitmqctl set_user_tags admin administrator 在 RabbitMQ 中，用户角色可分为五类： 超级管理员（administrator）：可登陆管理控制台，可查看所有的信息，并且可以对用户、策略（policy） 进行操作。 监控者（monitoring）：可登陆管理控制台，同时可以查看 RabbitMQ 节点的相关信息（进程数，内存使用情况，磁盘使用情况等）。 策略制定者（policymaker）：可登陆管理控制台，同时可以对 policy 进行管理，但无法查看节点的相关信息。 普通管理者（management）：仅可登陆管理控制台，无法看到节点信息，也无法对策略进行管理。 其他：无法登陆管理控制台，通常就是普通的生产者和消费者。 RabbitMQ 的六种消息队列教程 1、 导入 RabbitMQ 的客户端依赖 12345&lt;dependency&gt; &lt;groupId&gt;com.rabbitmq&lt;/groupId&gt; &lt;artifactId&gt;amqp-client&lt;/artifactId&gt; &lt;version&gt;5.8.0&lt;/version&gt;&lt;/dependency&gt; 2、 定义创建连接工具类 1234567891011121314151617public class ConnectionUtils &#123; public static Connection getConnection() throws Exception &#123; // 定义连接工厂 ConnectionFactory factory = new ConnectionFactory(); // 设置服务器地址 factory.setHost("localhost"); // 设置服务器端口 factory.setPort(5672); // 设置服务器账号 factory.setUsername("guest"); // 设置服务器密码 factory.setPassword("guest"); // 通过连接工厂获取连接 return factory.newConnection(); &#125;&#125; “Hello World!”创建一个生产者项目用来向消息队列发送数据，创建一个消费者项目用来从消息队列里接收数据，消费者需要注册到指定到 MQ 队列中，如下图所示： 1、Publisher ：消息生产者 Publisher 向交换器（AMQP default）发送消息，交换器类型为 Direct Exchange，消息传递时，ROUTING_KEY 必须完全匹配，才会被队列接收，否则该消息会被抛弃。 1234567891011121314151617public class Send &#123; private final static String QUEUE_NAME = "hello"; public static void main(String[] args) throws Exception &#123; // 获取连接并创建通道 try (Connection connection = ConnectionUtils.getConnection(); Channel channel = connection.createChannel()) &#123; // 声明创建队列（队列名称，非持久化，非独占，不自动删除队列，空参数） channel.queueDeclare(QUEUE_NAME, false, false, false, null); // 消息内容 String message = "Hello World!"; // 发送消息（exchange，routingKey，其他属性，消息正文），交换器名称为默认(AMQP default)，交换器类型为direct channel.basicPublish("", QUEUE_NAME, null, message.getBytes()); System.out.println(" [x] Sent '" + message + "'"); &#125; &#125;&#125; 2、Consumer ：消息消费者 Consumer 从消息队列 hello 中取得消息 123456789101112131415161718192021public class Recv &#123; private final static String QUEUE_NAME = "hello"; public static void main(String[] args) throws Exception &#123; // 获取连接并创建通道 try (Connection connection = ConnectionUtils.getConnection(); Channel channel = connection.createChannel()) &#123; // 声明创建队列（队列名称，非持久化，非独占，不自动删除队列，空参数） channel.queueDeclare(QUEUE_NAME, false, false, false, null); // 消费者接收到队列投递的回调（consumerTag 服务端生成的消费者标识，delivery投递） DeliverCallback deliverCallback = (consumerTag, delivery) -&gt; &#123; // 获取消息内容并输出 String message = new String(delivery.getBody(), StandardCharsets.UTF_8); System.out.println(" [x] Received '" + message + "'"); &#125;; // 开启消费者监听（队列名称，自动确认消息，投递消息回调，取消消息回调），此时 RabbitMQ 将消息推送至消费者（若想用拉的方式，则可以使用channel.basicGet()方法） channel.basicConsume(QUEUE_NAME, true, deliverCallback, consumerTag -&gt; &#123; &#125;); &#125; &#125;&#125; Work queuesWork queues 工作队列也称为任务队列，主要思想是避免立即执行资源密集型的任务。将需要执行的任务放到消息队列中，等待资源空闲时消费者从消息队列中取出消息并逐个执行。使用任务队列的优点之一是能够轻松并行化工作，如果我们正在积压工作，我们可以增加更多的消费者，这样就可以轻松扩展。工作队列适用于很多场景，一般的使用方式也都是采用任务队列，如下图所示： 1、Publisher ：消息生产者 Publisher 向交换器（AMQP default）发送消息，交换器类型为 Direct Exchange。 1234567891011121314151617181920public class Send &#123; private final static String QUEUE_NAME = "hello"; public static void main(String[] args) throws Exception &#123; // 获取连接并创建通道 try (Connection connection = ConnectionUtils.getConnection(); Channel channel = connection.createChannel()) &#123; // 声明创建队列（队列名称，持久化，非独占，不自动删除队列，空参数） channel.queueDeclare(QUEUE_NAME, true, false, false, null); // 遍历发送 100 条消息 for (int i = 0; i &lt; 100; i++) &#123; // 消息内容 String message = String.format("Hello World! %s", i); // 发送消息（exchange，routingKey，其他属性，消息正文），交换器名称为默认(AMQP default)，交换器类型为direct channel.basicPublish("", QUEUE_NAME, null, message.getBytes()); System.out.println(" [x] Sent '" + message + "'"); &#125; &#125; &#125;&#125; 2、Consumer：消息消费者 Consumer 从消息队列 hello 中取得消息，通过 Thread.sleep() 函数来伪造资源密集型的任务 12345678910111213141516171819202122232425262728293031323334public class Recv &#123; private final static String QUEUE_NAME = "hello"; public static void main(String[] args) throws Exception &#123; // 获取连接并创建通道 Connection connection = ConnectionUtils.getConnection(); Channel channel = connection.createChannel(); // 声明创建队列（队列名称，持久化，非独占，不自动删除队列，空参数） channel.queueDeclare(QUEUE_NAME, true, false, false, null); // 消费者接收到队列投递的回调（consumerTag 服务端生成的消费者标识，delivery投递） DeliverCallback deliverCallback = (consumerTag, delivery) -&gt; &#123; // 获取消息内容并输出 String message = new String(delivery.getBody(), StandardCharsets.UTF_8); System.out.println(" [x] Received '" + message + "'"); try &#123; doWork(); &#125; finally &#123; System.out.println(" [x] Done"); // 手动消息确认（若忘记手动消息确认，当您的客户端退出时，消息将被重新发送（可能看起来像是随机重新发送），但是RabbitMQ将消耗越来越多的内存，因为它将无法释放任何未确认的消息。） channel.basicAck(delivery.getEnvelope().getDeliveryTag(), false); &#125; &#125;; // 开启消费者监听（队列名称，自动确认消息，投递消息回调，取消消息回调），此时 RabbitMQ 将消息推送至消费者（若想用拉的方式，则可以使用channel.basicGet()方法） channel.basicConsume(QUEUE_NAME, false, deliverCallback, consumerTag -&gt; &#123; &#125;); &#125; @SneakyThrows private static void doWork() &#123; // Thread.sleep（）函数来伪造资源密集型的任务 Thread.sleep(1000); &#125;&#125; 3、Consumer：当任务积压时，我们只需要启动多个消费者，这样就可以轻松扩展，完成消费。 注意：默认情况下，RabbitMQ 将每个消息依次发送给下一个使用者。平均而言，每个消费者都会收到相同数量的消息。这种分发消息的方式称为循环。 Publish/SubscribePublish/Subscribe 发布订阅模式，将消息广播发送给所有消费者。这里以日志系统为例，假设生产者程序将消息发送给两个消费者，其中一个消费者负责将日志输出到控制台，另外一个消费者负责将日志写入到磁盘。Publish/Subscribe 发布订阅模式中交换器类型为 Fanout Exchange。扇形交换器，所有发送到 Fanout Exchange 的消息都会被转发到与该 Exchange 绑定 （Binding） 的所有 Queue 上，如下图所示： 1、Publisher ：消息生产者 Publisher 向交换器 logs 发送消息，交换器类型为 Fanout Exchange。 1234567891011121314151617public class EmitLog &#123; private static final String EXCHANGE_NAME = "logs"; public static void main(String[] args) throws Exception &#123; // 获取连接并创建通道 try (Connection connection = ConnectionUtils.getConnection(); Channel channel = connection.createChannel()) &#123; // 声明创建交换器（交换器名称，交换器类型，非持久化），交换器类型为扇形交换器 channel.exchangeDeclare(EXCHANGE_NAME, BuiltinExchangeType.FANOUT, false); // 消息内容 String message = "Hello World!"; // 发送消息（exchange，routingKey，其他属性，消息正文），当ExchangeType为fanout时，将忽略routingKey参数 channel.basicPublish(EXCHANGE_NAME, "", null, message.getBytes()); System.out.println(" [x] Sent '" + message + "'"); &#125; &#125;&#125; 2、Consumer：消息消费者 Consumer 从 channel 中获取一个随机的非持久化自动删除队列（客户端退出就自动删除），然 绑定消息队列和 exchange。 1234567891011121314151617181920212223242526public class ReceiveLogs &#123; private static final String EXCHANGE_NAME = "logs"; public static void main(String[] args) throws Exception &#123; // 获取连接 Connection connection = ConnectionUtils.getConnection(); // 创建通道 Channel channel = connection.createChannel(); // 声明创建交换器（交换器名称，交换器类型，非持久化），交换器类型为扇形交换器 channel.exchangeDeclare(EXCHANGE_NAME, BuiltinExchangeType.FANOUT, false); // 获取队列，得到一个随机名称（非持久化的自动删除队列） String queueName = channel.queueDeclare().getQueue(); // 绑定消息队列和exchange channel.queueBind(queueName, EXCHANGE_NAME, ""); // 消费者接收到队列投递的回调（consumerTag 服务端生成的消费者标识，delivery投递） DeliverCallback deliverCallback = (consumerTag, delivery) -&gt; &#123; // 获取消息内容并输出 String message = new String(delivery.getBody(), StandardCharsets.UTF_8); System.out.println(" [x] Received '" + message + "'"); &#125;; // 开启消费者监听（队列名称，自动确认消息，投递消息回调，取消消息回调），此时 RabbitMQ 将消息推送至消费者（若想用拉的方式，则可以使用channel.basicGet()方法） channel.basicConsume(queueName, true, deliverCallback, consumerTag -&gt; &#123; &#125;); &#125;&#125; 3、Consumer：当启动多个消费者，每个消费者会创建队列并绑定至交换器 logs 上，消息生产者向交换器 logs 发送消息，交换器 logs 将消息转发到与该 Exchange 绑定 （Binding） 的所有 Queue 上。 注意：消费者必须先绑定到 exchange 上，然后生产者再发送消息，否则 exchange 无法将消息路由到任何队列。 RoutingRouting 路由，进行有选择的接收消息，可以订阅某个消息队列的子集。例如，我们将只能将严重错误消息定向到日志文件（以节省磁盘空间），同时仍然能够在控制台上打印所有日志消息。Routing 路由模式中交换器类型为 Fanout Exchange。直接交换器，Direct Exchange 是 RabbitMQ 默认的交换机模式，也是最简单的交换机模式，根据 ROUTING_KEY 全文匹配去寻找队列。其工作方式类似于单播，Exchange 会将消息发送完全匹配 ROUTING_KEY 的 Queue，如下图所示： 1、Publisher ：消息生产者 Publisher 向交换器 direct_logs 发送消息，交换器类型为 Direct Exchange。 123456789101112131415161718192021public class EmitLogDirect &#123; private static final String EXCHANGE_NAME = "direct_logs"; public static void main(String[] args) throws Exception &#123; // 获取连接并创建通道 try (Connection connection = ConnectionUtils.getConnection(); Channel channel = connection.createChannel()) &#123; // 声明创建交换器（交换器名称，交换器类型），交换器类型为直接交换器 channel.exchangeDeclare(EXCHANGE_NAME, BuiltinExchangeType.DIRECT, true); // 向指定exchange发送消息，路由key为 info channel.basicPublish(EXCHANGE_NAME, "info", null, "info message".getBytes(StandardCharsets.UTF_8)); // 向指定exchange发送消息，路由key为 warning channel.basicPublish(EXCHANGE_NAME, "warning", null, "warning message".getBytes(StandardCharsets.UTF_8)); // 向指定exchange发送消息，路由key为 error channel.basicPublish(EXCHANGE_NAME, "error", null, "error message".getBytes(StandardCharsets.UTF_8)); &#125; &#125;&#125; 2、Consumer：消息消费者 Consumer 从 channel 中获取一个随机的非持久化自动删除队列（客户端退出就自动删除），绑定消息队列、exchange、路由 key。 1234567891011121314151617181920212223242526public class ReceiveLogsDirect &#123; private static final String EXCHANGE_NAME = "direct_logs"; public static void main(String[] args) throws Exception &#123; // 获取连接 Connection connection = ConnectionUtils.getConnection(); // 创建通道 Channel channel = connection.createChannel(); // 声明创建交换器（交换器名称，交换器类型），交换器类型为直接交换器 channel.exchangeDeclare(EXCHANGE_NAME, BuiltinExchangeType.DIRECT); // 获取队列，得到一个随机名称（非持久化的自动删除队列） String queueName = channel.queueDeclare().getQueue(); // 绑定消息队列、exchange、路由key为error channel.queueBind(queueName, EXCHANGE_NAME, "error"); // 消费者接收到队列投递的回调（consumerTag 服务端生成的消费者标识，delivery投递） DeliverCallback deliverCallback = (consumerTag, delivery) -&gt; &#123; // 获取消息内容并输出 String message = new String(delivery.getBody(), StandardCharsets.UTF_8); System.out.println(" [x] Received '" + message + "'"); &#125;; // 开启消费者监听（队列名称，自动确认消息，投递消息回调，取消消息回调），此时 RabbitMQ 将消息推送至消费者（若想用拉的方式，则可以使用channel.basicGet()方法） channel.basicConsume(queueName, true, deliverCallback, consumerTag -&gt; &#123; &#125;); &#125;&#125; 3、Consumer：消费者可以为一个队列绑定多个 routingKey。 123456789101112131415161718192021222324252627282930public class ReceiveLogsDirect &#123; private static final String EXCHANGE_NAME = "direct_logs"; public static void main(String[] args) throws Exception &#123; // 获取连接 Connection connection = ConnectionUtils.getConnection(); // 创建通道 Channel channel = connection.createChannel(); // 声明创建交换器（交换器名称，交换器类型），交换器类型为直接交换器 channel.exchangeDeclare(EXCHANGE_NAME, BuiltinExchangeType.DIRECT); // 获取队列，得到一个随机名称（非持久化的自动删除队列） String queueName = channel.queueDeclare().getQueue(); // 绑定消息队列、exchange、路由key为info channel.queueBind(queueName, EXCHANGE_NAME, "info"); // 绑定消息队列、exchange、路由key为error channel.queueBind(queueName, EXCHANGE_NAME, "error"); // 绑定消息队列、exchange、路由key为warning channel.queueBind(queueName, EXCHANGE_NAME, "warning"); // 消费者接收到队列投递的回调（consumerTag 服务端生成的消费者标识，delivery投递） DeliverCallback deliverCallback = (consumerTag, delivery) -&gt; &#123; // 获取消息内容并输出 String message = new String(delivery.getBody(), StandardCharsets.UTF_8); System.out.println(" [x] Received '" + message + "'"); &#125;; // 开启消费者监听（队列名称，自动确认消息，投递消息回调，取消消息回调），此时 RabbitMQ 将消息推送至消费者（若想用拉的方式，则可以使用channel.basicGet()方法） channel.basicConsume(queueName, true, deliverCallback, consumerTag -&gt; &#123; &#125;); &#125;&#125; TopicsTopics 主题，基于主题交换的策略接收消息，基于 Topics 的方式可以让消息队列的使用更加灵活，为消息的发送和订阅提供更加细粒度的控制。例如，在我们的日志记录系统中，我们可能不仅要根据严重性订阅日志，还要根据发出日志的源订阅日志。Topics 主题模式中交换器类型为 Topic Exchange。主题交换器，工作方式类似于组播，Exchange 会将消息转发和 ROUTING_KEY 匹配模式相同的所有队列，如下图所示： 1、Publisher ：消息生产者 Publisher 向交换器 topic_logs 发送消息，交换器类型为 Topic Exchange。首先需要指定 exchange 的类型为 topic，在生产者发送消息时设置 routingKey 为一个符号表达式。 123456789101112131415161718public class EmitLogTopic &#123; private static final String EXCHANGE_NAME = "topic_logs"; public static void main(String[] args) throws Exception &#123; // 获取连接并创建通道 try (Connection connection = ConnectionUtils.getConnection(); Channel channel = connection.createChannel()) &#123; // 声明创建交换器（交换器名称，交换器类型），交换器类型为主题交换器 channel.exchangeDeclare(EXCHANGE_NAME, BuiltinExchangeType.TOPIC); // 向指定exchange发送消息, routingKey 为 s.info.l， 订阅的消费者需要指定主题routingKey为 *.info.* 或者 #.info.# channel.basicPublish(EXCHANGE_NAME, "s.info.l", null, "info message".getBytes(StandardCharsets.UTF_8)); // 向指定exchange发送消息, routingKey 为 lazy.test.one, 订阅的消费者需要指定主题routingKey为 lazy.# channel.basicPublish(EXCHANGE_NAME, "lazy.test.one", null, "lazy message".getBytes(StandardCharsets.UTF_8)); &#125; &#125;&#125; 2、Consumer：消息消费者 Consumer 从 channel 中获取一个随机的非持久化自动删除队列（客户端退出就自动删除），通过符号表达式绑定消息队列、exchange、路由 key。 1234567891011121314151617181920212223242526public class ReceiveLogsTopic &#123; private static final String EXCHANGE_NAME = "topic_logs"; public static void main(String[] args) throws Exception &#123; // 获取连接 Connection connection = ConnectionUtils.getConnection(); // 创建通道 Channel channel = connection.createChannel(); // 声明创建交换器（交换器名称，交换器类型），交换器类型为主题交换器 channel.exchangeDeclare(EXCHANGE_NAME, BuiltinExchangeType.TOPIC); // 获取队列，得到一个随机名称（非持久化的自动删除队列） String queueName = channel.queueDeclare().getQueue(); // 绑定消息队列和exchange channel.queueBind(queueName, EXCHANGE_NAME, "*.info.*"); // 消费者接收到队列投递的回调（consumerTag 服务端生成的消费者标识，delivery投递） DeliverCallback deliverCallback = (consumerTag, delivery) -&gt; &#123; // 获取消息内容并输出 String message = new String(delivery.getBody(), StandardCharsets.UTF_8); System.out.println(" [x] Received '" + message + "'"); &#125;; // 开启消费者监听（队列名称，自动确认消息，投递消息回调，取消消息回调），此时 RabbitMQ 将消息推送至消费者（若想用拉的方式，则可以使用channel.basicGet()方法） channel.basicConsume(queueName, true, deliverCallback, consumerTag -&gt; &#123; &#125;); &#125;&#125; RPCRPC 远程过程调用，RabbitMQ 也支持这种同步调用的特性，调用之后等待调用结果返回。该模式使用率较少，在实际项目中应用场景较小。如下图所示： 客户端通过 RabbitMQ 的 RPC 调用服务端，等待服务端返回结果，示例程序如下： 1、为了说明如何使用 RPC 服务，我们将创建一个简单的客户端类。它将公开一个名为 call 的方法，该方法 发送 RPC 请求并阻塞，直到收到答案为止： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class RPCClient implements AutoCloseable &#123; private Connection connection; private Channel channel; private String RPC_QUEUE_NAME = "rpc_queue"; public RPCClient() throws IOException, TimeoutException &#123; ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost("localhost"); connection = connectionFactory.newConnection(); channel = connection.createChannel(); &#125; public static void main(String[] args) &#123; try (RPCClient fibonacciRpc = new RPCClient()) &#123; for (int i = 0; i &lt; 32; i++) &#123; String i_str = Integer.toString(i); System.out.println(" [x] Requesting fib(" + i_str + ")"); String response = fibonacciRpc.call(i_str); System.out.println(" [.] Got '" + response + "'"); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; public String call(String message) throws IOException, InterruptedException &#123; final String corrId = UUID.randomUUID().toString(); String replayQueueName = channel.queueDeclare().getQueue(); AMQP.BasicProperties basicProperties = new AMQP.BasicProperties .Builder() .correlationId(corrId) .replyTo(replayQueueName) .build(); channel.basicPublish("", RPC_QUEUE_NAME, basicProperties, message.getBytes(StandardCharsets.UTF_8)); final BlockingQueue&lt;String&gt; response = new ArrayBlockingQueue&lt;&gt;(1); String cTag = channel.basicConsume(replayQueueName, true, (consumeTag, delivery) -&gt; &#123; if (delivery.getProperties().getCorrelationId().equals(corrId)) &#123; response.offer(new String(delivery.getBody(), StandardCharsets.UTF_8)); &#125; &#125;, consumerTag -&gt; &#123; &#125;); String result = response.take(); channel.basicCancel(cTag); return result; &#125; @Override public void close() throws Exception &#123; connection.close(); &#125;&#125; 2、由于我们没有值得分配的耗时任务，因此我们将创建一个虚拟 RPC 服务，该服务返回斐波那契数： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667public class RPCServer &#123; private static final String RPC_QUEUE_NAME = "rpc_queue"; private static int fib(int n) &#123; if (n == 0) &#123; return n; &#125; if (n == 1) &#123; return n; &#125; return fib(n - 1) + fib(n - 2); &#125; public static void main(String[] args) &#123; ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost("localhost"); try (Connection connection = connectionFactory.newConnection()) &#123; Channel channel = connection.createChannel(); channel.queueDeclare(RPC_QUEUE_NAME, false, false, false, null); channel.queuePurge(RPC_QUEUE_NAME); channel.basicQos(1); System.out.println(" [x] Awaiting PRC request"); Object monitor = new Object(); DeliverCallback deliverCallback = (consumerTag, message) -&gt; &#123; AMQP.BasicProperties replyProperties = new AMQP.BasicProperties .Builder() .correlationId(message.getProperties().getCorrelationId()) .build(); String response = ""; try &#123; String theMessage = new String(message.getBody(), StandardCharsets.UTF_8); int n = Integer.parseInt(theMessage); System.out.println(" [.] fib(" + n + ")"); response += fib(n); &#125; catch (Exception e) &#123; System.out.println(" [.] " + e.toString()); &#125; finally &#123; channel.basicPublish("", message.getProperties().getReplyTo(), replyProperties, response.getBytes(StandardCharsets.UTF_8)); channel.basicAck(message.getEnvelope().getDeliveryTag(), false); synchronized (monitor) &#123; monitor.notify(); &#125; &#125; &#125;; channel.basicConsume(RPC_QUEUE_NAME, false, deliverCallback, (consumeTag -&gt; &#123; &#125;)); while (true) &#123; synchronized (monitor) &#123; try &#123; monitor.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; catch (TimeoutException | IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; Spring Boot 集成 RabbitMQSpring Boot 集成 RabbitMQ 非常简单，如果只是简单的使用配置非常少，Spring Boot 提供了spring-boot-starter-amqp 项目对消息各种支持。 简单使用1、配置 Pom 包，主要是添加 spring-boot-starter-amqp 的支持 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt;&lt;/dependency&gt; 2、配置 RabbitMQ 的安装地址、端口以及账户信息 1234spring.rabbitmq.host=localhostspring.rabbitmq.port=5672spring.rabbitmq.username=guestspring.rabbitmq.password=guest 3、队列配置 12345678@Configurationpublic class RabbitConfig &#123; @Bean public Queue Queue() &#123; return new Queue("hello"); &#125;&#125; 4、发送者 123456789101112@Componentpublic class HelloSender &#123; @Autowired private AmqpTemplate rabbitTemplate; public void send() &#123; String context = "hello " + new Date(); System.out.println("Sender : " + context); this.rabbitTemplate.convertAndSend("hello", context); &#125;&#125; 5、接收者 123456789@Component@RabbitListener(queues = "hello")public class HelloReceiver &#123; @RabbitHandler public void process(String hello) &#123; System.out.println("Receiver : " + hello); &#125;&#125; 6、测试 1234567891011@SpringBootTestpublic class RabbitMqHelloTest &#123; @Autowired private HelloSender helloSender; @Test public void hello() throws Exception &#123; helloSender.send(); &#125;&#125; 高级使用对象的支持Spring Boot 以及完美的支持对象的发送和接收，不需要格外的配置。 123456789101112131415// 发送者public void send(User user) &#123; System.out.println("Sender object: " + user.toString()); this.rabbitTemplate.convertAndSend("object", user); // Sender object: User&#123;name='neo', pass='123456'&#125;&#125;...// 接收者@RabbitHandler(queues = "object")public void process(User user) &#123; System.out.println("Receiver object : " + user); // Receiver object : User&#123;name='neo', pass='123456'&#125;&#125; 消息持久化在使用 RabbitMQ 时，我们可以通过消息持久化来解决服务器因异常崩溃而造成的消息丢失。在使用 RabbitMQ 时，我们可以通过消息持久化来解决服务器因异常崩溃而造成的消息丢失。其中，RabblitMQ 的持久化分为三个部分：交换器（Exchange）的持久化、队列（Queue）的持久化、消息（Message）的持久化。 1、设置队列、交换器持久化，防止在 RabbitMQ 出现异常情况（重启，宕机）时，Exchange、Queue 丢失，影响后续的消息写入。 1234567891011121314151617181920212223@Configurationpublic class DirectRabbitConfig &#123; @Bean public Queue queue() &#123; // 参数1 name ：队列名 // 参数2 durable ：是否持久化 // 参数3 exclusive ：仅创建者可以使用的私有队列，断开后自动删除 // 参数4 autoDelete : 当所有消费客户端连接断开后，是否自动删除队列 return new Queue("hello", true, false, false, null); &#125; @Bean public DirectExchange directExchange() &#123; // （交换器名称，设置 durable=true 持久化交换器，非独占，不自动删除队列，空参数） return new DirectExchange("directExchange", true, false, null); &#125; @Bean public Binding bindingExchangeMessage(Queue queue, DirectExchange directExchange) &#123; return BindingBuilder.bind(queue).to(directExchange).with("hello"); &#125;&#125; 或者通过消费者 @RabbitListener 注解的方式进行持久化。 1234567891011121314@Component// 如果不存在，自动创建队列和交换器并且绑定@RabbitListener( bindings = @QueueBinding( value = @Queue(value = "hello", durable = "true"), exchange = @Exchange(value = "directExchange", type = "direct", durable = "true"), key = "hello"))public class HelloReceiver &#123; @RabbitHandler public void process(String hello) &#123; System.out.println("Receiver : " + hello); &#125;&#125; 注意：Exchange 交换器的持久化，在声明时指定 durable =&gt; true，若 durable=false 非持久化，在 RabbitMQ 出现异常情况（重启，宕机）时，该 Exchange 会丢失，会影响后续的消息写入该 Exchange；Queue 队列的持久化，在声明时指定 durable =&gt; true，若 durable=false 非持久化，在 RabbitMQ 出现异常情况（重启，宕机）时，队列丢失，队列丢失导致队列与 Exchange 绑定关系丢失，会影响后续的消息路由给服务器中的队列。 3、发送者 123456789101112131415@Componentpublic class HelloSender &#123; @Autowired private AmqpTemplate rabbitTemplate; public void send() &#123; String context = "hello " + new Date(); System.out.println("Sender : " + context); // 通过阅读源代码可以发现 new MessageProperties() 持久化的策略是 MessageDeliveryMode.PERSISTENT，因此它会初始化时默认消息是持久化的 Message message = MessageBuilder.withBody(context.getBytes()).build(); message.getMessageProperties().setDeliveryMode(MessageDeliveryMode.PERSISTENT); this.rabbitTemplate.convertAndSend("hello", message); &#125;&#125; 注意：Spring AMQP 是对原生的 RabbitMQ 客户端的封装。一般情况下，我们只需要定义交换器的持久化和队列的持久化。Message 消息的持久化，在投递时指定 delivery_mode =&gt; 2（1 是非持久化），RabbitTemplate 它持久化的策略是 MessageDeliveryMode.PERSISTENT，因此它会初始化时默认消息是持久化的。 4、注意事项 （1）理论上可以将所有的消息都设置为持久化，但是这样会严重影响 RabbitMQ 的性能。因为写入磁盘的速度比写入内存的速度慢得不止一点点。对于可靠性不是那么高的消息可以不采用持久化处理以提高整体的吞吐量。在选择是否要将消息持久化时，需要在可靠性和吞吐量之间做一个权衡。 （2）将交换器、队列、消息都设置了持久化之后仍然不能百分之百保证数据不丢失，因为当持久化的消息正确存入 RabbitMQ 之后，还需要一段时间（虽然很短，但是不可忽视）才能存入磁盘之中。如果在这段时间内 RabbitMQ 服务节点发生了宕机、重启等异常情况，消息还没来得及落盘，那么这些消息将会丢失。 （3）单单只设置队列持久化，重启之后消息会丢失；单单只设置消息的持久化，重启之后队列消失，继而消息也丢失。单单设置消息持久化而不设置队列的持久化显得毫无意义。 ACK 确认机制默认情况下消息消费者是自动 ack （确认）消息的，自动确认会在消息发送给消费者后立即确认，这样存在丢失消息的可能。 1、配置 RabbitMQ 的安装地址、端口、账户信息以及 ACK 确认模式 1234567spring.rabbitmq.host=118.25.39.41spring.rabbitmq.port=5672spring.rabbitmq.username=adminspring.rabbitmq.password=admin## 确认模式设置为手动签收，1、NONE：没有ack的意思，对应RabbitMQ的自动确认模式；2、MANUAL：手动模式，对应RabbitMQ的显式确认模式；AUTO：自动模式，对应RabbitMQ的显式确认模式；## MANUAL与AUTO的关系有点类似于在Spring中手动提交事务与自动提交事务的区别：一个是手动发送ack；一个是在方法执行完，没有异常的情况下自动发送ackspring.rabbitmq.listener.simple.acknowledge-mode=MANUAL 注意：AcknowledgeMode.MANUAL 模式需要人为地获取到 channel 之后调用方法向 server 发送 ack（或消费失败时的 nack）信息；AcknowledgeMode.AUTO 模式下，由 spring-rabbit 依据消息处理逻辑是否抛出异常自动发送 ack（无异常）或 nack（异常）到 server 端。 2、接收者，消息消费者手动确认消息以及消息拒绝 12345678910111213141516171819@Component@RabbitListener(queues = "hello")public class HelloReceiver &#123; @SneakyThrows @RabbitHandler public void process(String hello, Message message, Channel channel) &#123; String ackMessage = "hello"; if (ackMessage.equals(hello)) &#123; // 确认消息接收：第一个deliveryTag参数为每条信息带有的tag值，第二个multiple参数为布尔类型；为true时会将小于等于此次tag的所有消息都确认掉，如果为false则只确认当前tag的信息，可根据实际情况进行选择。 channel.basicAck(message.getMessageProperties().getDeliveryTag(), false); &#125; else &#123; // 消息拒绝：第一个deliveryTag参数为每条信息带有的tag值，第二个multiple参数为布尔类型，第三个requeue参数为拒绝后是否重新回到队列； channel.basicNack(message.getMessageProperties().getDeliveryTag(), false, false); // 消息拒绝：basicReject() 和 basicNack()的区别在于basicNack可以批量拒绝多条消息，而basicReject一次只能拒绝一条消息。 // channel.basicReject(message.getMessageProperties().getDeliveryTag(), false); &#125; &#125;&#125; Topic Exchangetopic 是 RabbitMQ 中最灵活的一种方式，可以根据 routing_key 自由的绑定不同的队列。 1、首先对 topic 规则配置，使 queueMessages 同时匹配两个队列，queueMessage 只匹配 “topic.message” 队列。 12345678910111213141516171819202122232425262728293031@Configurationpublic class TopicRabbitConfig &#123; final static String message = "topic.message"; final static String messages = "topic.messages"; @Bean public Queue queueMessage() &#123; return new Queue(TopicRabbitConfig.message); &#125; @Bean public Queue queueMessages() &#123; return new Queue(TopicRabbitConfig.messages); &#125; @Bean public TopicExchange exchange() &#123; return new TopicExchange("exchange"); &#125; @Bean public Binding bindingExchangeMessage(Queue queueMessage, TopicExchange exchange) &#123; return BindingBuilder.bind(queueMessage).to(exchange).with("topic.message"); &#125; @Bean public Binding bindingExchangeMessages(Queue queueMessages, TopicExchange exchange) &#123; return BindingBuilder.bind(queueMessages).to(exchange).with("topic.#"); &#125;&#125; 2、发送 send1 会匹配到 topic.# 和 topic.message 两个 Receiver 都可以收到消息，发送 send2 只有 topic.# 可以匹配所有只有 Receiver2 监听到消息。 1234567891011public void send1() &#123; String context = "hi, i am message 1"; System.out.println("Sender : " + context); this.rabbitTemplate.convertAndSend("exchange", "topic.message", context);&#125;public void send2() &#123; String context = "hi, i am messages 2"; System.out.println("Sender : " + context); this.rabbitTemplate.convertAndSend("exchange", "topic.messages", context);&#125; Fanout ExchangeFanout 就是我们熟悉的广播模式或者订阅模式，给 Fanout 交换机发送消息，绑定了这个交换机的所有队列都收到这个消息。 1、首先对 fanout 规则配置，这里使用了 A、B、C 三个队列绑定到 Fanout 交换机上面，发送端的 routing_key 写任何字符都会被忽略。 1234567891011121314151617181920212223242526272829303132333435363738@Configurationpublic class FanoutRabbitConfig &#123; @Bean public Queue AMessage() &#123; return new Queue("fanout.A"); &#125; @Bean public Queue BMessage() &#123; return new Queue("fanout.B"); &#125; @Bean public Queue CMessage() &#123; return new Queue("fanout.C"); &#125; @Bean public FanoutExchange fanoutExchange() &#123; return new FanoutExchange("fanoutExchange"); &#125; @Bean public Binding bindingExchangeA(Queue AMessage, FanoutExchange fanoutExchange) &#123; return BindingBuilder.bind(AMessage).to(fanoutExchange); &#125; @Bean public Binding bindingExchangeB(Queue BMessage, FanoutExchange fanoutExchange) &#123; return BindingBuilder.bind(BMessage).to(fanoutExchange); &#125; @Bean public Binding bindingExchangeC(Queue CMessage, FanoutExchange fanoutExchange) &#123; return BindingBuilder.bind(CMessage).to(fanoutExchange); &#125;&#125; 2、绑定到 fanout 交换机上面的队列都收到了消息 12345public void send() &#123; String context = "hi, fanout msg "; System.out.println("Sender : " + context); this.rabbitTemplate.convertAndSend("fanoutExchange","", context);&#125; 运行结果： 12345Sender : hi, fanout msg ...fanout Receiver B: hi, fanout msg fanout Receiver A : hi, fanout msg fanout Receiver C: hi, fanout msg 基于 RabbitMQ 实现消息延迟队列方案延时队列顾名思义，即放置在该队列里面的消息是不需要立即消费的，而是等待一段时间之后取出消费。在很多的业务场景中，延时队列可以实现很多功能，此类业务中，一般上是非实时的，需要延迟处理的，需要进行重试补偿的。 订单超时关闭：在支付场景中，一般上订单在创建后 30 分钟或 1 小时内未支付的，会自动取消订单。 短信或者邮件通知：在一些注册或者下单业务时，需要在 1 分钟或者特定时间后进行短信或者邮件发送相关资料的。本身此类业务于主业务是无关联性的，一般上的做法是进行异步发送。 重试场景：比如消息通知，在第一次通知出现异常时，会在隔几分钟之后进行再次重试发送。 RabbitMQ 实现延时队列一般而言有两种形式： 第一种方式：利用两个特性，Time To Live（TTL）、Dead Letter Exchanges（DLX），通过消息过期后进入死信交换器，再由交换器转发到延迟消费队列，实现延迟功能 第二种方式：利用 RabbitMQ 中的插件 x-delay-message 实现延迟功能 第一种方式：利用两个特性，Time To Live（TTL）、Dead Letter Exchanges（DLX）AMQP 协议和 RabbitMQ 队列本身没有直接支持延迟队列功能，但是我们可以通过 RabbitMQ 的两个特性 TTL（Time-To-Live，存活时间）和 DLX（Dead-Letter-Exchange，死信队列交换机）来曲线实现延迟队列： 存活时间（Time-To-Live 简称 TTL）RabbitMQ 可以通过设置队列过期时间实现延时消费或者通过设置消息过期时间实现延时消费，如果超时（两者同时设置以最先到期的时间为准），则消息变为 dead letter（死信）。 RabbitMQ 针对队列中的消息过期时间有两种方法可以设置，A：通过队列属性设置，队列中所有消息都有相同的过期时间；B：对消息进行单独设置，每条消息 TTL 可以不同。如果同时使用，则消息的过期时间以两者之间 TTL 较小的那个数值为准。消息在队列的生存时间一旦超过设置的 TTL 值，就成为 dead letter。 死信交换（Dead Letter Exchanges 简称 DLX）设置了 TTL 的消息或队列最终会成为 Dead Letter，当消息在一个队列中变成死信之后，它能被重新发送到另一个交换机中，这个交换机就是 DLX，绑定此 DLX 的队列就是死信队列。 RabbitMQ 的 Queue 可以配置 x-dead-letter-exchange 和 x-dead-letter-routing-key（可选）两个参数，如果队列内出现了 dead letter，则按照这两个参数重新路由转发到指定的队列。 x-dead-letter-exchange：出现 dead letter 之后将 dead letter 重新发送到指定 exchange；x-dead-letter-routing-key：出现 dead letter 之后将 dead letter 重新按照指定的 routing-key 发送。 队列出现 dead letter 的情况有： 消息或者队列的 TTL 过期； 队列达到最大长度； 消息被消费端拒绝（basic.reject or basic.nack）并且 requeue=false。 Spring Boot 集成 RabbitMQ 实现延时队列实战1、队列以及交换器配置 12345678910111213141516171819202122232425262728293031323334@Configurationpublic class RabbitConfig &#123; @Bean public DirectExchange directExchange() &#123; // 死信队列跟交换机类型没有关系，不一定为directExchange，不影响该类型交换机的特性 return new DirectExchange("dead.letter.direct"); &#125; @Bean public Queue repeatTradeQueue() &#123; // 用于延时消费的队列 return new Queue("repeat.trade.queue", true, false, false); &#125; @Bean public Binding repeatTradeBinding(Queue repeatTradeQueue, DirectExchange directExchange) &#123; // 绑定交换机并指定routing key return BindingBuilder.bind(repeatTradeQueue).to(directExchange).with("repeat.trade.queue"); &#125; @Bean public Queue deadLetterQueue() &#123; // 配置死信队列 Map&lt;String, Object&gt; args = new HashMap&lt;&gt;(3); // 方式一：设置队列过期时间实现延时消费，设置队列中消息存活时间为3秒 args.put("x-message-ttl", 3000); // 出现 dead letter 之后将 dead letter 重新发送到指定 exchange args.put("x-dead-letter-exchange", "dead.letter.direct"); // 出现 dead letter 之后将 dead letter 重新按照指定的 routing-key 发送 args.put("x-dead-letter-routing-key", "repeat.trade.queue"); return new Queue("dead.letter.queue", true, false, false, args); &#125;&#125; 2、发送者，这里发送者需要指定前面配置了过期时间的队列 dead.letter.queue 12345678910111213141516171819202122@Componentpublic class DeadLetterSender &#123; @Autowired private AmqpTemplate rabbitTemplate; public void send() &#123; String context = "hello " + new Date(); System.out.println("DeadLetterSender sendTime:" + LocalDateTime.now().toString() + " message:" + context); // 方式二：设置消息过期时间实现延时消费，设置消息存活时间为10秒，同时使用队列过期时间以及消息过期时间，则消息的过期时间以两者之间TTL较小的那个数值为准。 MessagePostProcessor messagePostProcessor = message -&gt; &#123; MessageProperties messageProperties = message.getMessageProperties(); // 设置过期时间10*1000毫秒 messageProperties.setExpiration("10000"); return message; &#125;; // 向dead.letter.queue发送消息，10*1000毫秒后过期，形成死信 rabbitTemplate.convertAndSend("dead.letter.queue", (Object) context, messagePostProcessor); &#125;&#125; 3、接收者，消费者监听指定用于延时消费的队列 repeat.trade.queue 123456789@Component@RabbitListener(queues = "repeat.trade.queue")public class RepeatTradeReceiver &#123; @RabbitHandler public void process(String msg) &#123; System.out.println("RepeatTradeReceiver receiptTime:" + LocalDateTime.now().toString() + " message:" + msg); &#125;&#125; 4、测试 1234567891011121314@SpringBootTestpublic class RabbitMqHelloTest &#123; @Autowired private DeadLetterSender deadLetterSender; @SneakyThrows @Test public void hello() &#123; deadLetterSender.send(); // 线程休眠5s，等待死信队列中的消息过期变成死信，重新发送到 repeatTradeQueue 队列中 TimeUnit.SECONDS.sleep(5); &#125;&#125; 运行结果： 12DeadLetterSender sendTime:2020-01-19T17:15:42.633 message:hello Sun Jan 19 17:15:42 CST 2020RepeatTradeReceiver receiptTime:2020-01-19T17:15:45.672 message:hello Sun Jan 19 17:15:42 CST 2020 第二种方式：利用 RabbitMQ 中的插件 x-delay-message延迟插件 rabbitmq-delayed-message-exchange 是在 RabbitMQ 3.5.7 及以上的版本才支持的，依赖 Erlang/OPT 18.0 及以上运行环境。 实现机制：安装插件后会生成新的 Exchange 类型 x-delayed-message，该类型消息支持延迟投递机制, 接收到消息后并未立即将消息投递至目标队列中，而是存储在 mnesia（一个分布式数据系统） 表中，检测消息延迟时间，如达到可投递时间时并将其通过 x-delayed-type 类型标记的交换机类型投递至目标队列。 安装延迟插件1、下载插件 可以通过 RabbitMQ 官网的官方插件 Community Plugins 下载相对应的 rabbitmq_delayed_message_exchange 插件，并将插件包放在 RabbitMQ 安装目录 plugins 目录下。 1[root@VM_24_98_centos plugins]# wget https://github.com/rabbitmq/rabbitmq-delayed-message-exchange/releases/download/v3.8.0/rabbitmq_delayed_message_exchange-3.8.0.ez 2、开启 rabbitmq_delayed_message_exchange 插件 1[root@VM_24_98_centos plugins]# rabbitmq-plugins enable rabbitmq_delayed_message_exchange 3、查询安装的所有插件，检查 rabbitmq_delayed_message_exchange 插件是否是开启状态 1[root@VM_24_98_centos plugins]# rabbitmq-plugins list 4、重启 RabbitMQ，使插件生效 1[root@VM_24_98_centos ~]# systemctl restart rabbitmq-server.service 此时，通过浏览器访问控制台在交换器栏目下新增交换器多了 “x-delayed-message” 类型。 Spring Boot 集成 RabbitMQ 实现延时队列实战1、队列以及交换器配置 123456789101112131415161718192021222324@Configurationpublic class RabbitConfig &#123; @Bean public CustomExchange directExchange() &#123; // 自定义的交换机类型 Map&lt;String, Object&gt; args = new HashMap&lt;&gt;(2); args.put("x-delayed-type", "direct"); return new CustomExchange("dead.letter.direct", "x-delayed-message", true, false, args); &#125; @Bean public Queue deadLetterQueue() &#123; // 配置死信队列 return new Queue("dead.letter.queue", true); &#125; @Bean public Binding repeatTradeBinding(Queue repeatTradeQueue, CustomExchange directExchange) &#123; // 绑定交换机并指定routing key return BindingBuilder.bind(repeatTradeQueue).to(directExchange).with("repeat.trade.queue").noargs(); &#125;&#125; 2、发送者，这里发送者需要指定前面配置了过期时间的队列 dead.letter.queue 12345678910111213141516171819@Componentpublic class DeadLetterSender &#123; @Autowired private AmqpTemplate rabbitTemplate; public void send() &#123; String context = "hello " + new Date(); System.out.println("DeadLetterSender sendTime:" + LocalDateTime.now().toString() + " message:" + context); // 向dead.letter.queue发送消息，10*1000毫秒后过期，形成死信 rabbitTemplate.convertAndSend("dead.letter.direct", "dead.letter.queue", context, message -&gt; &#123; // 设置消息过期时间实现延时消费，设置消息存活时间为10秒 message.getMessageProperties().setDelay(10000); return message; &#125; ); &#125;&#125; 3、接收者，消费者监听指定用于死信队列 dead.letter.queue 123456789@Component@RabbitListener(queues = "dead.letter.queue")public class DeadLetterReceiver &#123; @RabbitHandler public void process(String msg) &#123; System.out.println("DeadLetterReceiver receiptTime:" + LocalDateTime.now().toString() + " message:" + msg); &#125;&#125; 4、测试 1234567891011121314@SpringBootTestpublic class RabbitMqHelloTest &#123; @Autowired private DeadLetterSender deadLetterSender; @SneakyThrows @Test public void hello() &#123; deadLetterSender.send(); // 等待接收程序执行之后，再退出测试 TimeUnit.SECONDS.sleep(15); &#125;&#125; 运行结果： 12DeadLetterSender sendTime:2020-01-20T11:08:24.412 message:hello Mon Jan 20 11:08:24 CST 2020DeadLetterReceiver receiptTime:2020-01-20T11:08:34.440 message:hello Mon Jan 20 11:08:24 CST 2020 参考博文[1]. RabbitMQ 官方文档地址[2]. Spring Boot(八)：RabbitMQ 详解[3]. Spring Boot（十四）RabbitMQ延迟队列 注脚[1]. AMQP：AMQP（advanced message queuing protocol）在 2003 年时被提出，最早用于解决金融领不同平台之间的消息传递交互问题。顾名思义，AMQP 是一种协议，更准确的说是一种 binary wire-level protocol（链接协议）。在 AMQP 中，消息路由（message routing）和 JMS 存在一些差别，在 AMQP 中增加了 Exchange 和 binding 的角色。producer 将消息发送给 Exchange，binding 决定 Exchange 的消息应该发送到那个 queue，而 consumer 直接从 queue 中消费消息。 了不起的消息队列系列 了不起的消息队列（一）：浅谈消息队列及常见的分布式消息队列中间件 了不起的消息队列（二）：啊哈！RabbitMQ]]></content>
      <categories>
        <category>消息队列</category>
      </categories>
      <tags>
        <tag>安装教程</tag>
        <tag>RabbitMQ</tag>
        <tag>AMQP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解 Java 虚拟机（四）：Jvm 性能监控与调优]]></title>
    <url>%2Farchives%2F3c79a541.html</url>
    <content type="text"><![CDATA[前言处理过线上问题的同学基本上都会遇到系统突然运行缓慢，CPU 100%，以及 Full GC 次数过多的问题。当然，这些问题的最终导致的直观现象就是系统运行缓慢，并且有大量的报警。给一个系统定位问题的时候，知识、经验是关键基础，数据是依据，工具是运用处理数据的手段。这里说的数据包括：运行日志、异常堆栈、GC 日志、线程快照（threaddump/javacore 文件）、堆转侟快照（heapdump/hprof 文件）等。经常使用适当的虚拟机监控和分析的工具可以加快我们分析数据、定位解决问题的速度。 对于线上系统突然产生的运行缓慢问题，如果该问题导致线上系统不可用，那么首先需要做的就是，导出 jstack 和内存信息，然后重启系统，尽快保证系统的可用性。本文主要针对 JDK 的命令行工具，主要包括用于监视虚拟机和故障处理的工具。根据 JDK 的命令行工具提供解决问题的排查思路，从而定位出问题的代码点，进而提供解决该问题的思路。 JDK 命令行工具 名称 主要作用 jps JVM Process Status Tool，显示指定系统内所有 HotSpot 虚拟机进程 jstat JVM Statistics Monitoring Tool，用于收集 HotSpot 虚拟机各方面的运行数据 jinfo Configuration Info For Java，显示虚拟机配置信息 jmap Memory Map for Java，生成虚拟机的内存转储快照（heapdump 文件） jhat JVM Heap Dump Browser，用于分析 heapdump 文件，它会建立一个 HTTP/HTML 服务器，让用于可以在游览器上查看分析结果 jstack Stack Trace for Java，显示虚拟机的线程快照 这些命令行工具大多数是 jdk/lib/tools.jar 类库的一层薄包装而已，它们主要的功能代码是在 tools 类库中实现的。接来下介绍的 JDK 命令行工具大多都是基于 JDK 1.6，因此会存在个别参数在新版本 JDK 被淘汰的情况出现。所有的 JDK 工具都可以在 Oracle 官网的 Java Tools Reference 文档中找到使用说明，这是主要参考，包括命令格式、参数内容、输出信息等等。 jps：虚拟机进程状况工具jps 可以列出正在运行的虚拟机进程，并显示虚拟机执行主类（Main Class，main() 函数所在的类）名称以及这些进程的本地虚拟机唯一 ID（Local Virtual Machine Identifier，LVMID）。 查看 jps 的帮助信息： 123456[root@VM_24_98_centos jvm]# jps -helpusage: jps [-help] jps [-q] [-mlvV] [&lt;hostid&gt;]Definitions: &lt;hostid&gt;: &lt;hostname&gt;[:&lt;port&gt;] jps 命令格式： 1λ jps -lv 选项 作用 -q 只输出 LVMID，省略主类的名称 -m 输出虚拟机进程启动时传递给主类 main() 函数的参数 -l 输出主类的全名，如果进程执行的是 Jar 包，输出 Jar 路径 -v 输出虚拟机进程启动时 JVM 参数 jstat：虚拟机统计信息监视工具jstat（JVM Statistics Monitoring Tool）是用于监视虚拟机各种运行状态信息的命令行工具，它可以显示本地或者远程虚拟机进程中的类装载、内存、垃圾收集、JIT 编译等运行数据。 查看 jstat 的帮助信息： 123456789101112131415161718192021[root@VM_24_98_centos jvm]# jstat -helpUsage: jstat -help|-options jstat -&lt;option&gt; [-t] [-h&lt;lines&gt;] &lt;vmid&gt; [&lt;interval&gt; [&lt;count&gt;]]Definitions: &lt;option&gt; An option reported by the -options option &lt;vmid&gt; Virtual Machine Identifier. A vmid takes the following form: &lt;lvmid&gt;[@&lt;hostname&gt;[:&lt;port&gt;]] Where &lt;lvmid&gt; is the local vm identifier for the target Java virtual machine, typically a process id; &lt;hostname&gt; is the name of the host running the target Java virtual machine; and &lt;port&gt; is the port number for the rmiregistry on the target host. See the jvmstat documentation for a more complete description of the Virtual Machine Identifier. &lt;lines&gt; Number of samples between header lines. &lt;interval&gt; Sampling interval. The following forms are allowed: &lt;n&gt;["ms"|"s"] Where &lt;n&gt; is an integer and the suffix specifies the units as milliseconds("ms") or seconds("s"). The default units are "ms". &lt;count&gt; Number of samples to take before terminating. -J&lt;flag&gt; Pass &lt;flag&gt; directly to the runtime system. jstat 命令格式： 12// 每 250 毫秒查询一次进程 2764 垃圾收集状态，一共查询 20 次λ jstat -gc 2764 250 20 选项 作用 -class 监视类装载、卸载数量、总空间以及类装载所耗费的时间 -gc 监视 Java 堆状况，包括 Eden 区、两个 survivor 区、老年代、永久代等的容量、已用空间、GC 空间合计等信息 -gccapacity 监视内容与 -gc 基本相同，但输出主要关注 Java 堆各个区域使用到的最大、最小空间 -gcutil 监视内容与 -gc 基本相同，但输出主要关注已使用空间占总空间的百分比 -gccause 与 -gcutil 功能一样，但是会额外输出导致上一次 GC 产生的原因 -gcnew 监视新生代 GC 状况 -gcnewcapacity 监视内容与 -gcnew 基本相同，但输出主要关注使用到的最大、最小空间 -gcold 监视老年代 GC 状况 -gcoldcapacity 监视内容与 -gcold 基本相同，但输出主要关注使用到的最大、最小空间 -gcpermcapacity 输出永久代使用到的最大、最小空间（JDK1.8 已废弃） -gcmetacapacity 输出元数据空间使用到的最大、最小空间（JDK1.8 已废弃） -compiler 输出 JIT 编译器编译过的方法、耗时等信息 -printcompilation 输出已经被 JIT 编译的方法 特别说明：jstat 监视选项众多，由于版本原因无法逐一演示，感兴趣的朋友可以参考博客 《jvm 性能调优工具之 jstat》 jinfo：Java 配置信息工具jinfo（Configuration Info for Java）的作用是实时地查看和调整虚拟机各项参数。使用 jps 命令的 -v 参数可以查看虚拟机启动时显示指定的参数列表，但如果想知道未被显示指定的参数的系统默认值，就只能使用 jinfo 的 -flag 选项进行查询了。 查看 jinfo 的帮助信息： 1234567891011121314151617[root@VM_24_98_centos jvm]# jinfo -helpUsage: jinfo [option] &lt;pid&gt; (to connect to running process) jinfo [option] &lt;executable &lt;core&gt; (to connect to a core file) jinfo [option] [server_id@]&lt;remote server IP or hostname&gt; (to connect to remote debug server)where &lt;option&gt; is one of: -flag &lt;name&gt; to print the value of the named VM flag -flag [+|-]&lt;name&gt; to enable or disable the named VM flag -flag &lt;name&gt;=&lt;value&gt; to set the named VM flag to the given value -flags to print VM flags -sysprops to print Java system properties &lt;no option&gt; to print both of the above -h | -help to print this help message jinfo 命令格式： 12// jinfo 查询进程 2764 虚拟机各项参数λ jinfo -flags 2764 jmap：Java 内存映像工具jmap（Memory Map for Java）命令用于生成堆转储快照（一般称为 heapdump 或者 dump 文件）。如果不使用 jmap 命令，可以使用 -XX:+HeapDumpOnOutOfMemoryError 参数，让虚拟机在 OOM 异常出现之后自动生成 dump 文件。 jmap 的作用并不仅仅是为了获取 dump 文件，它还可以查询 finalize 执行队列、Java 堆和永久代的详细信息，如空间使用率、当前用的是哪种收集器等。 查看 jmap 的帮助信息： 1234567891011121314151617181920212223242526272829[root@VM_24_98_centos ~]# jmap -helpUsage: jmap [option] &lt;pid&gt; (to connect to running process) jmap [option] &lt;executable &lt;core&gt; (to connect to a core file) jmap [option] [server_id@]&lt;remote server IP or hostname&gt; (to connect to remote debug server)where &lt;option&gt; is one of: &lt;none&gt; to print same info as Solaris pmap -heap to print java heap summary -histo[:live] to print histogram of java object heap; if the "live" suboption is specified, only count live objects -clstats to print class loader statistics -finalizerinfo to print information on objects awaiting finalization -dump:&lt;dump-options&gt; to dump java heap in hprof binary format dump-options: live dump only live objects; if not specified, all objects in the heap are dumped. format=b binary format file=&lt;file&gt; dump heap to &lt;file&gt; Example: jmap -dump:live,format=b,file=heap.bin &lt;pid&gt; -F force. Use with -dump:&lt;dump-options&gt; &lt;pid&gt; or -histo to force a heap dump or histogram when &lt;pid&gt; does not respond. The "live" suboption is not supported in this mode. -h | -help to print this help message -J&lt;flag&gt; to pass &lt;flag&gt; directly to the runtime system jmap 命令格式： 12345// jinfo 查询进程 2764 Java 堆详细信息λ jmap -heap 2764// jinfo 生成进程 2764 Java 堆转储快照λ jmap -dump:live,format=b,file=file.dump 2764 选项 作用 -dump 生成 Java 堆转储快照。格式为：-dump:[live,]format=b,file=&lt;filename>，其中 live 子参数说明是否只 dump 出存活的对象 -finalizerinfo 显示在 F-Queue 中等待 Finalizer 线程执行 finalize 方法的对象。只有在 Linux/Solaris 平台下有效 -heap 显示 Java 堆详细信息，如使用哪种回收器、参数配置、分代状况等。只有在 Linux/Solaris 平台下有效 -histo 显示堆中对象统计信息，包括类、实例数量、合计容量 -histo:live 与 -histo:live 功能一样，在统计之前 JVM 会先触发一次 FULL GC，线上慎用 -permstat 以 ClassLoader 为统计口径显示永久代内存状态。只有在 Linux/Solaris 平台下有效 -F 当虚拟机进程对 -dump 选项没有响应时，可使用这个选项强制生成 dump 快照。只有在 Linux/Solaris 平台下有效 jhat：虚拟机堆转储快照分析工具Sun JDK 提供 jhat（JVM Heap Analysis Tool）命令与 jmap 搭配使用，来分析 jmap 生成的堆转快照。jhat 内置了一个微型的 HTTP/HTML 服务器，生成 dump 文件的分析结果后，可以在游览器中查看。 jhat 的分析功能相对来说比较简陋，在现实工作中，我们一般会使用 VisualVM、Eclipse Memory Analyzer、IBM HeapAnalyzer 等专业用于分析 dump 文件的工具。 查看 jhat 的帮助信息： 1234567891011121314151617181920212223242526[root@VM_24_98_centos ~]# jhat -helpUsage: jhat [-stack &lt;bool&gt;] [-refs &lt;bool&gt;] [-port &lt;port&gt;] [-baseline &lt;file&gt;] [-debug &lt;int&gt;] [-version] [-h|-help] &lt;file&gt; -J&lt;flag&gt; Pass &lt;flag&gt; directly to the runtime system. For example, -J-mx512m to use a maximum heap size of 512MB -stack false: Turn off tracking object allocation call stack. -refs false: Turn off tracking of references to objects -port &lt;port&gt;: Set the port for the HTTP server. Defaults to 7000 -exclude &lt;file&gt;: Specify a file that lists data members that should be excluded from the reachableFrom query. -baseline &lt;file&gt;: Specify a baseline object dump. Objects in both heap dumps with the same ID and same class will be marked as not being "new". -debug &lt;int&gt;: Set debug level. 0: No debug output 1: Debug hprof file parsing 2: Debug hprof file parsing, no server -version Report version number -h|-help Print this help and exit &lt;file&gt; The file to readFor a dump file that contains multiple heap dumps,you may specify which dump in the fileby appending "#&lt;number&gt;" to the file name, i.e. "foo.hprof#3".All boolean options default to "true" jmap 命令格式： 12// jhat 分析 file.dump，屏幕显示“Server is ready”的提示后，用户在游览器输入 http://localhost:7000/ 就可以看到分析结果λ jhat file.dump jstack：Java 堆栈跟踪工具jstack（Stack Trace for Java）命令用于生成虚拟机当前时刻的线程快照（一般称为 threaddump 或者 javacore 文件）。线程快照就是当前虚拟机内每一条线程正在执行的方法堆栈的集合，生成线程快照的主要目的是定位线程出现长时间停顿的原因，如线程间死锁、死循环、请求外部资源导致的长时间等待都是导致线程长时间停顿的常见原因。 查看 jstack 的帮助信息： 12345678910111213141516[root@VM_24_98_centos ~]# jstack -helpUsage: jstack [-l] &lt;pid&gt; (to connect to running process) jstack -F [-m] [-l] &lt;pid&gt; (to connect to a hung process) jstack [-m] [-l] &lt;executable&gt; &lt;core&gt; (to connect to a core file) jstack [-m] [-l] [server_id@]&lt;remote server IP or hostname&gt; (to connect to a remote debug server)Options: -F to force a thread dump. Use when jstack &lt;pid&gt; does not respond (process is hung) -m to print both java and native frames (mixed mode) -l long listing. Prints additional information about locks -h or -help to print this help message jstack 命令格式： 12// jstack 查看进程 2764 线程快照λ jstack -l 2764 选项 作用 -F 当正常输出的请求不被响应时，强制输出线程堆栈 -l 除堆栈外，显示关于锁的附加信息 -m 如果调用本地方法的话，可以显示 C/C++ 的堆栈 JDK 可视化工具JDK 中除了提供大量的命令行工具外，还有两个功能强大的可视化工具：JConsole 和 VisualVM。 JConsole：Java 监视与管理控制台JConsole（Java Monitoring and Management Console）是一种基于 JMX 的可视化监视、管理工具，它管理部分的功能是针对 JMX MBean 进行管理。 特殊说明：要对 Java 进程进行远程监控，在启动它的时候需要启用 JMX，对于 Java 进程开启远程调试可以参考博客《Java - jmx远程调优》 VisualVM：多合一故障处理工具VisualVM（All-in-One Java Troubleshooting Tool）是到目前为止随 JDK 发布的功能最强大的运行监视和故障处理程序。VisualVM 除了运行监视、故障处理外，还提供了很多其他方面你的功能，如性能分析（Profiling）。VisualVM 的性能分析功能甚至比起 Jprofiler、YourKit 等专业且收费的 Profiling 工具都不会逊色多少，而且 VisualVM 不需要被监视的程序基于特殊 Agent 运行，因此它对对应用程序的实际性能的影响很小，使得它可以直接应用在生产环境中。 调优案例分析与实战模拟环境：Centos7 1 核 2GB，Java 8 JVM 调优之 jstack 找出最耗 CPU 的线程并定位代码笔者之前在一家互联网公司从事爬虫业务，在解析 HTML 时，经常由于网站返回的 HTML 网页结构不完整而导致解析框架死循环，从而导致 CPU 飚高，系统运行缓慢。 场景模拟场景模拟：在线上的环境中，一般 CPU 飙高极大的可能性是出现了死循环了。因此我们通过模拟死循环的方式模拟 CPU 飚高的情况，然后通过 jstack 找出最耗 CPU 的线程并定位代码。启动程序后，发现该程序 CPU 直线飙高，直接到达 100% 根本没有要下降的趋势，并且系统平均负载也直线飙高至 3.79，导致系统缓慢。 123456789101112131415public class ExceptionHandler &#123; public static void main(String[] args) &#123; int nThreads = 3; Executor executor = Executors.newFixedThreadPool(nThreads); for (int i = 0; i &lt; nThreads; i++) &#123; executor.execute(() -&gt; &#123; long temp = 0; while (true) &#123; temp++; &#125; &#125;); &#125; &#125;&#125; 排查思路（1） jstack 找出最耗 CPU 的线程并定位代码 ① 通过 top 命令找到占用 CPU 最高的 pid[进程 ID]，定位到 pid 是 12870 ② 通过 top -Hp pid 查看该进程中占用 CPU 过高的 tid[线程 id]，定位到 tid 分别为 12894、12895、12896 ③ 通过 printf “0x%x\n” tid 把线程 id 转化为十六进制，转换后的十六进制 tid 分别为 0x325e、0x325f、0x3260 ④ 通过 jstack pid |grep tid -A 30 定位线程堆栈信息，这里的 tid 指的是转换后的十六进制 tid，定位到导致 CPU 飚高的代码为 ExceptionHandler 类 27 行处，发现里面有一个死循环。 （2） 通过在线可视化分析工具分析 threaddump fastthread.io 是一个在线线程日志分析网站，科学上网打开速度会更快。定位线程出现长时间停顿的原因，如线程间死锁、死循环、请求外部资源导致的长时间等待都是导致线程长时间停顿的常见原因。比较好的是它会提供一些优化的建议，可以作为参考，而且各个部分的分析也比较详细。 ① 通过 top 命令找到占用 CPU 最高的 pid[进程 ID]，定位到 pid 是 12870 ② 通过 jstack -l pid &gt; file-path 抓取 thread dump 文件，即 jstack -l 12870 &gt; threaddump-1576050339106.tdump ③ 将 thread dump 文件上传至 fastthread 网站，查看分析结果 JVM 调优之 jstack 找出死锁线程并定位代码Deadlock：死锁线程，一般指多个线程调用间，进入相互资源占用，导致一直等待无法释放的情况。由于锁使用不当，导致多个线程进入死锁状态，从而导致系统整体比较缓慢。 场景模拟123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class ExceptionHandler &#123; static final String obj1 = "obj1"; static final String obj2 = "obj2"; public static void main(String[] args) &#123; Thread a = new Thread(new Lock1()); Thread b = new Thread(new Lock2()); a.start(); b.start(); &#125; public static class Lock1 implements Runnable &#123; @Override public void run() &#123; try &#123; System.out.println("Lock1 running"); while (true) &#123; synchronized (ExceptionHandler.obj1) &#123; System.out.println("Lock1 lock obj1"); // 获取obj1后先等一会儿，让Lock2有足够的时间锁住obj2 TimeUnit.SECONDS.sleep(3); synchronized (ExceptionHandler.obj2) &#123; System.out.println("Lock1 lock obj2"); &#125; &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; public static class Lock2 implements Runnable &#123; @Override public void run() &#123; try &#123; System.out.println("Lock2 running"); while (true) &#123; synchronized (ExceptionHandler.obj2) &#123; System.out.println("Lock2 lock obj2"); // 获取obj2后先等一会儿，让Lock1有足够的时间锁住obj1 TimeUnit.SECONDS.sleep(3); synchronized (ExceptionHandler.obj1) &#123; System.out.println("Lock2 lock obj1"); &#125; &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 排查思路（1） jstack 分析线程运行状况，找出死锁线程并定位代码 ① 通过 top 命令找到占用 需要分析的 pid[进程 ID]，定位到 pid 是 7120 ② 通过 jstack pid 定位线程堆栈信息，通过分析线程快照，可以很轻松的发现死锁状况 （2） 通过在线可视化分析工具分析 threaddump ① 通过 top 命令找到占用 CPU 最高的 pid[进程 ID]，定位到 pid 是 7120 ② 通过 jstack -l pid &gt; file-path 抓取 thread dump 文件，即 jstack -l 7120 &gt; threaddump-1576050339106.tdump ③ 将 thread dump 文件上传至 fastthread 网站，查看分析结果 JVM 调优之 jstack 找出 Full GC 次数过多并定位代码Full GC 次数过多，这种情况是最容易出现的，尤其是新功能上线时。对于 Full GC 较多的情况，其主要有如下两个特征：1、线上多个线程的 CPU 都超过了 100%，通过 jstack 命令可以看到这些线程主要是垃圾回收线程；2、通过 jstat 命令监控 GC 情况，可以看到 Full GC 次数非常多，并且次数在不断增加。 排查思路（1） jstack 分析线程运行状况，找出 Full GC 次数过多原因并定位代码 ① 首先我们通过 top 命令查看当前 CPU 消耗过高的进程是哪个，从而得到进程 id；然后通过 top -Hp &lt;pid> 来查看该进程中有哪些线程 CPU 过高，一般超过 80% 就是比较高的，80% 左右是合理情况。这样我们就能得到 CPU 消耗比较高的线程 id。接着通过该线程 id 的十六进制表示在 jstack 日志中查看当前线程具体的堆栈信息 ② 通过 jstack pid |grep tid -A 30 定位线程堆栈信息，这里的 tid 指的是转换后的十六进制 tid，定位到导致 CPU 飚高的线程为 “VM Thread”，而 VM Thread 指的就是垃圾回收的线程。这里我们基本上可以确定，当前系统缓慢的原因主要是垃圾回收过于频繁，导致 GC 停顿时间较长 ③ 通过 jstat -gcutil pid 1000 10 查看 GC 的情况，可以看到，这里 FGC 指的是 Full GC 数量，若 FGC 过高，可能是由于内存溢出导致的系统缓慢，也可能是代码或者第三方依赖的包中有显示的 System.gc() 调用 ④ 通过分析 dump 文件，确定由于内存溢出导致 Full GC 次数过多还是由于代码或者第三方依赖的包中有显示的 System.gc() 调用导致 Full GC 次数过多 （2） 通过在线可视化分析工具分析 gc.log gceasy.io 是一个在线 GC 日志分析工具，科学上网打开速度会更快。gceasy 可帮助您可帮助您分析程序运行时 GC 情况，您可以根据分析结果及时优化程序。 ① 通过 -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:gc.log 命令获取 gc 日志 ② 将 gc.log 文件上传至 gceasy 网站，查看分析结果 总结对于 Full GC 次数过多，主要有以下两种原因： 代码中一次获取了大量的对象，导致内存溢出，此时可以通过 eclipse 的 mat 工具查看内存中有哪些对象比较多。 内存占用不高，但是 Full GC 次数还是比较多，此时可能是显示的 System.gc() 调用导致 GC 次数过多，这可以通过添加 - XX:+DisableExplicitGC 来禁用 JVM 对显示 GC 的响应。 JVM 调优之 jmap 找出内存泄漏并定位代码同样是笔者之前所在的爬虫业务，有个网站改版后，登录时采用 RSA 加密方式，笔者根据网站的加密方式改版后项目重新上线后，发现项目内存使用率每天都会增加一点，对于一个长期稳定运行 Java 项目而言，出现这种情况一般都有可能是出现了内存泄露。 场景模拟场景模拟：在线上的环境中，一个长期稳定运行的项目，内存使用率每天都增加，一般情况就是出现了内存泄露。而笔者所说的场景就是，网站采用 RSA 对网站的登录账号、密码进行加密，Java 默认的 RSA 实现是 “RSA/None/PKCS1Padding”（即 Cipher cipher = Cipher.getInstance(“RSA”);）；而该网站采用的 RSA 实现是 “RSA/None/NoPadding”（即 Cipher cipher = Cipher.getInstance(“RSA”, new org.bouncycastle.jce.provider.BouncyCastleProvider());）。项目长时间运行，就会出现 JceSecurity 占用的内存越来越多，而且不会释放。 1234567891011121314151617181920212223242526public class ExceptionHandler &#123; public static final String DEFAULT_PUBLIC_KEY = "MIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQCO/qrEGrKGX06KP9Aks1BeP/2RTQkg/WXUQoCbtKQOqQQGY5N/ekCSW0Ow9ZAiEl3GMG/E7elNvEPV17bpRP3k+PFB0YxhgqOPLqxSN+57PUSUJU1rTkVY8mcu3eWrrGf2xpud1eXKEW3YeI95uCPyGPq4sO3NtHJY1xUTaFhG7QIDAQAB"; public static void main(String[] args) throws Exception &#123; while (true) &#123; System.out.println(encrypt(DEFAULT_PUBLIC_KEY, new org.bouncycastle.jce.provider.BouncyCastleProvider(), "morning")); &#125; &#125; /** * 使用publicKey来加密内容 * * @param publicKeyString 公钥key * @param provider 加密提供商 * @param content 内容 * @return byte[] * @throws Exception */ public static String encrypt(String publicKeyString, Provider provider, String content) throws Exception &#123; Cipher cipher = Cipher.getInstance("RSA", provider); PublicKey publicKey = KeyFactory.getInstance("RSA", provider).generatePublic(new X509EncodedKeySpec(Base64.decodeBase64(publicKeyString))); cipher.init(Cipher.ENCRYPT_MODE, publicKey); return Base64.encodeBase64String(cipher.doFinal(content.getBytes())); &#125;&#125; 排查思路（1） MAT 找出内存泄漏并定位代码 Eclipse Memory Analyzer Tool（MAT）是一个强大的基于 Eclipse 的内存分析工具，可以帮助我们找到内存泄露，减少内存消耗。MAT 是有两种安装方式的：一种安装方式是将 MAT 当做 eclipse 的插件进行安装：启动 Eclipse –&gt; Help –&gt; Eclipse Marketplace，然后搜索 Memory Analyzer，安装，重启 eclipse 即可；另外一种安装方式是将 MAT 作为一个独立的软件进行安装：去 官网，根据操作系统版本下载最新的 MAT。下载后解压就可以运行了。 ① 通过 top 命令找到占用内存使用率持续增加的 pid[进程 ID]，定位到 pid 是 12870 ② 通过 jmap -dump:[live,]format=b,file=&lt;filename> &lt;pid> 获取堆转储文件，即 jmap -dump:live,format=b,file=heapdump-1576054225319.hprof 12870 ③ 将 heap dump 文件导入 MAT，查看分析结果 ④ 加载后首页如下图，在首页上比较有用的是 Histogram 和 Leak Suspects。由下图看出这个类 javax.crypto.JceSecurity 占用 896.3 MB，表示这其中很多不能够被回对象的对象 ⑤ 根据 Leak Suspects 快速查看泄露的可疑点，在 Leak Suspects 页面会给出可能的内存泄露，点击 Details 进入详情页面。在详情页面 Shortest Paths To the Accumulation Point 表示 GC root 到内存消耗聚集点的最短路径，如果某个内存消耗聚集点有路径到达 GC root，则该内存消耗聚集点不会被当做垃圾被回收。由下图可以看到大量的 BouncyCastleProvider 对象没有被垃圾回收器回收，占用了大量内存空间。 （3） 通过在线可视化分析工具分析 heaphero heaphero.io 是一个在线内存分析工具，科学上网打开速度会更快。heaphero 可帮助您查找内存泄漏并减少内存消耗，运行报告以自动提取泄漏嫌疑者，主要展示项有：堆统计，大对象，字符串重复，低效率对象，线程数，及优化建议等。 ① 通过 top 命令找到占用内存使用率持续增加的 pid[进程 ID]，定位到 pid 是 12870 ② 通过 jmap -dump:[live,]format=b,file=&lt;filename> &lt;pid> 获取堆转储文件，即 jmap -dump:live,format=b,file=heapdump-1576054225319.hprof 12870 ③ 将 heap dump 文件上传至 heaphero 网站，查看分析结果 解决方式（1）修改 provider 指定方式：Security.addProvider(new BouncyCastleProvider()) 1234567891011121314151617181920212223242526272829303132public class ExceptionHandler &#123; public static final String DEFAULT_PUBLIC_KEY = "MIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQCO/qrEGrKGX06KP9Aks1BeP/2RTQkg/WXUQoCbtKQOqQQGY5N/ekCSW0Ow9ZAiEl3GMG/E7elNvEPV17bpRP3k+PFB0YxhgqOPLqxSN+57PUSUJU1rTkVY8mcu3eWrrGf2xpud1eXKEW3YeI95uCPyGPq4sO3NtHJY1xUTaFhG7QIDAQAB"; public static void main(String[] args) throws Exception &#123; while (true) &#123; System.out.println(encrypt(DEFAULT_PUBLIC_KEY, BouncyCastleProvider.PROVIDER_NAME, "morning")); &#125; &#125; static &#123; if (Security.getProvider(BouncyCastleProvider.PROVIDER_NAME) == null) &#123; Security.addProvider(new BouncyCastleProvider()); &#125; &#125; /** * 使用publicKey来加密内容 * * @param publicKeyString 公钥key * @param provider 加密提供商 * @param content 内容 * @return byte[] * @throws Exception */ public static String encrypt(String publicKeyString, String provider, String content) throws Exception &#123; Cipher cipher = Cipher.getInstance("RSA", provider); PublicKey publicKey = KeyFactory.getInstance("RSA", provider).generatePublic(new X509EncodedKeySpec(Base64.decodeBase64(publicKeyString))); cipher.init(Cipher.ENCRYPT_MODE, publicKey); return Base64.encodeBase64String(cipher.doFinal(content.getBytes())); &#125;&#125; （2）把 BouncyCastleProvider 改成单例模式 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public class ExceptionHandler &#123; public static final String DEFAULT_PUBLIC_KEY = "MIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQCO/qrEGrKGX06KP9Aks1BeP/2RTQkg/WXUQoCbtKQOqQQGY5N/ekCSW0Ow9ZAiEl3GMG/E7elNvEPV17bpRP3k+PFB0YxhgqOPLqxSN+57PUSUJU1rTkVY8mcu3eWrrGf2xpud1eXKEW3YeI95uCPyGPq4sO3NtHJY1xUTaFhG7QIDAQAB"; public static void main(String[] args) throws Exception &#123; while (true) &#123; System.out.println(encrypt(DEFAULT_PUBLIC_KEY, BouncyCastleProviderEnum.INSTANCE.getBouncyCastleProvider(), "morning")); &#125; &#125; static &#123; if (Security.getProvider(BouncyCastleProvider.PROVIDER_NAME) == null) &#123; Security.addProvider(new BouncyCastleProvider()); &#125; &#125; /** * 使用publicKey来加密内容 * * @param publicKeyString 公钥key * @param provider 加密提供商 * @param content 内容 * @return byte[] * @throws Exception */ public static String encrypt(String publicKeyString, Provider provider, String content) throws Exception &#123; Cipher cipher = Cipher.getInstance("RSA", provider); PublicKey publicKey = KeyFactory.getInstance("RSA", provider).generatePublic(new X509EncodedKeySpec(Base64.decodeBase64(publicKeyString))); cipher.init(Cipher.ENCRYPT_MODE, publicKey); return Base64.encodeBase64String(cipher.doFinal(content.getBytes())); &#125; /** * 枚举实现单例模式. * 避免项目中出现反复 new BouncyCastleProvider() 导致内存泄漏. */ public enum BouncyCastleProviderEnum &#123; /** * 枚举类实例 */ INSTANCE; /** * 枚举类实例变量 */ private BouncyCastleProvider bouncyCastleProvider; /** * 枚举类构造方法，默认为private */ BouncyCastleProviderEnum() &#123; bouncyCastleProvider = new BouncyCastleProvider(); &#125; /** * 枚举类成员方法. * 获取产生的bouncyCastleProvider变量. * * @return BouncyCastleProvider */ public BouncyCastleProvider getBouncyCastleProvider() &#123; return bouncyCastleProvider; &#125; &#125;&#125; 参考博文[1]. jvm 性能调优工具之 jstat[2]. JDK的可视化工具系列 (四) JConsole、VisualVM[3]. 使用Eclipse Memory Analyzer Tool（MAT）分析线上故障(一) - 视图&amp;功能篇[4]. 一道必备面试题：系统CPU飙高和GC频繁，如何排查？ 深入理解 Java 虚拟机系列 深入理解 Java 虚拟机（一）：Java 内存区域与内存溢出异常 深入理解 Java 虚拟机（二）：JVM 垃圾收集器 深入理解 Java 虚拟机（三）：内存分配与回收策略 深入理解 Java 虚拟机（四）：Jvm 性能监控与调优]]></content>
      <categories>
        <category>Jvm</category>
      </categories>
      <tags>
        <tag>Jvm</tag>
        <tag>性能监控</tag>
        <tag>线上问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git 在团队中的最佳实践（三）：如何优雅的使用 Git？]]></title>
    <url>%2Farchives%2F97ab96a0.html</url>
    <content type="text"><![CDATA[前言在本系列的前两篇博文中，笔者对 Git 以及 Git flow 进行了大致的介绍，相信各位读者已经对 Git 有了大致的了解。但是，在我们的日常工作中使用 Git 时常会遇到的各种突发状况，那么我们应该怎么合理的应对这些状况呢？俗话说，无规矩不成方圆，在团队协作中，如何规范 Git Commit 呢？本文将针对以上问题展开讨论，探讨一下在日常工作中，我们应该如何优雅的使用 Git？ 你可能会忽略的 Git 提交规范无规矩不成方圆，编程也一样。如果在团队协作中，大家都张扬个性，那么代码将会是一团糟，好好的项目就被糟践了。不管是开发还是日后维护，都将是灾难。对于 Git Commit 同样如此，统一 Git Commit 规范可以方便管理团队代码，方便后续进行 code review 以及生成 change log；统一 Git Commit 规范容易理解提交的信息。 分支规范根据 Git flow 工作流分支模型将我们开发分支规范为五大分支： Master 分支 - 生产分支：最为稳定功能比较完整的随时可发布的代码，即代码开发完成，经过测试，没有明显的 bug，才能合并到 master 中。 Develop 分支 - 开发分支：用作平时开发的主分支，并一直存在，永远是功能最新最全的分支，所有的 feature、release 分支都是从 develop 分支上拉的。 Feature 分支 - 功能分支：这个分支主要是用来开发新的功能，一旦开发完成，通过测试没问题，我们合并回 develop 分支进入下一个 release。 Release 分支 - 发布分支：用于发布准备的专门分支。当开发进行到一定程度，或者说快到了既定的发布日，可以发布时，建立一个 release 分支并指定版本号（可以在 finish 的时候添加）。开发人员可以对 release 分支上的代码进行集中测试和修改 bug。全部完成经过测试没有问题后，将 release 分支上的代码合并到 master 分支和 develop 分支。 Hotfix 分支 - 热修复分支：用于修复线上代码的 bug。从 master 分支上拉，完成 hotfix 后，打上 tag 我们合并回 master 和 develop 分支。 标签规范采用三段式: v版本. 里程碑. 序号，例如 v1.2.1 项目结构发生重大修改，增加第一个数字 发布新的功能，增加第二个数字 修复项目中的 bug，修改第三个数字 Git Commit 信息规范目前一般采用 Angular 的提交信息规范：信息分为 Header、Body、Footer 三部分 例子： 1refactor: Restructure SQLRecognizer and UndoExecutor (#1883) HeaderHeader 信息分为三部分 type(scope): subject type（必须），用于说明 Git 提交信息的类别，有以下几个分类 Type 说明 feat 新增功能 fix 修复 bug docs 修改文档 refactor 重构代码，未新增任何功能或修复任何 bug build 改变构建流程、新增依赖库 style 仅对样式做出修改（如空格和代码缩进等，不对逻辑进行修改） perf 改善性能的修改 chore 非 src 或 test 下代码的修改 test 测试用例的修改 ci 自动化流程配置修改 revert 回滚到上一个版本 scope（可选），用于说明 commit 的影响范围，比如数据层、控制层、视图层等等，视项目不同而不同。 subject（必须），commit 的信息主题，尽量言简意赅，说明提交代码的主要变化。 Body对本次提交的详细描述。 Footer 不兼容变动（需要说明变动信息） 关闭issue（需要输入issue信息） 使用 Git 时常会遇到的各种突发状况git stash【1】场景重现 one：当正在 feature 分支上开发某个新功能，这时项目中出现一个 bug，需要紧急修复，但是正在开发的内容只是完成一半，还不想提交，这时可以用 git stash 命令将修改的内容保存至堆栈区，然后顺利切换到 hotfix 分支进行 bug 修复，修复完成后，再次切回到 feature 分支，从堆栈中恢复刚刚保存的内容。 【2】场景重现 two：由于疏忽，本应该在 feature 分支开发的内容，却在 develop 上进行了开发，需要重新切回到 feature 分支上进行开发，可以用 git stash 将内容保存至堆栈中，切回到 feature 分支后，再次恢复内容即可。 1234567891011121314151617181920212223# 1. 创建 feature 分支$ git flow feature start some-feature# 2.在 feature 分支上开发某个新功能...# 3. git stash会把所有未提交的修改（包括暂存的和非暂存的）都保存起来，用于后续恢复当前工作目录，当前的工作目录就干净了。$ git stash save "feat: add user (#1983)" # 4. 创建 hotfix 分支$ git flow hotfix start 0.1.1# 5. 在 hotfix 分支上进行 bug 修复...# 6. 完成 bug 修复，提交 hotfix 分支$ git flow hotfix finish --no-ff 0.1.1# 7. 切换到 feature 分支$ git checkout some-feature# 8. 恢复工作进度到工作区，此命令的 stash@&#123;num&#125; 是可选项，在多个工作进度中可以选择恢复，不带此项则默认恢复最近的一次进度相当于 git stash pop stash@&#123;0&#125;$ git stash pop stash@&#123;num&#125; git stash 常用命令指南 1234567891011121314151617181920# 保存，save为可选项，message为本次保存的注释$ git stash [save message]# 所有保存的记录列表$ git stash list# 恢复，num是可选项，通过git stash list可查看具体值。只能恢复一次$ git stash pop stash@&#123;num&#125;# 恢复，num是可选项，通过git stash list可查看具体值。可回复多次$ git stash apply stash@&#123;num&#125;# 删除某个保存，num是可选项，通过git stash list可查看具体值$ git stash drop stash@&#123;num&#125;# 查看堆栈中最新保存的 stash 和当前目录的差异，num是可选项，通过git stash list可查看具体值$ git stash show stash@&#123;num&#125;# 删除所有保存$ git stash clear git rebase不知怎么，git rebase 命令被赋予了一个神奇的污毒声誉，初学者应该远离它，但它实际上可以让开发团队在使用时更加轻松。 Rebase 的黄金法则：git rebase 的黄金法则是永远不要在公共分支上使用它。 【1】场景重现 one：当你在功能分支上开发新 feature 时，然后另一个团队成员在 master 分支提交了新的 commits，这会发生什么？这会导致分叉的历史记录，对于这个问题，使用 Git 作为协作工具的任何人来说都应该很熟悉。现在，假设在 master 分支上的新提交与你正在开发的 feature 相关。需要将新提交合并到你的 feature 分支中，你可以有两个选择：merge 或者 rebase。 Merge 方式：最简单的方式是通过 git merge 命令将 master 分支合并到 feature 分支中 12345$ git checkout feature$ git merge master# 或者将其浓缩为一行命令$ git merge feature master 这会在 feature 分支中创建一个新的 merge commit，它将两个分支的历史联系在一起。使用 merge 是很好的方式，因为它是一种 非破坏性的 操作，现有分支不会以任何方式被更改；另一方面，这也意味着 feature 分支每次需要合并上游更改时，它都将产生一个额外的合并提交。如果master 提交非常活跃，这可能会严重污染你的 feature 分支历史记录。尽管可以使用高级选项 git log 缓解此问题，但它可能使其他开发人员难以理解项目的历史记录。 Rebase 方式：作为 merge 的替代方法，你可以使用以下命令将 master 分支合并到 feature分支上 12345$ git checkout feature$ git rebase master# 或者将其浓缩为一行命令$ git rebase master feature 这会将整个 feature 分支移动到 master 分支的顶端，从而有效地整合了所有 master 分支上的提交。但是，与 merge 提交方式不同，rebase 通过为原始分支中的每个提交创建全新的 commits 来 重写项目历史记录。 rebase 的主要好处是可以获得更清晰的项目历史。首先，它消除了 git merge 所需的不必要的合并提交；其次，正如你在上图中所看到的，rebase 会产生完美线性的项目历史记录，你可以在 feature 分支上没有任何分叉的情况下一直追寻到项目的初始提交。这样可以通过命令 git log，git bisect 和 gitk 更容易导航查看项目。 【2】场景重现 two：当你在功能分支上开发新 feature 时，多次提交了记录，这时，想要在在合并 feature 分支到 master 之前清理其杂乱的历史记录。 交互式 rebase 使你有机会在将 commits 移动到新分支时更改这些 commits。这比自动 rebase 更强大，因为它提供了对分支提交历史的完全控制。 要使用交互式 rebase，需要使用 git rebase 和 -i 选项： 12$ git checkout feature$ git rebase -i master 这将打开一个文本编辑器，列出即将移动的所有提交： 123pick 33d5b7a Message for commit #1pick 9480b3d Message for commit #2pick 5c67e61 Message for commit #3 此列表准确定义了执行 rebase 后分支的外观。通过更改 pick命令或重新排序条目，你可以使分支的历史记录看起来像你想要的任何内容。例如，如果第二次提交 fix 了第一次提交中的一个小问题，您可以使用以下 fixup 命令将它们浓缩为一个提交： 123pick 33d5b7a Message for commit #1fixup 9480b3d Message for commit #2pick 5c67e61 Message for commit #3 保存并关闭文件时，Git将根据您的指示执行 rebase，从而产生如下所示的项目历史记录： 消除这种无意义的提交使你的功能历史更容易理解。这是 git merge 根本无法做到的事情。至于 commits 条目前的 pick（ 保留该 commit ）、fixup（ 将该 commit 和前一个 commit 合并，但我不要保留该提交的注释信息 ）、squash（ 将该 commit 和前一个 commit 合并 ） 等命令，在 git 目录执行 git rebase -i 即可查看到，大家按需重排或合并提交即可，注释说明非常清晰，在此不做过多说明。 git cherry-pickgit cherry-pick 可以理解为” 挑拣” 提交，它会获取某一个分支的单笔提交，并作为一个新的提交引入到你当前分支上。 当我们需要在本地合入其他分支的提交时，如果我们不想对整个分支进行合并，而是只想将某一次提交合入到本地当前分支上，那么就要使用 git cherry-pick 了。 【1】场景重现 one：当正在 feature 分支上开发某个新功能，并且进行了多个提交。这时，你切到另外一个 feature 分支，想把之前 feature 分支上的某个提交复制过来，怎么办？这时候，神奇的 git cherry-pick 就闪亮的登场了。 1$ git cherry-pick c2 c4 git resetgit reset 通过把分支记录回退几个提交记录来实现撤销改动。你可以将这想象成“改写历史”。git reset 向上移动分支，原来指向的提交记录就跟从来没有提交过一样。git reset是指将 HEAD 指针指到指定提交，历史记录中不会出现放弃的提交记录。 【1】场景重现 one：有时候，我们用 Git 的时候有可能 commit 提交代码后，发现这一次 commit 的内容是有错误的，那么有两种处理方法：1、修改错误内容，再次 commit 一次；2、使用 git reset 命令撤销这一次错误的 commit。第一种方法比较直接，但会多次一次 commit 记录。而我个人更倾向第二种方法，错误的 commit 没必要保留下来。那么今天来说一下 git reset。 Git reset 命令有三个主要选项：git reset –soft; git reset –mixed; git reset –hard; git reset –soft：软合并 - 保留工作目录，并把重置 HEAD 所带来的新的差异放进暂存区。重置位置的同时，保留 working Tree 工作目录和 index 暂存区的内容，只让 repository 中的内容和 reset 目标节点保持一致，因此原节点和 reset 节点之间的【差异变更集】会放入 index 暂存区中 (Staged files)。所以效果看起来就是工作目录的内容不变，暂存区原有的内容也不变，只是原节点和 Reset 节点之间的所有差异都会放到暂存区中。 git reset –mixed： 混合合并（默认） - 保留工作目录， 并清空暂存区。重置位置的同时，只保留 Working Tree 工作目录的內容，但会将 Index 暂存区 和 Repository 中的內容更改和 reset 目标节点一致，因此原节点和 Reset 节点之间的【差异变更集】会放入 Working Tree 工作目录中。所以效果看起来就是原节点和 Reset 节点之间的所有差异都会放到工作目录中。 git reset –hard：强行合并 - 重置 stage 区和工作目录。重置位置的同时，直接将 working Tree 工作目录、 index 暂存区及 repository 都重置成目标 Reset 节点的內容， 所以效果看起来等同于清空暂存区和工作区。 git revertgit revert 撤销一个提交的同时会创建一个新的提交。这是一个安全的方法，因为它不会重写提交历史。 【1】场景重现 one：改完代码匆忙提交，上线发现有问题，怎么办？赶紧回滚。改完代码测试也没有问题，但是上线发现你的修改影响了之前运行正常的代码报错，必须回滚。 12# 撤销指定 commit 到当前 HEAD 之间所有的变化$ git revert [commit]..HEAD git revert 用于反转提交，用一个新提交来撤销某次提交，执行 git revert 命令时要求工作树必须是干净的。git revert 之后你再 git push 既可以把线上的代码更新。git revert 是放弃指定提交的修改，但是会生成一次新的提交，需要填写提交注释，以前的历史记录都在。 强烈推荐如果你不能很好的应用 Git，那么这里为你提供一个非常棒的 Git 在线练习工具 Learn Git Branching。 参考博文[1]. Oh Shit, Git!?! Git 在团队中的最佳实践系列 Git 在团队中的最佳实践（一）：Git 备忘清单 Git 在团队中的最佳实践（二）：如何正确使用 Git flow 工作流 Git 在团队中的最佳实践（三）：如何优雅的使用 Git？]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>实战指南</tag>
        <tag>规范</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式设计之美（二）：微服务架构下分布式事务解决方案]]></title>
    <url>%2Farchives%2F691d905d.html</url>
    <content type="text"><![CDATA[前言在微服务架构中，随着服务的逐步拆分，数据库私有已经成为共识，这也导致所面临的分布式事务问题成为微服务落地过程中一个非常难以逾越的障碍，但是目前尚没有一个完整通用的解决方案。 其实不仅仅是在微服务架构中，随着用户访问量的逐渐上涨，数据库甚至是服务的分片、分区、水平拆分、垂直拆分已经逐渐成为较为常用的提升瓶颈的解决方案，因此越来越多的原子操作变成了跨库甚至是跨服务的事务操作。最终结果是在对高性能、高扩展性、高可用性的追求的道路上，我们开始逐渐放松对一致性的追求，但是在很多场景下，尤其是账务，电商等业务中，不可避免的存在着一致性问题，使得我们不得不去探寻一种机制，用以在分布式环境中保证事务的一致性。 分布式事务有多种主流形态，包括： 基于 2PC（两阶段提交）实现的分布式事务 基于 3PC（三阶段提交）实现的分布式事务 基于 TCC（补偿事务）实现的分布式事务 基于 Saga 实现的分布式事务 基于可靠消息（事务消息）最终一致性实现的分布式事务 基于本地消息（本地消息表）最终一致性实现的分布式事务 接下来，本文将对这些形态的分布式事务进行剖析，然后讲解一下如何根据业务选择对应的分布式事务形态。 本地数据库事务数据库事务（transaction）是访问并可能操作各种数据项的一个数据库操作序列，这些操作要么全部执行，要么全部不执行，是一个不可分割的工作单位。事务由事务开始与事务结束之间执行的全部数据库操作组成。关系型数据库（例如：MySQL、SQL Server、Oracle 等）事务都有以下几个特性：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）和持久性（Durabilily），简称就是 ACID。 名称 描述 A Atomicity（原子性） 一个事务中的所有操作，要么全部完成，要么全部不完成，不会在中间某个环节结束。事务在执行过程中发生错误，会被回滚到事务开始前的状态，就像这个事务从来没有执行过一样。 C Consistency（一致性） 在事务开始之前和事务结束以后，数据库的完整性没有被破坏。 I Isolation（隔离性） 数据库允许多个并发事务同时对数据进行读写和修改的能力。隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。 D Durability（持久性） 事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 本地数据库事务操作也比较简单：开始一个事务，改变（插入，删除，更新）数据，然后提交事务（如果有异常时回滚事务）。MySQL 事务处理使用到 begin 开始一个事务，rollback 事务回滚，commit 事务确认。这里，事务提交后，通过 redo log 记录变更，通过 undo log 在失败时进行回滚，保证事务的原子性。 1234567891011121314151617181920212223Connection con = null;try &#123; // 工具类得到 connection 对象 con = JdbcUtils.getConnection(); // 关闭自动提交，开启事务 con.setAutoCommit(false); // 增、删、改 等操作 ... // 成功操作后提交事务 con.commit(); con.close();&#125; catch (Exception e) &#123; try &#123; // 如果有异常时回滚事务 con.rollback(); con.close(); &#125; catch (SQLException e1) &#123; e1.printStackTrace(); &#125;&#125; 但随着业务数据规模的快速发展，数据量越来越大，单库单表逐渐成为瓶颈。所以我们对数据库进行了水平拆分，将原单库单表拆分成数据库分片。分库分表之后，原来在一个数据库上就能完成的写操作，可能就会跨多个数据库，这就产生了跨数据库事务问题。 何时选择本地数据库事务？在条件允许的情况下，我们应该尽可能地使用单机事务，因为单机事务里，无需额外协调其他数据源，减少了网络交互时间消耗以及协调时所需的存储 IO 消耗，在修改等量业务数据的情况下，单机事务将会有更高的性能。但单机数据库由于业务逻辑解耦等因素进行了数据库垂直拆分或者由于单机数据库性能压力等因素进行了数据库水平拆分之后，数据分布于多个数据库，这时若需要对多个数据库的数据进行协调变更，则需要引入分布式事务。 分布式事务理论微服务使得单体架构扩展为分布式架构，在扩展的过程中，逐渐丧失了单体架构中数据源单一，可以直接依赖于数据库进行事务操作的能力，而关系型数据库中，提供了强大的事务处理能力，可以满足 ACID（Atomicity，Consistency，Isolation，Durability）的特性，这种特性保证了数据操作的强一致性，这也是分布式环境中弱一致性以及最终一致性能够得以实现的基础。 数据一致性分为三个种类型：强一致性，弱一致性以及最终一致性。对于关系型数据库，要求更新过的数据能被后续的访问都能看到，这是强一致性。如果能容忍后续的部分或者全部访问不到，则是弱一致性。如果经过一段时间后要求能访问到更新后的数据，则是最终一致性。数据库实现的就是强一致性，能够保证在写入一份新的数据，立即使其可见；最终一致性是弱一致性的强化版，系统保证在没有后续更新的前提下，系统最终返回上一次更新操作的值。在没有故障发生的前提下，不一致窗口的时间主要受通信延迟，系统负载和复制副本的个数影响。 CAP 定理CAP 定理是由加州大学伯克利分校 Eric Brewer 教授提出来的，指的是在一个分布式系统中， Consistency（一致性）、 Availability（可用性）、Partition tolerance（分区容错性），三者不可得兼： 一致性（Consistency）：在分布式系统中的所有数据备份，在同一时刻是否同样的值。（对某个指定的客户端来说，读操作保证能返回最新的写操作结果） 可用性（Availability）：在集群中一部分节点故障后，集群整体是否还能响应客户端的读写请求。（非故障的节点在合理的时间内返回合理的响应） 分区容错性（Partition tolerance）：系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在 C 和 A 之间做出选择。（分布式系统在遇到某节点或网络分区故障的时候，仍然能够对外提供满足一致性或可用性的服务） 微服务作为分布式系统，同样受 CAP 原理的制约，在 CAP 理论中， C：Consistency、A：Availability、P：Partition tolerance 三者不可同时满足，而服务化中，更多的是提升 A 以及 P，在这个过程中不可避免的会降低对 C 的要求，因此，BASE 理论随之而来。 BASE 理论BASE 理论来源于 ebay 在 2008 年 ACM 中发表的论文，BASE 理论的基本原则有三个：Basically Available（基本可用），Soft state（软状态），Eventually consistent（最终一致性），主要目的是为了提升分布式系统的可伸缩性，论文同样阐述了如何对业务进行调整以及折中的手段，BASE 理论是对 CAP 定理中的一致性和可用性进行一个权衡的结果，理论的核心思想就是：我们无法做到强一致，但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性（Eventual consistency）。 Basically Available（基本可用）：整个系统在某些不可抗力的情况下，仍然能够保证 “可用性”，即一定时间内仍然能够返回一个明确的结果 Soft state（软状态）：同一数据的不同副本的状态，可以不需要实时一致 Eventually Consistent（最终一致性）：同一数据的不同副本的状态，可以不需要实时一致，但一定要保证经过一定时间后仍然是一致的 在最终一致性的实现过程中，最基本的操作就是保证事务参与者的幂等性，所谓的幂等性，就是业务方能够使用相关的手段，保证单个事务多次提交依然能够保证达到同样的目的。 分布式事务现在，业内比较常用的分布式事务解决方案，包括强一致性的两阶段提交模式、三阶段提交模式，以及最终一致性的事务消息模式、补偿事务模式、本地消息表模式、SAGA 模式，我们会在后面的章节中详细介绍与实战。 两阶段提交（2PC）- 基于 2PC 实现的分布式事务两阶段提交协议（The two-phase commit protocol，2PC）是 XA[1] 用于在全局事务中协调多个资源的机制，2PC 是一个非常经典的强一致、中心化的原子提交协议，。这里所说的中心化是指协议中有两类节点：一个是中心化协调者节点（coordinator）和 N 个参与者节点（partcipant）。在分布式系统中，每一个机器节点能够知道自己在执行事务操作过程是成功或失败，却无法直接获取其他分布式节点的执行结果。因此，为保持事务处理的 ACID，则引入协调者 (即 XA 协议中的事务管理器) 来统一调度所有分布式节点的执行逻辑，而被调度的分布式节点则称为参与者（即 XA 协议中的资源管理器）。 两阶段提交协议，事务管理器（协调者）分两个阶段来协调资源管理器（参与者），第一阶段准备资源，也就是预留事务所需的资源，如果每个资源管理器都资源预留成功，则进行第二阶段资源提交，否则协调资源管理器回滚资源。两阶段提交协议属于牺牲了一部分可用性来换取一致性的分布式事务方案。 第一阶段：投票阶段该阶段的主要目的在于打探数据库集群中的各个参与者是否能够正常的执行事务，具体步骤如下： 事务询问：协调者向所有的参与者发送事务执行请求，并等待参与者反馈事务执行结果。 事执行事务：务参与者收到请求之后，执行事务但不提交，并将 Undo 和 Redo 信息记入事务日志中。 各参与者向协调者反馈事务询问的响应：参与者将自己事务执行情况反馈给协调者，同时阻塞等待协调者的后续指令。 第二阶段：事务提交阶段在经过第一阶段协调者的询盘之后，各个参与者会回复自己事务的执行情况，这时候存在三种可能性：（1）所有的参与者都回复能够正常执行事务；（2）一个或多个参与者回复事务执行失败；（3）协调者等待超时 对于第一种情况，协调者将向所有的参与者发出提交事务的通知，具体步骤如下： 发送提交请求：协调者向各个参与者发送 commit 通知，请求提交事务。 参事务提交：参与者收到事务提交通知之后，执行 commit 操作，然后释放占有的资源。 反馈事务提交结果：参与者向协调者返回事务 commit 结果信息，即向协调者发送 Ack 消息。 对于第二、三种情况，协调者均认为参与者无法成功执行事务，为了整个集群数据的一致性，所以要向各个参与者发送事务回滚通知，具体步骤如下： 发送回滚请求：协调者向各个参与者发送事务 rollback 通知，请求回滚事务。 事务回滚：参与者收到事务回滚通知之后，执行 rollback 操作，然后释放占有的资源。 反馈事务回滚结果：参与者向协调者返回事务 rollback 结果信息，即向协调者发送 Ack 消息。 总结两阶段提交协议原理简单、易于实现，但是缺点也是显而易见的，主要缺点如下： 单点问题：协调者在整个两阶段提交过程中扮演着举足轻重的作用，一旦协调者所在服务器宕机，就会影响整个数据库集群的正常运行，比如在第二阶段中，如果协调者因为故障不能正常发送事务提交或回滚通知，那么参与者们将一直处于阻塞状态，整个数据库集群将无法提供服务。 同步阻塞：两阶段提交执行过程中，所有的参与者都需要听从协调者的统一调度，期间处于阻塞状态而不能从事其他操作，这样效率极其低下。 数据不一致性：两阶段提交协议虽然是分布式数据强一致性所设计，但仍然存在数据不一致性的可能性，比如在第二阶段中，假设协调者发出了事务 commit 通知，但是因为网络问题该通知仅被一部分参与者所收到并执行了 commit 操作，其余的参与者则因为没有收到通知一直处于阻塞状态，这时候就产生了数据的不一致性。 何时选择两阶段提交分布式事务？两阶段提交分布式事务，在prepare阶段需要等待所有参与子事务的反馈，因此可能造成数据库资源锁定时间过长，对性能影响很大，不适合并发高以及子事务生命周长较长的业务场景；因此适用于参与者较少，单个本地事务执行时间较少，并且参与者自身可用性很高的场景，否则，其很可能导致性能下降严重。两阶段提交分布式事务方案属于牺牲了一部分可用性来换取的一致性。 三阶段提交（3PC）- 基于 3PC 实现的分布式事务三阶段提交协议（The three-phase commit protocol，3PC）针对两阶段提交协议存在的问题，将两阶段提交协议的 “投票阶段” 过程一分为二，在两阶段提交协议的基础上增加了 “预询盘” 阶段，以及超时策略使得原先在两阶段提交协议中，参与者在投票之后，由于协调者发生崩溃或错误，而导致参与者处于无法知晓是否提交或者中止的 “不确定状态” 所产生的可能相当长的延时的问题得以解决，从而来减少整个集群的阻塞时间，提升系统性能。三阶段提交协议的三个阶段分别为：can_commit，pre_commit，do_commit。 第一阶段：can_commit 阶段该阶段协调者会去询问各个参与者是否能够正常执行事务，参与者根据自身情况回复一个预估值，相对于真正的执行事务，这个过程是轻量的，具体步骤如下： 事务询问：协调者向各个参与者发送事务询问通知，询问是否可以执行事务操作，并等待回复。 各参与者向协调者反馈事务询问的响应：各个参与者依据自身状况回复一个预估值，如果预估自己能够正常执行事务就返回确定信息，并进入预备状态，否则返回否定信息。 第二阶段：pre_commit 阶段本阶段协调者会根据第一阶段的询盘结果采取相应操作，询盘结果主要有三种：（1）所有的参与者都返回确定信息；（2）一个或多个参与者返回否定信息；（3）协调者等待超时 针对第一种情况，协调者会向所有参与者发送事务执行请求，具体步骤如下： 发送预提交请求：协调者向所有的事务参与者发送事务执行通知。 事务预提交：参与者收到通知后，执行事务但不提交，并将 Undo 和 Redo 信息记录到事务日志中。 各参与者向协调者反馈事务执行的响应：参与者将自己事务执行情况反馈给协调者，同时阻塞等待协调者的后续指令，提交（commit）或中止（abort）。 针对第二、三种情况，协调者认为事务无法正常执行，于是向各个参与者发出 abort 通知，请求退出预备状态，具体步骤如下： 发送中断请求：协调者向所有事务参与者发送 abort 通知。 中断事务：无论是收到来自协调者的 abort 请求，或者是在等待协调者请求过程中出现超时，参与者都会中断事务。 第三阶段：do_commit 阶段如果第二阶段事务未中断，那么本阶段协调者将会依据事务执行返回的结果来决定提交或回滚事务，分为三种情况：（1）所有的参与者都回复能够正常执行事务；（2）一个或多个参与者回复事务执行失败；（3）协调者等待超时 对于第一种情况，协调者将向所有的参与者发出提交事务的通知，具体步骤如下： 发送提交请求：协调者向各个参与者发送 commit 通知，请求提交事务。 参事务提交：参与者收到事务提交通知之后，执行 commit 操作，然后释放占有的资源。 反馈事务提交结果：参与者向协调者返回事务 commit 结果信息，即向协调者发送 Ack 消息。 对于第二、三种情况，协调者均认为参与者无法成功执行事务，为了整个集群数据的一致性，所以要向各个参与者发送事务回滚通知，具体步骤如下： 发送回滚请求：协调者向各个参与者发送事务 rollback 通知，请求回滚事务。 事务回滚：参与者收到事务回滚通知之后，执行 rollback 操作，然后释放占有的资源。 反馈事务回滚结果：参与者向协调者返回事务 rollback 结果信息，即向协调者发送 Ack 消息。 在本阶段如果因为协调者或网络问题，导致参与者迟迟不能收到来自协调者的 commit 或 rollback 请求，那么参与者将不会如两阶段提交协议中那样陷入阻塞，而是等待超时后继续 commit，相对于两阶段提交虽然降低了同步阻塞，但仍然无法完全避免数据的不一致。 总结相比较 2PC 而言，3PC 对于协调者（Coordinator）和参与者（Partcipant）都设置了超时时间，而 2PC 只有协调者才拥有超时机制。这一优化主要避免了参与者在长时间无法与协调者节点通讯（协调者挂掉了）的情况下，无法释放资源的问题，因为参与者自身拥有超时机制会在超时后，自动进行本地 commit 从而进行释放资源，而这种机制也侧面降低了整个事务的阻塞时间和范围。3PC 在去除阻塞的同时也引入了新问题，当参与者接收到 preCommit 消息后，如果网络出现分区，此时协调者所在节点和参与者无法进行正常的网络通信，在这种情况下，该参与者依然会进行事务的提交，这必然出现数据的不一致性。 何时选择三阶段提交分布式事务？两阶段提交协议中所存在的长时间阻塞状态发生的几率还是非常低的，所以虽然三阶段提交协议相对于两阶段提交协议对于数据强一致性更有保障，但是因为效率问题，两阶段提交协议在实际系统中反而更加受宠。 补偿事务（TCC）- 基于 TCC 实现的分布式事务TCC（Try-Confirm-Cancel）实际上是服务化的两阶段提交协议，是一种达到最终一致性的补偿性事务，相对于 XA 等传统模型，其特征在于它不依赖 RM 对分布式事务的支持，而是通过对业务逻辑的分解来实现分布式事务。其核心思想是：”针对每个操作都要注册一个与其对应的确认和补偿（撤销）操作”。它分为三个阶段：Try、Confirm、Cancel，业务开发者需要实现这三个服务接口： Try 阶段：完成所有业务检查，预留必须的业务资源，所有参与者的 Try 接口都成功了，事务管理器会提交事务，并调用每个参与者的 Confirm 接口真正提交业务操作，否则调用每个参与者的 Cancel 接口回滚事务。 Confirm 阶段：真正执行的业务逻辑，不做任何业务检查，只使用 Try 阶段预留的业务资源。因此，只要 Try 操作成功，Confirm 必须能成功。另外，Confirm 操作需满足幂等性，保证一笔分布式事务能且只能成功一次。 Cancel 阶段：释放 Try 阶段预留的业务资源。同样的，Cancel 操作也需要满足幂等性。 事务开始时，业务应用会向事务协调器注册启动事务。之后业务应用会调用所有服务的 try 接口，完成一阶段准备。之后事务协调器会根据 try 接口返回情况，决定调用 confirm 接口或者 cancel 接口。如果接口调用失败，会进行重试。事务协调器记录了全局事务的推进状态以及各子事务的执行状态，负责推进各个子事务共同进行提交或者回滚。同时负责在子事务处理超时后不停重试，重试不成功后转手工处理，用以保证事务的最终一致性。 业务场景介绍假设现在有一个电商系统，里面有一个支付订单的场景。那对一个订单支付之后，我们需要做下面的步骤： [1] 更改订单的状态为“已支付” - 对本地的的订单数据库修改订单状态为 “已支付”[2] 扣减商品库存 - 调用库存服务扣减库存[3] 给会员增加积分 - 调用积分服务增加积分[4] 创建销售出库单通知仓库发货 - 调用仓储服务通知发货 对于分布式事务来说，上面那几个步骤，要么全部成功，如果任何一个服务的操作失败了，就全部一起回滚，撤销已经完成的操作。 TCC 实现阶段一：Try订单服务：修改订单的状态为支付中 OrderStatus.UPDATING库存服务：库存数量不变，可销售库存数量减 1，设计一个单独的冻结库存的字段 freeze_inventory 数量加 1，表示有 1 个库存被冻结积分服务：会员积分不变，设计一个单独的预增加积分字段 prepare_add_credit 数量设置为 10，表示有 10 个积分准备增加仓储服务：先创建一个销售出库单，但是这个销售出库单的状态是 “UNKNOWN”未知 TCC 实现阶段二：Confirm订单服务：修改订单的状态为已支付 OrderStatus.PAYED库存服务：将冻结库存的字段 freeze_inventory 数量清空，表示正式完成了库存的扣减积分服务：将预增加积分字段 prepare_add_credit 10 个积分扣掉，然后加入实际的会员积分字段中仓储服务：将销售出库单的状态正式修改为 “CREATED” 已创建，可以供仓储管理人员查看和使用 TCC 实现阶段三：Cancel订单服务：修改订单的状态为已取消 OrderStatus.CANCELED库存服务：将冻结库存的字段 freeze_inventory 1 个库粗扣掉，然后加入可销售库存字段中积分服务：将预增加积分字段 prepare_add_credit 10 个积分扣掉仓储服务：将销售出库单的状态正式修改为 “CANCELED” 已取消 如果使用基于 TCC 实现的分布式事务，最好选择某种 TCC 分布式事务框架， 事务的 Try、Confirm、Cancel 三个状态交给框架来感知 。服务调用链路依次执行 Try 逻辑，如果都正常的话，TCC 分布式事务框架推进执行 Confirm 逻辑，完成整个事务；如果某个服务的 Try 逻辑有问题，TCC 分布式事务框架感知到之后就会推进执行各个服务的 Cancel 逻辑，撤销之前执行的各种操作。这里笔者给大家推荐几个比较不错的 TCC 框架：ByteTCC，TCC-transaction，Himly。 TCC 异常控制在微服务架构下，很有可能出现网络超时、重发，机器宕机等一系列的异常 Case。一旦遇到这些 Case，就会导致我们的分布式事务执行过程出现异常。最常见的主要是这三种异常，分别是空回滚、幂等、悬挂。 允许空回滚什么是空回滚？事务协调器在调用 TCC 服务的一阶段 Try 操作时，可能会出现因为丢包而导致的网络超时，此时事务管理器会触发二阶段回滚，调用 TCC 服务的 Cancel 操作，而 Cancel 操作调用未出现超时。 TCC 服务在未收到 Try 请求的情况下收到 Cancel 请求，这种场景被称为空回滚；空回滚在生产环境经常出现，用户在实现 TCC 服务时，应允许允许空回滚的执行，即收到空回滚时返回成功。 防悬挂控制事务协调器在调用 TCC 服务的一阶段 Try 操作时，可能会出现因网络拥堵而导致的超时，此时事务管理器会触发二阶段回滚，调用 TCC 服务的 Cancel 操作，Cancel 调用未超时；在此之后，拥堵在网络上的一阶段 Try 数据包被 TCC 服务收到，出现了二阶段 Cancel 请求比一阶段 Try 请求先执行的情况，此 TCC 服务在执行晚到的 Try 之后，将永远不会再收到二阶段的 Confirm 或者 Cancel ，造成 TCC 服务悬挂。 用户在实现 TCC 服务时，要允许空回滚，但是要拒绝执行空回滚之后 Try 请求，要避免出现悬挂。 幂等控制无论是网络数据包重传，还是异常事务的补偿执行，都会导致 TCC 服务的 Try、Confirm 或者 Cancel 操作被重复执行；用户在实现 TCC 服务时，需要考虑幂等控制，即 Try、Confirm、Cancel 执行一次和执行多次的业务结果是一样的。 总结TCC 方案的处理流程与 2PC 方案的处理流程类似，不过 2PC 通常都是在跨库的 DB 层面，而 TCC 本质上就是一个应用层面的 2PC，需要通过业务逻辑来实现。这种分布式事务的实现方式的优势在于，可以让应用自己定义数据库操作的粒度，使得降低锁冲突、提高吞吐量成为可能。当然 TCC 方案也有不足之处，集中表现在以下两个方面： 对应用的侵入性强：业务逻辑的每个分支都需要实现 try、confirm、cancel 三个操作，应用侵入性较强，改造成本高。 实现难度较大：需要按照网络状态、系统故障等不同的失败原因实现不同的回滚策略。为了满足一致性的要求，confirm 和 cancel 接口必须实现幂等。 何时选择基于 TCC 实现的分布式事务？TCC 方案适用于时效性要求高，如转账、支付等场景，因此 TCC 方案在电商、金融领域落地较多，但是上述原因导致 TCC 方案大多被研发实力较强、有迫切需求的大公司所采用。微服务倡导服务的轻量化、易部署，而 TCC 方案中很多事务的处理逻辑需要应用自己编码实现，对业务的侵入强，复杂且开发量大。因此，TCC 实际上是最为复杂的一种情况，其能处理所有的业务场景，但无论出于性能上的考虑，还是开发复杂度上的考虑，都应该尽量避免该类事务。 SAGA - 基于 Saga 实现的分布式事务Saga 事务模型又叫做长时间运行的事务（Long-running-transaction）, 它是由普林斯顿大学的 Hector Garcia-Molina 和 Kenneth Salem 等人提出，它描述的是另外一种在没有两阶段提交的的情况下解决分布式系统中复杂的业务事务问题。该模型其核心思想就是拆分分布式系统中的长事务为多个短事务，或者叫多个本地事务，然后由 Sagas 工作流引擎负责协调，如果整个流程正常结束，那么就算是业务成功完成，如果在这过程中实现失败，那么 Sagas 工作流引擎就会以相反的顺序调用补偿操作，重新进行业务回滚。 Saga 的具体实现分为两种：协同式（Choreography） 以及 编排式（Orchestration） 协同式（Choreography）这种模式下不存在协调器的概念，每个节点均对自己的上下游负责，在监听处理上游节点事件的同时，对下游节点发布事件。 把 Saga 的决策和执行顺序逻辑分布在 Saga 的每一个参与方中，它们通过交换事件的方式来进行沟通。 编排式（Orchestration）把 Saga 的决策和执行顺序逻辑集中在一个 Saga 编排器类中。Saga 编排器发出命令式消息给每个 Saga 参与方，指示这些参与方服务完成具体操作。该中心节点，即协调器知道整个事务的分布状态，相比于无中心节点方式，该方式有着许多优点：（1）能够避免事务之间的循环依赖关系；（2）参与者只需要执行命令 / 回复，降低参与者的复杂性；（3）开发测试门槛低；（4）在添加新步骤时，事务复杂性保持线性，回滚更容易管理。因此大多数 Saga 模型实现均采用了这种思路。 总结Saga 方案的优点在于其降低了事务粒度，使得事务扩展更加容易，同时采用了异步化方式提升性能。但是其缺点在于很多时候很难定义补偿接口，回滚代价高，而且由于 Saga 在执行过程中采用了先提交后补偿的思路进行操作，所以单个子事务在并发提交时的隔离性很难保证。 何时选择基于 Saga 实现的分布式事务？Saga 方案适用于无需马上返回业务发起方最终状态的场景，例如：你的请求已提交，请稍后查询或留意通知之类的场景。Saga 方案中所有的本地子事务执行过程中，都无需等待其调用的子事务执行，减少了加锁的时间，这在事务流程较多较长的业务中性能优势更为明显。同时，其利用队列进行进行通讯，具有削峰填谷的作用。因此该形式适用于不需要同步返回发起方执行最终结果、可以进行补偿、对性能要求较高、不介意额外编码的业务场景。 事务消息 - 基于可靠消息最终一致性实现的分布式事务基于普通消息的最终一致性分布式事务方案存在的一致性问题：（1）以订单创建为例，订单系统先创建订单(本地事务)，再发送消息给下游处理；如果订单创建成功，然而消息没有发送出去，那么下游所有系统都无法感知到这个事件，会出现脏数据；（2）如果先发送订单消息，再创建订单；那么就有可能消息发送成功，但是在订单创建的时候却失败了，此时下游系统却认为这个订单已经创建，也会出现脏数据。 此时可能有同学会想，我们可否将消息发送和业务处理放在同一个本地事务中来进行处理，如果业务消息发送失败，那么本地事务就回滚，这样是不是就能解决消息发送的一致性问题呢？ 可能的情况 一致性 订单处理成功，然后突然宕机，事务未提交，消息没有发送出去 一致 订单处理成功，由于网络原因或者 MQ 宕机，消息没有发送出去，事务回滚 一致 订单处理成功，消息发送成功，但是 MQ 由于其他原因，导致消息存储失败，事务回滚 一致 订单处理成功，消息存储成功，但是 MQ 处理超时，从而 ACK 确认失败，导致发送方本地事务回滚 不一致 对于消息发送的异常情况分析，我们可以看到，使用基于普通消息的最终一致性分布式事务方案无论如何，都无法保证业务处理与消息发送两边的一致性，其根本的原因就在于：远程调用，结果最终可能为成功、失败、超时；而对于超时的情况，处理方最终的结果可能是成功，也可能是失败，调用方是无法知晓的。为了保证两边数据的一致性，我们只能从其他地方寻找新的突破口。 事物消息由于传统的处理方式无法解决消息生成者本地事务处理成功与消息发送成功两者的一致性问题，因此事务消息就诞生了，事务消息特性可以看作是两阶段协议的消息实现方式，用以确保在以消息中间件解耦的分布式系统中本地事务的执行和消息的发送，可以以原子的方式进行。 事务消息作为一种异步确保型事务，本质就是为了解决本地事务执行与消息发送的原子性问题。目前，事务消息在多种分布式消息中间件中均有实现，但是其实现方式思路却各有不同。 传统事务消息实现传统事务消息实现，一种思路是依赖于 AMQP 协议用来确保消息发送成功，AMQP 模式下需要在发送事务消息时进行两阶段提交，首先进行 tx_select 开启事务，然后再进行消息发送，最后进行消息的 commit 或者是 rollback。这个过程可以保证在消息发送成功的同时本地事务也一定成功执行，但事务粒度不好控制，而且会导致性能急剧下降，同时依然无法解决本地事务执行与消息发送的原子性问题。 还有另外一种思路，就是通过保证多条消息的同时可见性来保证事务一致性。但是此类消息事务实现机制更多的是用到事务循环（consume-transform-produce）场景中，其本质还是用来保证消息自身事务，并没有把外部事务包含进来。 RocketMQ 事务消息实现RocketMQ 事务消息设计则主要是为了解决 Producer 端的消息发送与本地事务执行的原子性问题，RocketMQ 的设计中 broker 与 producer 端的双向通信能力，使得 broker 天生可以作为一个事务协调者存在；而 RocketMQ 本身提供的存储机制，则为事务消息提供了持久化能力；RocketMQ 的高可用机制以及可靠消息设计，则为事务消息在系统在发生异常时，依然能够保证事务的最终一致性达成。 RocketMQ 事务消息的设计流程同样借鉴了两阶段提交理论，整体交互流程如下图所示： 事务发起方首先发送 prepare 消息到 MQ。 在发送 prepare 消息成功后执行本地事务。 根据本地事务执行结果返回 commit 或者是 rollback。 如果消息是 rollback，MQ 将删除该 prepare 消息不进行下发，如果是 commit 消息，MQ 将会把这个消息发送给 consumer 端。 如果执行本地事务过程中，执行端挂掉，或者超时，MQ 将会不停的询问其同组的其它 producer 来获取状态。 consumer 端的消费成功机制有 MQ 保证。 在具体实现上，RocketMQ 通过使用 Half Topic 以及 Operation Topic 两个内部队列来存储事务消息推进状态。其中，Half Topic 对应队列中存放着 prepare 消息，Operation Topic 对应的队列则存放了 prepare message 对应的 commit/rollback 消息，消息体中则是 prepare message 对应的 offset，服务端定期扫描消息集群中的事物消息，比对两个队列的差值来找到尚未提交的超时事务，进行回查。 从用户侧来说，用户需要分别实现本地事务执行以及本地事务回查方法，因此只需关注本地事务的执行状态即可；而在 service 层，则对事务消息的两阶段提交进行了抽象，同时针对超时事务实现了回查逻辑，通过不断扫描当前事务推进状态，来不断反向请求 Producer 端获取超时事务的执行状态，在避免事务挂起的同时，也避免了 Producer 端的单点故障。而在存储层，RocketMQ 通过 Bridge 封装了与底层队列存储的相关操作，用以操作两个对应的内部队列，用户也可以依赖其它存储介质实现自己的 service，RocketMQ 会通过 ServiceProvider 加载进来。 总结总结一下关于事物消息的常见问题： 如果 consumer 消费失败，是否需要 producer 做回滚呢？ 答：事务消息适用于上游事务对下游事务无依赖的场景，即 producer 不会因为 consumer 消费失败而做回滚，采用事务消息的应用，其所追求的是高可用和最终一致性，消息消费失败的话，MQ 自己会负责重推消息，直到消费成功。因此，事务消息是针对生产端而言的，而消费端，消费端的一致性是通过 MQ 的重试机制来完成的。 如果 consumer 端因为业务异常而导致回滚，那么岂不是两边最终无法保证一致性? 答：基于消息的最终一致性方案必须保证消费端在业务上的操作没障碍，它只允许系统异常的失败，不允许业务上的失败，比如在你业务上抛出个 NPE 之类的问题，导致你消费端执行事务失败，那就很难做到一致了。 何时选择基于可靠消息的最终一致性实现的分布式事务？事务消息较好的解决了事务的最终一致性问题，事务发起方仅需要关注本地事务执行以及实现回查接口给出事务状态判定等实现，而且在上游事务峰值高时，可以通过消息队列，避免对下游服务产生过大压力。所以，事务消息不仅适用于上游事务对下游事务无依赖的场景，还可以与一些传统分布式事务架构相结合，而 MQ 的服务端作为天生的具有高可用能力的协调者，使基于可靠消息的最终一致性分布式事务解决方案，用以满足各种场景下的分布式事务需求。 不过这种方式技术实现的难度比较大，目前主流的开源 MQ（ActiveMQ、RabbitMQ、Kafka、RocketMQ）中只有 RocketMQ 实现对事物消息的支持，其余 MQ 均未实现对事务消息的支持，因此，如果我们希望强依赖一个 MQ 的事务消息来做到消息最终一致性的话，在目前的情况下，技术选型上只能去选择 RocketMQ 来解决。 本地消息表 - 基于本地消息最终一致性实现的分布式事务由于并非所有的 MQ 都支持事务消息，假如我们不选择 RocketMQ 来作为系统的 MQ，是否能够做到消息的最终一致性呢？答案是可以的。 基于 MQ 事物消息的分布式事务方案其实是对本地消息表的封装，将本地消息表基于 MQ 内部，其他方面的协议基本与本地消息表一致。因此，我们可以以事物消息的实现方式去看待基于本地消息表的分布式事务方案。 本地消息表这种实现方式应该是业界使用最多的，该方案也是目前我参与的项目组所使用的分布式事务方案，其核心思想是将分布式事务拆分成本地事务进行处理，通过消息日志的方式来异步执行，这种思路是来源于 ebay。 方案通过在事务主动发起方额外新建事务消息表，事务发起方处理业务和记录事务消息在本地事务中完成，保证了业务与消息同时成功持久化；通过定时任务轮询事务消息表的数据发送事务消息，如果消息投递失败，依靠重试机制重试发送，发送成功后将消息状态更新或者消息清除；事务被动方基于消息中间件消费事务消息表中的事务，如果处理失败，那么依赖 MQ 本身的重试来完成重试执行，同时需要注意重试的幂等行设计；如果是业务上面的失败，可以给事务主动发起方发送一个业务补偿消息，通知事务主动发起方进行回滚等操作。事务主动发起和事务被动方定时扫描本地消息表，把还没处理完成的消息或者失败的消息再发送一遍。这样设计可以避免”业务处理成功 + 事务消息发送失败”，或”业务处理失败 + 事务消息发送成功”的棘手情况出现，保证 2 个系统事务的数据一致性。 何时选择基于本地消息最终一致性实现的分布式事务？基于本地消息最终一致性分布式事务是一种非常经典的分布式事务实现方案，基本避免了分布式事务，实现了“最终一致性”。该方法从应用设计开发的角度实现了消息数据的可靠性，消息数据的可靠性不依赖于消息中间件，弱化了对 MQ 中间件特性的依赖。但是该方案与具体的业务场景绑定，耦合性强，不可公用。 消息数据与业务数据同库，占用业务系统资源。 业务系统在使用关系型数据库的情况下，消息服务性能会受到关系型数据库并发性能的局限。 基于消息实现的事务适用于分布式事务的提交或回滚只取决于事务发起方的业务需求，其他数据源的数据变更跟随发起方进行的业务场景。 总结上述几种的分布式事务方案中，笔者大致总结了其设计思路、流程、优势、劣势、使用场景等，相信读者已经有了一定的理解。其实分布式系统的事务一致性本身是一个技术难题，目前没有一种很简单很完美的方案能够应对所有场景。笔者认为对于分布式事务具体还是要使用者根据不同的业务场景去抉择，结合自己的业务分析，看看自己的业务比较适合哪一种，是在乎强一致，还是最终一致即可。上面对解决方案只是一些简单介绍，如果真正的想要落地，其实每种方案需要思考的地方都非常多，复杂度都比较大，所以最后再次提醒一定要判断好是否使用分布式事务。 微服务兴起这几年涌现出不少分布式事务框架，比如 ByteTCC、TCC-transaction、TCC-transaction 以及最近很火爆的 Seata。目前笔者也在阅读、研究 Seata 源码，如果诸位对分布式事务感兴趣，我想 Seata 框架是一个值得研究的框架！ 参考博文[1]. 对分布式事务及两阶段提交、三阶段提交的理解[2]. 分布式事务：两阶段提交与三阶段提交[3]. 里程碑 | Apache RocketMQ 正式开源分布式事务消息[4]. 分布式事务 Seata Saga 模式首秀以及三种模式详解 | Meetup#3 回顾[5]. 分布式事务 Seata TCC 模式深度解析 | SOFAChannel#4 直播整理 注脚[1]. XA：为了统一标准减少行业内不必要的对接成本，需要制定标准化的处理模型及接口标准，国际开放标准组织 Open Group 定义分布式事务处理模型 DTP（Distributed Transaction Processing Reference Model），DTP 模型定义 TM 和 RM 之间通讯的接口规范叫 XA。XA 协议由 Tuxedo 首先提出的，并交给 X/Open 组织，作为资源管理器 RM（Resource Manager）与事务管理器 TM（Transaction Manager）之间进行通信的接口标准。目前，Oracle、Informix、DB2 和 Sybase 等各大数据库厂家都提供对 XA 的支持。XA 协议采用两阶段提交方式来管理分布式事务。在 XA 规范中，数据库充当 RM 角色，应用需要充当 TM 的角色，即生成全局的 txId，调用 XAResource 接口，把多个本地事务协调为全局统一的分布式事务。 分布式设计之美系列 分布式设计之美（一）：主流分布式锁实现方案 分布式设计之美（二）：微服务架构下分布式事务解决方案]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>2PC</tag>
        <tag>TCC</tag>
        <tag>RocketMQ</tag>
        <tag>Saga</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis 深度探险（四）：Redis 高可用性解决方案之哨兵与集群]]></title>
    <url>%2Farchives%2Fd63e1e23.html</url>
    <content type="text"><![CDATA[前言在开始本章的讲解之前，我们首先从宏观角度回顾一下 Redis 实现高可用相关的技术。它们包括：持久化、复制、哨兵和集群，在本系列的前篇文章介绍了持久化以及复制的原理以及实现。本文将对剩下的两种高可用技术哨兵、集群进行讲解，讲一讲它们是如何进一步提高系统的高可用性？ Redis 的主从复制模式下，一旦主节点由于故障不能提供服务，需要手动将从节点晋升为主节点，同时还要通知客户端更新主节点地址，这种故障处理方式从一定程度上是无法接受的。Redis 2.8 以后提供了 Redis Sentinel 哨兵机制来解决这个问题。 在 Redis 3.0 之前，使用哨兵（sentinel）机制来监控各个节点之间的状态。Redis Cluster 是 Redis 的分布式解决方案，在 3.0 版本正式推出，有效地解决了 Redis 在分布式方面的需求。当遇到单机内存、并发、流量等瓶颈时，可以采用 Cluster 架构方案达到负载均衡的目的。 Redis HA 实践（Redis Sentinel）Redis Sentinel 概述Sentinel（哨岗、哨兵）是 Redis 的高可用（high availability）解决方案：由一个或多个 Sentinel 实例（instance）组成的 Sentinel 系统（system）可以监视任意多个主服务器，以及这些主服务器属下的所有从服务器，并在被监视的主服务器进入下线状态时，自动将下线主服务器属下的某个从服务器升级为新的主服务器，然后由新的主服务器代替已下线的主服务器继续处理命令请求。 当 server1 的下线时长超过用户设定的下线时长上限时，Sentinel 系统就会对 server1 执行故障转移操作： 首先，Sentinel 系统会挑选 server1 属下的其中一个从服务器，并将这个被选中的从服务升级为新的主服务器。 之后，Sentinel 系统会向 server1 属下的所有从服务器发送新的复制指令，让他们成为新的主服务器的从服务器，当所有从服务器都开始复制新的主服务器时，故障转移操作执行完毕。 另外，Sentinel 还会继续监视已下线的 server1，并在它重新上线时，将它设置为新的主服务器的从服务器。 Sentinel 系统用于管理多个 Redis 服务器（instance）， 该系统执行以下三个任务： 监控（Monitoring）： Sentinel 会不断地检查你的主服务器和从服务器是否运作正常。 提醒（Notification）： 当被监控的某个 Redis 服务器出现问题时， Sentinel 可以通过 API 向管理员或者其他应用程序发送通知。 自动故障迁移（Automatic failover）： 当一个主服务器不能正常工作时， Sentinel 会开始一次自动故障迁移操作， 它会将失效主服务器的其中一个从服务器升级为新的主服务器， 并让失效主服务器的其他从服务器改为复制新的主服务器； 当客户端试图连接失效的主服务器时， 集群也会向客户端返回新主服务器的地址， 使得集群可以使用新主服务器代替失效服务器。 Redis Sentinel 重点总结 Sentinel 只是一个运行在特殊模式下的 Redis 服务器，因此初始化服务器时将普通 Redis 服务器使用的代码替换成 Sentinel 专门代码，它使用了和普通模式不同的命令表，所以 Sentinel 模式能够使用的命令和普通 Redis 服务器能够使用的命令不同。 Sentinel 会读入用户指定的配置文件，为每个要被监视的主服务器创建相应的实例结构，并创建连向主服务器的命令连接和订阅连接，其中命令连接用于向主服务器发送命令请求，而订阅连接则用于接收指定频道的消息。 Sentinel 通过主服务器发送 INFO 命令来获得主服务器属下所有从服务器的地址信息，并为这些从服务器创建相应的实例结构，以及连向这些从服务器的命令连接和订阅连接。在一般情况下，Sentinel 以每十秒一次的频率向被监视的主服务器和从服务器发送 INFO 命令，当主服务器处于下线状态，或者 Sentinel 正在对主服务器进行故障转移操作时，Sentinel 向从服务器发送 INFO 命令的频率会改为每秒一次。 对于监视同一个主服务器和从服务器的多个 Sentinel 来说，它们会以每两秒一次的频率，通过向被监视服务器的 _sentinel_:hello 频道发送消息来向其他 Sentinel 宣告自己的存在。每个 Sentinel 也会从 _sentinel_:hello 频道中接收其他 Sentinel 发来的消息，并根据这些消息为其他 Sentinel 创建相应的实例结构以及命令连接。Sentinel 只会与主服务器和从服务器创建命令连接和订阅连接，Sentinel 与 Sentinel 之间则只创建命令连接。 Sentinel 以每秒一次的频率向实例（包括主服务器、从服务器、其他 Sentinel）发送 PING 命令，并根据实例对 PING 命令的回复来判断实例是否在线，当一个实例在指定的时长中连续向 Sentinel 发送无效回复时，Sentinel 会将这个实例判断为主观下线。 当 Sentinel 将一个主服务器判断为主观下线时，它会向同样监视这个主服务器的其它 Sentinel 进行询问，看它们是否同意这个主服务器已经进入主观下线状态。当 Sentinel 收集到足够多的的主观下线投票之后，它会将主服务器判断为客观下线，并发起一次针对主服务器的故障转移操作。 当一个主服务器被判断为客观下线时，监视这个下线主服务器的各个 Sentinel 会进行协商，选举出一个领头 Sentinel[1]，并由领头 Sentinel 对下线主服务器进行故障转移操作。 Redis Sentinel 搭建Redis Sentinel 部署技巧及其环境 一个健壮的部署至少需要三个哨兵实例，并且使用奇数个 Sentinel。 三个哨兵实例应该放置在客户使用独立方式确认故障的计算机或虚拟机中，例如不同的物理机或不同可用区域的虚拟机。 哨兵配置文件中只需要配置主从复制中的主副本 ip 和端口即可，当主从进行切换时哨兵会自动修改哨兵配置文件中的主副本 ip 为新在主副本 ip。 由于本人没有这么多服务器，因此在一台机器上模拟一个 Redis Sentinel 集群。 角色 IP 地址 端口号 Redis Master 127.0.0.1 6380 Redis Slave-01 127.0.0.1 6381 Redis Slave-02 127.0.0.1 6382 Redis Slave-03 127.0.0.1 6383 Redis Sentinel-01 127.0.0.1 26381 Redis Sentinel-02 127.0.0.1 26382 Redis Sentinel-03 127.0.0.1 26383 Redis Sentinel 安装指南1、下载 Redis 服务软件包到服务器，解压后并编译安装。 12345[root@VM_24_98_centos ~]# mkdir /usr/local/redis[root@VM_24_98_centos ~]# wget http://download.redis.io/releases/redis-5.0.6.tar.gz[root@VM_24_98_centos ~]# tar -zvxf redis-5.0.6.tar.gz -C /usr/local/redis[root@VM_24_98_centos ~]# cd /usr/local/redis/redis-5.0.6/[root@VM_24_98_centos redis-5.0.6]# make PREFIX=/usr/local/redis install 2、设置 Redis 主服务器 a. 创建目录以及复制配置文件 123[root@VM_24_98_centos redis]# mkdir -p /usr/local/redis/redis-master/redis-6380[root@VM_24_98_centos redis-master]# cp -r /usr/local/redis/redis-5.0.6/redis.conf /usr/local/redis/redis-master/redis-6380/[root@VM_24_98_centos redis-master]# vim /usr/local/redis/redis-master/redis-6380/redis.conf b. 设置 Redis Master 主服务器配置环境 1234567891011121314151617181920# 开启远程连接bind 0.0.0.0# 端口号port 6380# 守护进程daemonize yes# 进程文件pidfile /usr/local/redis/redis-master/redis-6380/redis.pid# 日志文件logfile /usr/local/redis/redis-master/redis-6380/redis.log# 工作目录dir /usr/local/redis/redis-master/redis-6380/# 主服务器密码masterauth foobared# 认证密码requirepass foobared# 开启 AOF 持久化appendonly yes# 每秒调用一次 fsyncappendfsync everysec c. 启动 Redis Master 主服务器 12[root@VM_24_98_centos redis-6380]# /usr/local/redis/bin/redis-server /usr/local/redis/redis-master/redis-6380/redis.conf[root@VM_24_98_centos redis-6380]# ps -ef |grep redis d. 客户端测试连接 12345678910111213141516[root@VM_24_98_centos ~]# /usr/local/redis/bin/redis-server /usr/local/redis/redis-master/redis-6380/redis.conf[root@VM_24_98_centos ~]# /usr/local/redis/bin/redis-cli -h 127.0.0.1 -p 6380 -a foobared127.0.0.1:6380&gt; INFO REPLICATION# Replicationrole:masterconnected_slaves:0master_replid:5c1034ac4dec31d6a4ae883e1eaacca3a78bc3b6master_replid2:0000000000000000000000000000000000000000master_repl_offset:0second_repl_offset:-1repl_backlog_active:0repl_backlog_size:1048576repl_backlog_first_byte_offset:0repl_backlog_histlen:0 3、设置 Redis 从服务器 a. 创建目录以及复制配置文件 123[root@VM_24_98_centos redis]# mkdir -p /usr/local/redis/redis-slave/redis-6381[root@VM_24_98_centos redis-slave]# cp -r /usr/local/redis/redis-5.0.6/redis.conf /usr/local/redis/redis-slave/redis-6381/[root@VM_24_98_centos redis-slave]# vim /usr/local/redis/redis-slave/redis-6381/redis.conf b. 设置 Redis Slave 从服务器配置环境 12345678910111213141516171819202122# 开启远程连接bind 0.0.0.0# 端口号port 6381# 守护进程daemonize yes# 进程文件pidfile /usr/local/redis/redis-slave/redis-6381/redis.pid# 日志文件logfile /usr/local/redis/redis-slave/redis-6381/redis.log# 工作目录dir /usr/local/redis/redis-slave/redis-6381/# 主从复制 Master 节点地址 + 端口replicaof 127.0.0.1 6380# 主服务器密码masterauth foobared# 认证密码requirepass foobared# 开启 AOF 持久化appendonly yes# 每秒调用一次 fsyncappendfsync everysec c. 启动 Redis Slave 从服务器 12[root@VM_24_98_centos redis-6381]# /usr/local/redis/bin/redis-server /usr/local/redis/redis-slave/redis-6381/redis.conf[root@VM_24_98_centos redis-6381]# ps -ef |grep redis d. 客户端测试连接 123456789101112131415161718192021222324[root@VM_24_98_centos ~]# /usr/local/redis/bin/redis-server /usr/local/redis/redis-slave/redis-6381/redis.conf[root@VM_24_98_centos ~]# /usr/local/redis/bin/redis-cli -h 127.0.0.1 -p 6381 -a foobared127.0.0.1:6381&gt; INFO REPLICATION# Replicationrole:slavemaster_host:127.0.0.1master_port:6380master_link_status:upmaster_last_io_seconds_ago:5master_sync_in_progress:0slave_repl_offset:14slave_priority:100slave_read_only:1connected_slaves:0master_replid:8ecb8d89dba51e54aabb1c7feeda42fe6e6a8dc0master_replid2:0000000000000000000000000000000000000000master_repl_offset:14second_repl_offset:-1repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:1repl_backlog_histlen:14 e. 同理，从服务器 redis-6382、redis-6383 按照上面的步骤部署。 4、Redis Sentinel 部署 a. 创建目录以及复制配置文件 123[root@VM_24_98_centos redis]# mkdir -p /usr/local/redis/redis-sentinel/redis-26381[root@VM_24_98_centos redis-sentinel]# cp -r /usr/local/redis/redis-5.0.6/sentinel.conf /usr/local/redis/redis-sentinel/redis-26381/[root@VM_24_98_centos redis-sentinel]# vim /usr/local/redis/redis-sentinel/redis-26381/sentinel.conf b. 设置 Redis Sentinel 哨兵服务器配置环境 1234567891011121314# 端口号port 26381# 守护进程daemonize yes# 进程文件pidfile /usr/local/redis/redis-sentinel/redis-26381/redis.pid# 日志文件logfile /usr/local/redis/redis-sentinel/redis-26381/redis.log# 工作目录dir /usr/local/redis/redis-sentinel/redis-26381/# 指定监控 master&#123;2 表示多少个 sentinel 同意&#125;sentinel monitor mymaster 127.0.0.1 6380 2# 安全信息sentinel auth-pass mymaster foobared c. 启动 Redis Sentinel 哨兵服务器 12[root@VM_24_98_centos redis-26381]# /usr/local/redis/bin/redis-sentinel /usr/local/redis/redis-sentinel/redis-26381/sentinel.conf[root@VM_24_98_centos redis-26381]# ps -ef |grep redis d. 客户端测试连接 12345678910[root@VM_24_98_centos ~]# /usr/local/redis/bin/redis-cli -h 127.0.0.1 -p 26381127.0.0.1:26381&gt; INFO SENTINEL# Sentinelsentinel_masters:1sentinel_tilt:0sentinel_running_scripts:0sentinel_scripts_queue_length:0sentinel_simulate_failure_flags:0master0:name=mymaster,status=ok,address=127.0.0.1:6380,slaves=3,sentinels=1 e. 同理，哨兵服务器 redis-26382、redis-26383 按照上面的步骤部署 f. 查看 Redis Master 主服务器连接状况 1234567891011121314151617[root@VM_24_98_centos ~]# /usr/local/redis/bin/redis-cli -h 127.0.0.1 -p 6380 -a foobared127.0.0.1:6380&gt; INFO REPLICATION# Replicationrole:masterconnected_slaves:3slave0:ip=127.0.0.1,port=6383,state=online,offset=20836,lag=0slave1:ip=127.0.0.1,port=6381,state=online,offset=20836,lag=0slave2:ip=127.0.0.1,port=6382,state=online,offset=20836,lag=0master_replid:cc8ef3fe2e51a714f5b73b2fbe3bd697cacbc453master_replid2:8ecb8d89dba51e54aabb1c7feeda42fe6e6a8dc0master_repl_offset:20836second_repl_offset:1522repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:1repl_backlog_histlen:20836 Redis Sentinel 场景测试模拟场景：Redis Master 节点挂掉，查看 Redis 集群状态。 Step1、关掉 Master 节点 123[root@VM_24_98_centos ~]# /usr/local/redis/bin/redis-cli -h 127.0.0.1 -p 6380 -a foobared127.0.0.1:6380&gt; SHUTDOWN Step2、通过哨兵查看集群状态 123456789[root@VM_24_98_centos ~]# /usr/local/redis/bin/redis-cli -h 127.0.0.1 -p 26381127.0.0.1:26381&gt; INFO SENTINEL# Sentinelsentinel_masters:1sentinel_tilt:0sentinel_running_scripts:0sentinel_scripts_queue_length:0sentinel_simulate_failure_flags:0master0:name=mymaster,status=ok,address=127.0.0.1:6381,slaves=3,sentinels=3 通过 Sentinel 信息可以看到，Master 节点已经自动切换到 6381 端口了，说明主节点挂掉后，6381 Slave 节点自动升级成为了 Master 节点。 通过 Sentinel 日志文件显示了 failover 的过程： Step3、启动 6380 Redis 服务，然后查看节点角色，此时 6380 变成了 Slave，6381 为 Master 节点 123456789101112131415161718192021222324[root@VM_24_98_centos ~]# /usr/local/redis/bin/redis-server /usr/local/redis/redis-master/redis-6380/redis.conf[root@VM_24_98_centos ~]# /usr/local/redis/bin/redis-cli -h 127.0.0.1 -p 6380 -a foobared127.0.0.1:6380&gt; INFO REPLICATION# Replicationrole:slavemaster_host:127.0.0.1master_port:6381master_link_status:upmaster_last_io_seconds_ago:1master_sync_in_progress:0slave_repl_offset:782228slave_priority:100slave_read_only:1connected_slaves:0master_replid:84aa69ee0b191bba31162c26c4ddb1c87a705f7emaster_replid2:0000000000000000000000000000000000000000master_repl_offset:782228second_repl_offset:-1repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:777789repl_backlog_histlen:4440 Redis HA 实践（Redis Cluster）Redis Cluster 概述Redis 集群是 Redis 提供的分布式数据库方案，集群通过分片（sharding）而非一致性哈希（consistency hashing）来进行数据分享，并提供复制和故障转移功能。Redis Cluster，主要是针对海量数据 + 高并发 + 高可用的场景。Redis Cluster 支撑 N 个 Redis Master Node，每个 Master Node 都可以挂载多个 Slave Node。Redis Cluster 节点间采用 Gossip 协议[2]进行通信。 节点：一个 Redis 集群通常由多个节点（node）组成，连接各个节点的工作可以使用 CLUSTER MEET \&lt;ip> \&lt;port> 命令来完成，将各个独立的节点连接起来，构成一个包含多个节点的集群。向一个节点 node 发送 CLUSTER MEET 命令，可以让 node 节点与 ip 和 port 所指定的节点进行握手（handshake），当握手成功时，node 节点就会将 ip 和 port 所指定的节点添加到 node 节点当前所在的集群中。 槽指派：Redis 集群通过分片的方式来保存数据库中的键值对，集群的整数据库被分为 16384 个槽（slot），数据库中的每个键都属于这 16384 个槽的其中一个，集群中的每个节点可以处理 0 个或最多 16384 个槽。Redis 集群有固定的 16384 个 hash slot，对每个 key 计算 CRC16 值，然后对 16384 取模，可以获取 key 对应的 hash slot。当数据库中的 16384 个槽都有节点在处理时，集群处于上线状态（ok）；相反地，如果数据库中任何一个槽没有得到处理，那么集群处于下线状态（fail）。 Redis Cluster 重点总结 节点通过握手来将其他节点添加到自己所处的集群当中。 集群中的 16384（2的14次方）个槽可以分别指派给集群中的各个节点，每个节点都会记录哪些槽指派给了自己，而哪些槽又被指派给其他节点。 节点在接到一个命令请求时，会先检查这个命令请求要处理的键所在的槽是否由自己负责，如果不是的话，节点将向客户端返回一个 MOVED 错误，MOVED 错误携带的信息可以指引客户端转向至正在负责相关槽的节点。 对 Redis 集群的重新分片工作是由 redis-trib 负责执行的，重新分片的关键是将属于某个槽的所有键值对从一个节点转移至另外一个节点。重新分片操作可以在线（online）进行，在重新分片的过程中，集群不需要下线，并且源节点和目标节点都可以继续处理命令请求。 如果节点 A 正在迁移槽 i 至节点 B，那么当节点 A 没能在自己的数据库中找到命令指定的数据库键时，节点 A 会向客户端返回一个 ASK 错误，指引客户端到节点 B 继续查找指定的数据库键。 MOVED 错误表示槽的负责权已经从一个节点转移到了另外一个节点，而 ASK 错误只是两个节点在迁移槽的过程中使用的一种临时措施。 Redis 集群中的节点分为主节点（master）和从节点（slave），其中主节点用于处理槽，而从节点用于复制主节点，并在主节点下线时，代替主节点继续处理命令请求。 集群中的节点通过发送和接收消息来进行通信，常见的消息包括 MEET、PING、PONG、PUBLISH、FAIL 五种。 Redis Cluster 搭建Redis Cluster 部署技巧及其环境 Redis 集群至少需要 3 个节点，因为投票容错机制要求超过半数节点认为某个节点挂了该节点才是挂了，所以 2 个节点无法构成集群。 要保证集群的高可用，需要每个节点都有从节点，也就是备份节点，即三主三从，所以 Redis 集群至少需要 6 台服务器。 Redis 5.0 开始不再使用 Ruby 搭建集群，而是直接使用客户端命令 redis-cli 来创建。 不支持多数据库空间，集群模式下只能使用 db0 空间。 由于资源有限，因此在一台机器上模拟一个 Redis Cluster。 角色 IP 地址 端口号 Redis Cluster-Master-01-6391 127.0.0.1 6391 Redis Cluster-Master-02-6393 127.0.0.1 6393 Redis Cluster-Master-02-6395 127.0.0.1 6395 Redis Cluster-Slave-01-6394 127.0.0.1 6394 Redis Cluster-Slave-02-6396 127.0.0.1 6396 Redis Cluster-Slave-03-6392 127.0.0.1 6392 Redis Cluster 安装指南1、下载 Redis 服务软件包到服务器，解压后并编译安装。 12345[root@VM_24_98_centos ~]# mkdir /usr/local/redis[root@VM_24_98_centos ~]# wget http://download.redis.io/releases/redis-5.0.6.tar.gz[root@VM_24_98_centos ~]# tar -zvxf redis-5.0.6.tar.gz -C /usr/local/redis[root@VM_24_98_centos ~]# cd /usr/local/redis/redis-5.0.6/[root@VM_24_98_centos redis-5.0.6]# make PREFIX=/usr/local/redis install 2、设置 Redis Cluster 服务器 a. 创建目录以及复制配置文件 123[root@VM_24_98_centos ~]# mkdir -p /usr/local/redis/redis-cluster/redis-6391[root@VM_24_98_centos ~]# cp -r /usr/local/redis/redis-5.0.6/redis.conf /usr/local/redis/redis-cluster/redis-6391/[root@VM_24_98_centos ~]# vim /usr/local/redis/redis-cluster/redis-6391/redis.conf b. 设置 Redis Cluster 服务器配置环境 12345678910111213141516171819202122232425262728# 开启远程连接bind 0.0.0.0# protected-mode no# 端口号port 6391# 守护进程daemonize yes# 进程文件pidfile /usr/local/redis/redis-cluster/redis-6391/redis.pid# 日志文件logfile /usr/local/redis/redis-cluster/redis-6391/redis.log# 工作目录dir /usr/local/redis/redis-cluster/redis-6391/# 主服务器密码masterauth foobared# 认证密码requirepass foobared# 开启 AOF 持久化appendonly yes# 每秒调用一次 fsyncappendfsync everysec# 开启集群cluster-enabled yes# 集群的配置文件，首次启动会自动创建cluster-config-file nodes.conf# 集群节点连接超时时间，15秒cluster-node-timeout 15000 c. 启动 Redis Cluster 服务器 12[root@VM_24_98_centos ~]# /usr/local/redis/bin/redis-server /usr/local/redis/redis-cluster/redis-6391/redis.conf[root@VM_24_98_centos ~]# ps -ef |grep redis d. 客户端测试连接 12345678910111213141516[root@VM_24_98_centos ~]# /usr/local/redis/bin/redis-server /usr/local/redis/redis-cluster/redis-6391/redis.conf[root@VM_24_98_centos ~]# /usr/local/redis/bin/redis-cli -h 127.0.0.1 -p 6391 -a foobared127.0.0.1:6391&gt; CLUSTER INFOcluster_state:failcluster_slots_assigned:0cluster_slots_ok:0cluster_slots_pfail:0cluster_slots_fail:0cluster_known_nodes:1cluster_size:0cluster_current_epoch:0cluster_my_epoch:0cluster_stats_messages_sent:0cluster_stats_messages_received:0 e. 同理，集群服务器 redis-6392、redis-6393 、redis-6394、redis-6395、redis-6396 按照上面的步骤部署 3、Redis 5.0 开始不再使用 ruby 搭建集群，而是直接使用客户端命令 redis-cli 来创建。 a. 创建顺序三主三从，前面三个是主后面三个是从。由于我们设置了redis集群的密码，所以要带上密码。 1[root@VM_24_98_centos ~]# /usr/local/redis/bin/redis-cli --cluster create 127.0.0.1:6391 127.0.0.1:6393 127.0.0.1:6395 127.0.0.1:6392 127.0.0.1:6394 127.0.0.1:6396 --cluster-replicas 1 -a foobared b. 客户端测试连接 123456789101112131415161718192021222324252627[root@VM_24_98_centos ~]# /usr/local/redis/bin/redis-cli -h 127.0.0.1 -p 6391 -a foobared127.0.0.1:6391&gt; CLUSTER INFOcluster_state:okcluster_slots_assigned:16384cluster_slots_ok:16384cluster_slots_pfail:0cluster_slots_fail:0cluster_known_nodes:6cluster_size:3cluster_current_epoch:6cluster_my_epoch:1cluster_stats_messages_ping_sent:1153cluster_stats_messages_pong_sent:1241cluster_stats_messages_sent:2394cluster_stats_messages_ping_received:1236cluster_stats_messages_pong_received:1153cluster_stats_messages_meet_received:5cluster_stats_messages_received:2394127.0.0.1:6391&gt; CLUSTER NODEScdfd726c3c35586770412cade1fe5c56d4285b0e 127.0.0.1:6394@16394 slave c54ebef126b30b5bd9cb157ccc0b6ddb952a1f50 0 1574234724000 5 connectedc8daea9291a5cfdf0b3e032bc3a80d7e3cbc75cc 127.0.0.1:6393@16393 master - 0 1574234725711 2 connected 5461-1092243c325955c74f0ed79de6850dca8a509195acb13 127.0.0.1:6392@16392 slave 718f66ab8b3574597a97b90f8257773d0483c556 0 1574234724000 4 connected3a32043079bf6af3723230ee3e6412e84dd66180 127.0.0.1:6396@16396 slave c8daea9291a5cfdf0b3e032bc3a80d7e3cbc75cc 0 1574234724708 6 connectedc54ebef126b30b5bd9cb157ccc0b6ddb952a1f50 127.0.0.1:6391@16391 myself,master - 0 1574234725000 1 connected 0-5460718f66ab8b3574597a97b90f8257773d0483c556 127.0.0.1:6395@16395 master - 0 1574234723000 3 connected 10923-16383 Redis Cluster 场景测试（1）模拟场景：Redis Cluster 中 某个 Master 节点挂掉，查看 Redis Cluster 状态。 Step1、关掉 Cluster-Master-6391 节点 123[root@VM_24_98_centos ~]# /usr/local/redis/bin/redis-cli -h 127.0.0.1 -p 6391 -a foobared127.0.0.1:6391&gt; SHUTDOWN Step2、查看集群状态 123456789[root@VM_24_98_centos ~]# /usr/local/redis/bin/redis-cli -h 127.0.0.1 -p 6393 -a foobared127.0.0.1:6393&gt; CLUSTER NODES43c325955c74f0ed79de6850dca8a509195acb13 127.0.0.1:6392@16392 slave 718f66ab8b3574597a97b90f8257773d0483c556 0 1574236204772 4 connected3a32043079bf6af3723230ee3e6412e84dd66180 127.0.0.1:6396@16396 slave c8daea9291a5cfdf0b3e032bc3a80d7e3cbc75cc 0 1574236206778 6 connectedcdfd726c3c35586770412cade1fe5c56d4285b0e 127.0.0.1:6394@16394 master - 0 1574236201000 7 connected 0-5460718f66ab8b3574597a97b90f8257773d0483c556 127.0.0.1:6395@16395 master - 0 1574236206000 3 connected 10923-16383c54ebef126b30b5bd9cb157ccc0b6ddb952a1f50 127.0.0.1:6391@16391 master,fail - 1574236049092 1574236047289 1 disconnectedc8daea9291a5cfdf0b3e032bc3a80d7e3cbc75cc 127.0.0.1:6393@16393 myself,master - 0 1574236204000 2 connected 5461-10922 通过 CLUSTER NODES 信息可以看到，Cluster-Master-01-6391 主节点处于下线状态（fail），其 Cluster-Master-01-6391 节点的从节点 Cluster-Slave-01-6394 变为主节点；说明主节点挂掉后，6394 Slave 节点自动升级成为了 Master 节点。 Step3、启动 6391 Redis 服务，然后查看节点角色，此时 6391 变成了 Slave，6394 为 Master 节点 1234567891011[root@VM_24_98_centos ~]# /usr/local/redis/bin/redis-server /usr/local/redis/redis-cluster/redis-6391/redis.conf[root@VM_24_98_centos ~]# /usr/local/redis/bin/redis-cli -h 127.0.0.1 -p 6391 -a foobared127.0.0.1:6391&gt; CLUSTER NODES43c325955c74f0ed79de6850dca8a509195acb13 127.0.0.1:6392@16392 slave 718f66ab8b3574597a97b90f8257773d0483c556 0 1574238264397 4 connectedc54ebef126b30b5bd9cb157ccc0b6ddb952a1f50 127.0.0.1:6391@16391 myself,slave cdfd726c3c35586770412cade1fe5c56d4285b0e 0 1574238261000 1 connected3a32043079bf6af3723230ee3e6412e84dd66180 127.0.0.1:6396@16396 slave c8daea9291a5cfdf0b3e032bc3a80d7e3cbc75cc 0 1574238265400 6 connectedc8daea9291a5cfdf0b3e032bc3a80d7e3cbc75cc 127.0.0.1:6393@16393 master - 0 1574238263000 2 connected 5461-10922718f66ab8b3574597a97b90f8257773d0483c556 127.0.0.1:6395@16395 master - 0 1574238263000 3 connected 10923-16383cdfd726c3c35586770412cade1fe5c56d4285b0e 127.0.0.1:6394@16394 master - 0 1574238264000 7 connected 0-5460 （2）模拟场景：为 Redis Cluster 添加一个新主（master）节点 Step1、按照上面的步骤新增一 Redis Cluster 服务器 Cluster-Master-04-6397 Step2、将 Cluster-Master-04-6397 节点加入 Redis Cluster 中（127.0.0.1:6391 为集群中任意可用的节点） 1[root@VM_24_98_centos ~]# /usr/local/redis/bin/redis-cli --cluster add-node 127.0.0.1:6397 127.0.0.1:6391 -a foobared Step3、为节点 Cluster-Master-04-6397 分配 slots（127.0.0.1:6391 为集群中任意可用的节点） 1[root@VM_24_98_centos redis-cluster]# /usr/local/redis/bin/redis-cli --cluster reshard 127.0.0.1:6391 -a foobared （3）模拟场景：为 Redis Cluster 某个 Master 节点添加 一个新从（slave）节点 Step1、按照上面的步骤新增一 Redis Cluster 服务器 Cluster-Slave-04-6398 Step2、将 Cluster-Slave-04-6398 节点加入 Redis Cluster 中（127.0.0.1:6391 为集群中任意可用的节点） 12345// 这种方法随机为 6398 指定一个 master[root@VM_24_98_centos ~]# /usr/local/redis/bin/redis-cli --cluster add-node 127.0.0.1:6398 127.0.0.1:6391 --cluster-slave -a foobared// 这种方式将为 6398 指定某个 master-id[root@VM_24_98_centos ~]# /usr/local/redis/bin/redis-cli --cluster add-node 127.0.0.1:6398 127.0.0.1:6391 --cluster-slave --cluster-master-id 5cf471cf39f0104f69d06c80d0dfdcc8aaa96b7c -a foobared Step3、查看集群状态 1234567891011[root@VM_24_98_centos ~]# /usr/local/redis/bin/redis-cli -h 127.0.0.1 -p 6391 -a foobared127.0.0.1:6391&gt; CLUSTER NODES5cf471cf39f0104f69d06c80d0dfdcc8aaa96b7c 127.0.0.1:6397@16397 master - 0 1574243297701 7 connected 0-1364 5461-6826 10923-12287207628f6fb8b3bb9a22db757507350fb880d4990 127.0.0.1:6396@16396 slave 94af5d349465be57400b6a4a1d770d56ad3d94ce 0 1574243294000 6 connected94af5d349465be57400b6a4a1d770d56ad3d94ce 127.0.0.1:6393@16393 master - 0 1574243296700 2 connected 6827-109222e0134b0a87a73903d4774b6b37dd43e78e93733 127.0.0.1:6391@16391 myself,master - 0 1574243292000 1 connected 1365-546021a288afc7b6addebcd943ca606dd34f6b9c99db 127.0.0.1:6398@16398 slave 5cf471cf39f0104f69d06c80d0dfdcc8aaa96b7c 0 1574243295697 7 connected63bc9da88066b475bd878a56a11dd18023b211b6 127.0.0.1:6394@16394 slave 2e0134b0a87a73903d4774b6b37dd43e78e93733 0 1574243295000 5 connectedc3d20b7f2df806ec87f3d45a7e334b5a2d3abe5b 127.0.0.1:6395@16395 master - 0 1574243296000 3 connected 12288-16383f6a7c788d9e5d40bc62a3723ba02c25607cc2825 127.0.0.1:6392@16392 slave c3d20b7f2df806ec87f3d45a7e334b5a2d3abe5b 0 1574243293694 4 connected 参考博文[1]. 一文搞懂 Raft 算法[2]. Redis 哨兵模式实现主从故障互切换[3]. Redis cluster tutorial[4]. redis cluster 的Gossip协议介绍 注脚[1]. 领头 Sentinel：Sentinel 系统选举领头 Sentinel 的方式是对 Raft 算法的领头选举方法的实现，Raft 算法是一个共识算法，是工程上使用较为广泛的强一致性、去中心化、高可用的分布式协议。[2]. Gossip 协议：Gossip protocol 也叫 Epidemic Protocol（流行病协议），实际上它还有很多别名，比如：“流言算法”、“疫情传播算法” 等。Gossip 过程是由种子节点发起，当一个种子节点有状态需要更新到网络中的其他节点时，它会随机的选择周围几个节点散播消息，收到消息的节点也会重复该过程，直至最终网络中所有的节点都收到了消息。这个过程可能需要一定的时间，由于不能保证某个时刻所有节点都收到消息，但是理论上最终所有节点都会收到消息，因此它是一个“最终一致性协议”。 Redis 深度探险系列 Redis 深度探险（一）：那些绕不过去的 Redis 知识点 Redis 深度探险（二）：Redis 深入之道 Redis 深度探险（三）：Redis 单机环境搭建以及配置说明 Redis 深度探险（四）：Redis 高可用性解决方案之哨兵与集群]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>高可用</tag>
        <tag>集群</tag>
        <tag>哨兵</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大话数据结构（二）：大白话布隆过滤器 Bloom Filter]]></title>
    <url>%2Farchives%2F6e0aae0d.html</url>
    <content type="text"><![CDATA[前言“一个网站有 20 亿 url 存在一个黑名单中，这个黑名单要怎么存？若此时随便输入一个 url，你如何快速判断该 url 是否在这个黑名单中？并且需在给定内存空间（比如：500M）内快速判断出。” 这是一道经常在面试中出现的算法题。 很多人脑海中首先想到的可能是 HashSet，因为 HashSet 的底层是采用 HashMap 实现的，理论上时间复杂度为：O(1)。达到了快速的目的，但是空间复杂度呢？URL 字符串通过 Hash 得到一个 Integer 的值，Integer 占 4 个字节，那 20 亿个 URL 理论上需要：4 字节 (byte) * 20 亿 = 80 亿 (byte) ≈ 7.45G 的内存空间，不满足空间复杂度的要求。 还有一种方法就是位图法[1]，每个 URL 取整数哈希值，置于位图相应的位置上，看上去是可行的。但位图适合对海量的、取值分布很均匀的集合去重。位图法的所占空间随集合内最大元素的增大而增大，即空间复杂度随集合内最大元素增大而线性增大。要设计冲突率很低的哈希函数，势必要增加哈希值的取值范围，4G 的位图最大值是 320 亿左右，为 50 亿条 URL 设计冲突率很低、最大值为 320 亿的哈希函数比较困难。这就会带来一个问题，如果查找的元素数量少但其中某个元素的值很大，比如数字范围是 1 到 1000 亿，那消耗的空间不容乐观。因此，出于性能和内存占用的考虑，在这里使用布隆过滤器才是最好的解决方案：布隆过滤器是对位图的一种改进。 这里就引出本文要介绍的 “布隆过滤器”。 Bloom Filter 概述布隆过滤器（Bloom Filter）是 1970 年由布隆提出的。它实际上是一个很长的二进制向量和一系列随机映射函数。布隆过滤器可以用于检索一个元素是否在一个集合中。它的优点是空间效率和查询时间都远远超过一般的算法，缺点是有一定的误识别率和删除困难。 本质上布隆过滤器是一种数据结构，比较巧妙的概率型数据结构（probabilistic data structure），特点是高效地插入和查询，可以用来告诉你 “某样东西一定不存在或者可能存在”。相比于传统的 List、Set、Map 等数据结构，它更高效、占用空间更少，但是缺点是其返回的结果是概率性的，而不是确切的。 布隆过滤器可以理解为一个不怎么精确的 set 结构，当你使用它的 contains 方法判断某个对象是否存在时，它可能会误判。但是布隆过滤器也不是特别不精确，只要参数设置的合理，它的精确度可以控制的相对足够精确，只会有小小的误判概率。 Bloom Filter 原理布隆过滤器的原理是，当一个元素被加入集合时，通过 K 个散列函数将这个元素映射成一个位数组中的 K 个点，把它们置为 1。检索时，我们只要看看这些点是不是都是 1 就（大约）知道集合中有没有它了：如果这些点有任何一个 0，则被检元素一定不在；如果都是 1，则被检元素很可能在。这就是布隆过滤器的基本思想。 Bloom Filter 跟单哈希函数 Bit-Map 不同之处在于：Bloom Filter 使用了 k 个哈希函数，每个字符串跟 k 个 bit 对应，从而降低了冲突的概率。 应用场景主要是解决大规模数据下不需要精确过滤的场景。 检查垃圾邮件地址：如果用哈希表，每存储一亿个 email 地址，就需要 1.6GB 的内存（用哈希表实现的具体办法是将每一个 email 地址对应成一个八字节的信息指纹，然后将这些信息指纹存入哈希表，由于哈希表的存储效率一般只有 50%，因此一个 email 地址需要占用十六个字节。一亿个地址大约要 1.6GB，即十六亿字节的内存）。因此存贮几十亿个邮件地址可能需要上百 GB 的内存。而 Bloom Filter 只需要哈希表 1/8 到 1/4 的大小就能解决同样的问题。布隆过滤器决不会漏掉任何一个在黑名单中的可疑地址。至于误判问题，常见的补救办法是在建立一个小的白名单，存储那些可能被误判的清白邮件地址。 解决缓存穿透：缓存穿透，是指查询一个数据库中不一定存在的数据。正常情况下，查询先进行缓存查询，如果 key 不存在或者 key 已经过期，再对数据库进行查询，并将查询到的对象放进缓存。如果每次都查询一个数据库中不存在的 key，由于缓存中没有数据，每次都会去查询数据库，很可能会对数据库造成影响。缓存穿透的一种解决办法是为不存在的 key 缓存一个空值，直接在缓存层返回。这样做的弊端就是缓存太多空值占用了太多额外的空间，这点可以通过给缓存层空值设立一个较短的过期时间来解决。另一种解决办法就是使用布隆过滤器，查询一个 key 时，先使用布隆过滤器进行过滤，如果判断请求查询 key 值存在，则继续查询数据库；如果判断请求查询不存在，直接丢弃。 Key-Value 缓存系统的 Key 校验：在很多 Key-Value 系统中也使用了布隆过滤器来加快查询过程，如 Hbase，Accumulo，Leveldb，一般而言，Value 保存在磁盘中，访问磁盘需要花费大量时间，然而使用布隆过滤器可以快速判断某个 Key 对应的 Value 是否存在，因此可以避免很多不必要的磁盘 IO 操作，只是引入布隆过滤器会带来一定的内存消耗。 Google 的 BigTable：Google 的 BigTable 也使用了布隆过滤器，以减少不存在的行或列在磁盘上的 I/O。 新闻客户端推荐系统：推荐系统给用户推荐新闻，避免重复推送。 爬虫 URL 地址去重：爬虫时判断某个 URL 是否已经被爬取过。 单词拼写检查 黑名单过滤 … 代码实现HashSet 实现利用 HashSet 实现黑名单过滤，写入和判断元素是否存在都有对应的 API，所以实现起来也比较简单。 通过单元测试演示 HashSet 实现黑名单过滤功能；同时为了前后的对比将堆内存写死（-Xms64m -Xmx64m），为了方便调试加入了 GC 日志的打印（-XX:+PrintHeapAtGC），以及内存溢出后 Dump 内存（-XX:+HeapDumpOnOutOfMemoryError）。 1-Xms64m -Xmx64m -XX:+PrintHeapAtGC -XX:+HeapDumpOnOutOfMemoryError 1、写入 100 条数据时： 12345678910111213141516@Testpublic void testHashSet() &#123; long start = System.currentTimeMillis(); Set&lt;Integer&gt; hashSet = new HashSet&lt;&gt;(10000000); for (int i = 0; i &lt; 10000000; i++) &#123; hashSet.add(i); &#125; Assert.assertTrue(hashSet.contains(1)); Assert.assertTrue(hashSet.contains(2)); Assert.assertTrue(hashSet.contains(3)); long end = System.currentTimeMillis(); System.out.printf("执行时间：%s \n", end - start);&#125; 2、写入 1000 W 条数据时： 12345678910111213141516@Testpublic void testHashSet() &#123; long start = System.currentTimeMillis(); Set&lt;Integer&gt; hashSet = new HashSet&lt;&gt;(10000000); for (int i = 0; i &lt; 10000000; i++) &#123; hashSet.add(i); &#125; Assert.assertTrue(hashSet.contains(1)); Assert.assertTrue(hashSet.contains(2)); Assert.assertTrue(hashSet.contains(3)); long end = System.currentTimeMillis(); System.out.printf("执行时间：%s \n", end - start);&#125; 执行后马上就内存溢出，可见在内存有限的情况下我们不能使用这种方式。 Guava 中的布隆过滤器BloomFilter 实现的一个重点就是怎么利用 hash 函数把数据映射到 bit 数组中。Guava 的实现是对元素通过 MurmurHash3 计算 hash 值，将得到的 hash 值取高 8 个字节以及低 8 个字节进行计算，以得当前元素在 bit 数组中对应的多个位置。 Guava 会通过你预计的数量以及误报率帮你计算出你应当会使用的数组大小 numBits 以及需要计算几次 Hash 函数 numHashFunctions 。 1、组件依赖：通过 Maven 引入 Guava 开源组件，在 pom.xml 文件加入下面的代码： 12345&lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;28.1-jre&lt;/version&gt;&lt;/dependency&gt; 2、代码实现：通过 Java 代码实现布隆过滤器 12345678910111213141516171819202122232425262728293031323334353637383940@Testpublic void testGuava() &#123; long start = System.currentTimeMillis(); // 预期数据量 long expectedInsertions = 10000000; // 预期误判率 double fpp = 0.001; // 创建存储 Integer 数据的布隆过滤器 BloomFilter&lt;Integer&gt; bloomFilter = BloomFilter.create(Funnels.integerFunnel(), expectedInsertions, fpp); // 插入数据 for (int i = 0; i &lt; 10000000; i++) &#123; bloomFilter.put(i); &#125; // 判断过滤器中是否存在元素 int count = 0; for (int i = 0; i &lt; 10000000; i++) &#123; if (bloomFilter.mightContain(i)) &#123; count++; &#125; &#125; System.out.printf("预期数据量：%s，判断数量：%s，预期误判率：%s，存在比率：%s \n", 10000000, count, fpp, count * 1.0 / expectedInsertions); // 判断误判比例 int fppCount = 0; for (int i = 10000000; i &lt; 20000000; i++) &#123; if (bloomFilter.mightContain(i)) &#123; fppCount++; System.out.println(i + "误判了"); &#125; &#125; System.out.printf("预期数据量：%s，误判数量：%s，预期误判率：%s，实际误判率：%s \n", 10000000, fppCount, fpp, fppCount * 1.0 / expectedInsertions); long end = System.currentTimeMillis(); System.out.printf("执行时间：%s \n", end - start);&#125; 代码分析：我们的定义了一个预期数据量为 1000 W、预期误判率为 0.001 的布隆过滤器，接下来向布隆过滤器中插入了 0-10000000 数据，然后用 0- 10000000 以及 10000000-20000000 数据来测试误判率。 （1）经过测试：“预期数据量：10000000，判断数量：10000000，预期误判率：0.001，存在比率：1.0 ”，可发现当过滤器判断 0-10000000 的数据时，存在比率为 1.0，即布隆过滤器对于已经见过的元素肯定不会误判，它只会误判那些没见过的元素。 （2）经过测试：“预期数据量：10000000，误判数量：10132，预期误判率：0.001，实际误判率：0.0010132” ，符合预期误判率：0.001。 Redis 中的布隆过滤器安装 Rebloom 插件1、安装 Rebloom 插件：Redis 安装在这里就不介绍了，这里讲一下 Rebloom 插件安装。 123456// 下载 Rebloom 源文件$ git clone git://github.com/RedisLabsModules/rebloom// 进入 Rebloom 目录$ cd rebloom// 编译插件$ make 2、加载 Rebloom 插件方法 （1）、在启动的 client 中使用 MODULE LOAD 命令去加载（重启 Redis 后失效） 1MODULE LOAD /usr/lib64/redis/modules/rebloom/redisbloom.so （2）、命令行加载 Rebloom 插件 1/usr/bin/redis-server /etc/redis.conf --loadmodule /usr/lib64/redis/modules/rebloom/redisbloom.so （3）、在 redis.conf 文件中加入配置信息 1loadmodule /usr/lib64/redis/modules/rebloom/redisbloom.so 3、通过命令测试 Redis Bloom Filter 12345678910111213// 创建一个空的布隆过滤器，并设置一个期望的错误率和初始大小。127.0.0.1:6379&gt; BF.RESERVE userid 0.01 100000// 往过滤器中添加元素。如果 key 不存在，过滤器会自动创建。127.0.0.1:6379&gt; BF.ADD userid '101310299'127.0.0.1:6379&gt; BF.MADD userid '101310299' '101310366' '101310211'// 判断过滤器中是否存在该元素，不存在返回 0，存在返回 1。127.0.0.1:6379&gt; BF.EXISTS userid '101310299'127.0.0.1:6379&gt; BF.EXISTS userid '101310299' '10saa' '101310211'// 获取该 Bloom Filter 使用的内存大小127.0.0.1:6379&gt; Memory Usage userid 通过 Java 代码实现布隆过滤器1、组件依赖：通过 Maven 引入 Redisson [2]开源组件，在 pom.xml 文件加入下面的代码： 12345&lt;dependency&gt; &lt;groupId&gt;org.redisson&lt;/groupId&gt; &lt;artifactId&gt;redisson&lt;/artifactId&gt; &lt;version&gt;3.9.1&lt;/version&gt;&lt;/dependency&gt; 2、代码实现：通过 Java 代码实现布隆过滤器 123456789101112131415161718192021222324252627282930313233343536373839404142@Testpublic void testRedis() &#123; long start = System.currentTimeMillis(); // 设置 Redis 连接信息 Config config = new Config(); config.useSingleServer().setAddress("redis://127.0.0.1:6379") .setPassword("password") .setDatabase(0); // 创建 Redisson 客户端 RedissonClient redissonClient = Redisson.create(config); // Redisson 利用 Redis 实现了 Java 分布式布隆过滤器（Bloom Filter） RBloomFilter&lt;Integer&gt; bloomFilter = redissonClient.getBloomFilter("userid"); // 预期数据量 long expectedInsertions = 10000; // 预期误判率 double fpp = 0.01; // 初始化布隆过滤器，预计统计元素数量为 10000，期望误差率为 0.001 bloomFilter.tryInit(expectedInsertions, fpp); // 插入数据 for (int i = 0; i &lt; expectedInsertions; i++) &#123; bloomFilter.add(i); &#125; // 判断误判比例 int count = 0; for (int i = 10000; i &lt; 20000; i++) &#123; if (bloomFilter.contains(i)) &#123; count++; System.out.println(i + "误判了"); &#125; &#125; System.out.printf("预期数据量：%s，误判数量：%s，预期误判率：%s，实际误判率：%s \n", expectedInsertions, count, fpp, count * 1.0 / 10000); long end = System.currentTimeMillis(); System.out.printf("执行时间：%s \n", end - start);&#125; 代码分析：Redisson 利用 Redis 实现了 Java 分布式布隆过滤器（Bloom Filter），通过 RedissonClient 初始化布隆过滤器，预计统计元素数量为 10000，期望误差率为 0.001。“预期数据量：10000，误判数量：51，预期误判率：0.001，实际误判率：0.00255 ”。我们看到了误判率大约 0.255%，比预计的 0.1% 高，不过布隆的概率是有误差的，只要不比预计误判率高太多，都是正常现象。 总结在使用 Bloom Filter 时，绕不过的两点是预估数据量 n 以及期望的误判率 fpp，在实现 Bloom Filter 时，绕不过的两点就是 hash 函数的选取以及 bit 数组的大小。 期望的误判率越低，需要的空间越大。预估数据量，当实际数量超出这个数值时，误判率会上升。因此用户在使用之前一定要尽可能地精确估计好元素数量，还需要加上一定的冗余空间以避免实际元素可能会意外高出估计值很多导致误判率升高。 布隆过滤器的空间占用有一个简单的计算公式，但是推导比较繁琐，这里就省去推导过程了，感兴趣的读者可以点击「延伸阅读」深入理解公式的推导过程。虽然存在布隆过滤器的空间占用的计算公司，但是有很多现成的网站已经支持计算空间占用的功能了，我们只要把参数输进去，就可以直接看到结果，比如 布隆计算器。 延伸阅读 布隆过滤器 (Bloom Filter) 详解：详解布隆过滤器的相关算法和参数设计。 Murmur 哈希：Murmur 哈希，于 2008 年被发明。这个算法 hbase、redis、kafka 都在使用。 Counting Bloom Filter：要实现删除元素，可以采用 Counting Bloom Filter。它将标准布隆过滤器位图的每一位扩展为一个小的计数器 (Counter)，插入元素时将对应的 k 个 Counter 的值分别加 1，删除元素时则分别减 1。 布谷鸟过滤器：为了解决布隆过滤器不能删除元素的问题，布谷鸟过滤器横空出世。相比布谷鸟过滤器而言布隆过滤器有以下不足：查询性能弱、空间利用效率低、不支持反向操作（删除）以及不支持计数。 参考博文[1]. 如何判断一个元素在亿级数据中是否存在？[2]. 大数据量下的集合过滤—Bloom Filter[3]. 布隆过滤器 (Bloom Filter) 的原理、实现和探究 注脚[1]. 位图法：位图法就是 BitMap 的缩写，所谓 BitMap，就是用每一位来存放某种状态，适用于大规模数据，但数据状态又不是很多的情况。通常是用来判断某个数据存不存在的。[2]. Redisson：Redisson 在基于 NIO 的 Netty 框架上，充分的利用了 Redis 键值数据库提供的一系列优势，在 Java 实用工具包中常用接口的基础上，为使用者提供了一系列具有分布式特性的常用工具类。使得原本作为协调单机多线程并发程序的工具包获得了协调分布式多机多线程并发系统的能力，大大降低了设计和研发大规模分布式系统的难度。同时结合各富特色的分布式服务，更进一步简化了分布式环境中程序相互之间的协作。 大话数据结构系列 大话数据结构（一）：那些年面试常见的 Java 排序算法 大话数据结构（二）：大白话布隆过滤器 Bloom Filter]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入浅出集合框架（二）：为并发而生的 ConcurrentHashMap]]></title>
    <url>%2Farchives%2F91027543.html</url>
    <content type="text"><![CDATA[前言ConcurrentHashMap 从 JDK 1.5 开始随 java.util.concurrent 包一起引入 JDK 中，主要为了解决 HashMap 线程不安全和 Hashtable 效率不高的问题。 HashMap 是我们日常开发中最常见的一种容器，根据键值对键的哈希值来确定值对键在集合中的存储位置，因此具有良好的存取和查找功能。但众所周知，它在高并发的情境下是线程不安全的。尤其是在 JDK 1.8 之前，rehash 的过程中采用头插法转移结点，高并发下，多个线程同时操作一条链表将直接导致闭链，易出现逆序且环形链表死循环问题，导致死循环并占满 CPU。JDK 1.8 以来，对 HashMap 的内部进行了很大的改进，采用数组 + 链表 + 红黑树的形式来进行数据的存储。rehash 的过程也进行了改动，基于复制的算法思想，不直接操作原链，而是定义了两条链表分别完成对原链的结点分离操作，在多线程的环境下，采用了尾插法，扩容后，新数组中的链表顺序依然与旧数组中的链表顺序保持一致，所有即使是多线程的情况下也是安全的。JDK 1.8 中的 HashMap 虽然不会导致死循环，但是因为 HashMap 多线程下内存不共享的问题，两个线程同时指向一个 hash 桶数组时，会导致数据覆盖的问题，所以 HashMap 是依旧是线程不安全的。 HashTable 是线程安全的容器，它在所有涉及到多线程的操作都加上了 synchronized 关键字来锁住整个 table，这就意味着所有的线程都在竞争一把锁，在多线程的环境下，它是安全的，但是无疑是效率低下的，因此 Hashtable 已经是 Java 中的遗留容器，已经不推荐使用。 因此在多线程条件下，需要满足线程安全，我们可使用 Collections.synchronizedMap 方法构造出一个同步的 Map，使 HashMap 具有线程安全的能力；或者直接使用线程安全的 ConcurrentHashMap。本篇文章将要介绍的 ConcurrentHashMap 是 HashMap 的并发版本，它是线程安全的，并且在高并发的情境下，性能优于 HashMap 很多。 背景 线程不安全的 HashMap 由于 HashMap 是非线程安全的容器，遇到多线程操作同一容器的场景，可能会导致数据不一致： JDK 1.7 中 HashMap 采用了数组 + 链表的数据结构，有线程安全问题（统计不准确，丢失数据，环形链表死循环导致 Cpu 100%），JDK 1.8 中 HashMap 采用了数组 + 链表 + 红黑树的结构，有线程安全问题（统计不准确，丢失数据）。 效率低下的 HashTable 容器 HashTable 容器使用 synchronized 关键字来保证线程安全，但在线程竞争激烈的情况下 HashTable 的效率非常低下。因为当一个线程访问 HashTable 的同步方法时，其他线程访问 HashTable 的同步方法时，可能会进入阻塞或轮询状态。如线程 1 使用 put 进行添加元素，线程 2 不但不能使用 put 方法添加元素，并且也不能使用 get 方法来获取元素，所以竞争越激烈效率越低。 JDK 1.7 基于分段锁的 ConcurrentHashMap数据结构HashTable 容器在竞争激烈的并发环境下表现出效率低下的原因，是因为所有访问 HashTable 的线程都必须竞争同一把锁，那假如容器里有多把锁，每一把锁用于锁容器其中一部分数据，那么当多线程访问容器里不同数据段的数据时，线程间就不会存在锁竞争，从而可以有效的提高并发访问效率，这就是 ConcurrentHashMap 所使用的锁分段技术。JDK1.7 中的 ConcurrentHashMap 的底层数据结构是数组 + 链表。与 HashMap 不同的是，ConcurrentHashMap 最外层不是一个大的数组，而是一个 Segment 的数组。每个 Segment 包含一个与 HashMap 数据结构差不多的链表数组。 JDK 1.7 中 ConcurrentHashMap 采用 Segment 分段锁的数据结构，首先将数据分成一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据的时候，其他段的数据也能被其他线程访问。有些方法需要跨段，比如 size() 和 containsValue()，它们可能需要锁定整个表而而不仅仅是某个段，这需要按顺序锁定所有段，操作完毕后，又按顺序释放所有段的锁。这种做法，就称之为“分离锁”（lock striping）[1] 。 ConcurrentHashMap 是由 Segment 数组结构和 HashEntry 数组结构组成。Segment 是一种可重入锁 ReentrantLock，在 ConcurrentHashMap 里扮演锁的角色，HashEntry 则用于存储键值对数据。一个 ConcurrentHashMap 里包含一个 Segment 数组，Segment 的结构和 HashMap 类似，是一种数组和链表结构， 一个 Segment 里包含一个 HashEntry 数组，每个 HashEntry 是一个链表结构的元素， 每个 Segment 守护者一个 HashEntry 数组里的元素, 当对 HashEntry 数组的数据进行修改时，必须首先获得它对应的 Segment 锁。 存储结构123456789101112131415161718192021222324252627282930313233public class ConcurrentHashMap&lt;K, V&gt; extends AbstractMap&lt;K, V&gt; implements ConcurrentMap&lt;K, V&gt;, Serializable &#123; // 将整个 hashmap 分成几个小的 map，每个 segment 都是一个锁；与 hashtable 相比，这么设计的目的是对于 put, remove 等操作，可以减少并发冲突，对不属于同一个片段的节点可以并发操作，大大提高了性能 final Segment&lt;K,V&gt;[] segments; // 本质上 Segment 类就是一个小的 hashmap，里面 table 数组存储了各个节点的数据，继承了 ReentrantLock, 可以作为互拆锁使用 static final class Segment&lt;K,V&gt; extends ReentrantLock implements Serializable &#123; // count 用来统计该段数据的个数，它是 volatile，它用来协调修改和读取操作，以保证读取操作能够读取到几乎最新的修改。协调方式是这样的，每次修改操作做了结构上的改变，如增加 / 删除节点(修改节点的值不算结构上的改变)，都要写 count 值，每次读取操作开始都要读取 count 的值。这利用了 Java 5 中对 volatile 语义的增强，对同一个 volatile 变量的写和读存在 happens-before 关系。 transient volatileint count; // modCount 统计段结构改变的次数，主要是为了检测对多个段进行遍历过程中某个段是否发生改变 transient int modCount; // threashold 用来表示需要进行 rehash 的界限值 transient int threshold; // table 数组存储段中节点，每个数组元素是个 hash 链，用 HashEntry 表示。table 也是 volatile，这使得能够读取到最新的 table 值而不需要同步 transient volatile HashEntry&lt;K,V&gt;[] table; // loadFactor 表示负载因子。 final float loadFactor; &#125; // 基本节点，存储 Key，value 值，可以看到除了 value 不是 final 的，其它值都是 final 的，这意味着不能从 hash 链的中间或尾部添加或删除节点，因为这需要修改 next 引用值，所有的节点的修改只能从头部开始。对于 put 操作，可以一律添加到 Hash 链的头部。但是对于 remove 操作，可能需要从中间删除一个节点，这就需要将要删除节点的前面所有节点整个复制一遍，最后一个节点指向要删除结点的下一个结点。为了确保读操作能够看到最新的值，将 value 设置成 volatile，这避免了加锁。 static final class HashEntry&lt;K,V&gt; &#123; final int hash; final K key; volatile V value; final HashEntry&lt;K,V&gt; next; &#125;&#125; 构造方法它把区间按照并发级别（concurrentLevel），分成了若干个 segment。默认情况下内部按并发级别为 16 来创建。对于每个 segment 的容量，默认情况也是 16。concurrentLevel，segment 可以通过构造函数设定的。通过按位与的哈希算法来定位 segments 数组的索引，必须保证 segments 数组的长度是 2 的 N 次方（power-of-two size），所以必须计算出一个是大于或等于 concurrencyLevel 的最小的 2 的 N 次方值来作为 segments 数组的长度。假如concurrencyLevel等于14，15或16，ssize都会等于16，即容器里锁的个数也是16。 123456789101112131415161718192021222324252627282930313233public ConcurrentHashMap(int initialCapacity, float loadFactor, int concurrencyLevel) &#123; // 校验参数 if (!(loadFactor &gt; 0) || initialCapacity &lt; 0 || concurrencyLevel &lt;= 0) throw new IllegalArgumentException(); // 并发级别数大于最大Segment数量 if (concurrencyLevel &gt; MAX_SEGMENTS) concurrencyLevel = MAX_SEGMENTS; // Find power-of-two sizes best matching arguments int sshift = 0; int ssize = 1; while (ssize &lt; concurrencyLevel) &#123; ++sshift; ssize &lt;&lt;= 1; &#125; this.segmentShift = 32 - sshift; this.segmentMask = ssize - 1; if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; int c = initialCapacity / ssize; if (c * ssize &lt; initialCapacity) ++c; int cap = MIN_SEGMENT_TABLE_CAPACITY; while (cap &lt; c) cap &lt;&lt;= 1; // create segments and segments[0] Segment&lt;K, V&gt; s0 = new Segment&lt;K, V&gt;(loadFactor, (int) (cap * loadFactor), (HashEntry&lt;K, V&gt;[]) new HashEntry[cap]); Segment&lt;K, V&gt;[] ss = (Segment&lt;K, V&gt;[]) new Segment[ssize]; UNSAFE.putOrderedObject(ss, SBASE, s0); // ordered write of segments[0] this.segments = ss;&#125; 构造方法寻址方式对于读操作，通过 Key 哈希值的高 N 位对 Segment 个数取模从而得到该 Key 应该属于哪个 Segment，接着如同操作 HashMap 一样操作这个 Segment。Segment 继承自 ReentrantLock，所以我们可以很方便的对每一个 Segment 上锁。对于写操作，并不要求同时获取所有 Segment 的锁，因为那样相当于锁住了整个 Map。它会先获取该键值对所在的 Segment 的锁，获取成功后就可以像操作一个普通的 HashMap 一样操作该 Segment，并保证该 Segment 的安全性。为了保证不同的值均匀分布到不同的 Segment，需要通过如下方法计算哈希值。 12345678910111213private int hash(Object k) &#123; int h = hashSeed; if ((0 != h) &amp;&amp; (k instanceof String)) &#123; return sun.misc.Hashing.stringHash32((String) k); &#125; h ^= k.hashCode(); h += (h &lt;&lt; 15) ^ 0xffffcd7d; h ^= (h &gt;&gt;&gt; 10); h += (h &lt;&lt; 3); h ^= (h &gt;&gt;&gt; 6); h += (h &lt;&lt; 2) + (h &lt;&lt; 14); return h ^ (h &gt;&gt;&gt; 16);&#125; 同步方式Segment 继承自 ReentrantLock，所以我们可以很方便的对每一个 Segment 上锁。 对于读操作，获取 Key 所在的 Segment 时，需要保证可见性。具体实现上可以使用 volatile 关键字，也可使用锁。但使用锁开销太大，而使用 volatile 时每次写操作都会让所有 CPU 内缓存无效，也有一定开销。ConcurrentHashMap 使用如下方法保证可见性，取得最新的 Segment。 1234private Segment&lt;K, V&gt; segmentForHash(int h) &#123; long u = (((h &gt;&gt;&gt; segmentShift) &amp; segmentMask) &lt;&lt; SSHIFT) + SBASE; return (Segment&lt;K, V&gt;) UNSAFE.getObjectVolatile(segments, u);&#125; 对于写操作，并不要求同时获取所有 Segment 的锁，因为那样相当于锁住了整个 Map。它会先获取该 Key-Value 对所在的 Segment 的锁，获取成功后就可以像操作一个普通的 HashMap 一样操作该 Segment，并保证该 Segment 的安全性。同时由于其它 Segment 的锁并未被获取，因此理论上可支持 concurrencyLevel（等于 Segment 的个数）个线程安全的并发读写。 获取锁时，并不直接使用 lock 来获取，因为该方法获取锁失败时会挂起（参考可重入锁）。事实上，它使用了自旋锁，如果 tryLock 获取锁失败，说明锁被其它线程占用，此时通过循环再次以 tryLock 的方式申请锁。如果在循环过程中该 Key 所对应的链表头被修改，则重置 retry 次数。如果 retry 次数超过一定值，则使用 lock 方法申请锁。 这里使用自旋锁是因为自旋锁的效率比较高，但是它消耗 CPU 资源比较多，因此在自旋次数超过阈值时切换为互斥锁。 size 操作put、remove 和 get 操作只需要关心一个 Segment，而 size 操作需要遍历所有的 Segment 才能算出整个 Map 的大小。一个简单的方案是，先锁住所有 Sgment，计算完后再解锁。但这样做，在做 size 操作时，不仅无法对 Map 进行写操作，同时也无法进行读操作，不利于对 Map 的并行操作。 为更好支持并发操作，ConcurrentHashMap 会在不上锁的前提逐个 Segment 计算 3 次 size，如果某相邻两次计算获取的所有 Segment 的更新次数（每个 Segment 都与 HashMap 一样通过 modCount 跟踪自己的修改次数，Segment 每修改一次其 modCount 加一）相等，说明这两次计算过程中无更新操作，则这两次计算出的总 size 相等，可直接作为最终结果返回。如果这三次计算过程中 Map 有更新，则对所有 Segment 加锁重新计算 Size。该计算方法代码如下 12345678910111213141516171819202122232425262728293031323334353637public int size() &#123; final Segment&lt;K,V&gt;[] segments = this.segments; int size; boolean overflow; // true if size overflows 32 bits long sum; // sum of modCounts long last = 0L; // previous sum int retries = -1; // first iteration isn't retry try &#123; for (;;) &#123; if (retries++ == RETRIES_BEFORE_LOCK) &#123; for (int j = 0; j &lt; segments.length; ++j) ensureSegment(j).lock(); // force creation &#125; sum = 0L; size = 0; overflow = false; for (int j = 0; j &lt; segments.length; ++j) &#123; Segment&lt;K,V&gt; seg = segmentAt(segments, j); if (seg != null) &#123; sum += seg.modCount; int c = seg.count; if (c &lt; 0 || (size += c) &lt; 0) overflow = true; &#125; &#125; if (sum == last) break; last = sum; &#125; &#125; finally &#123; if (retries &gt; RETRIES_BEFORE_LOCK) &#123; for (int j = 0; j &lt; segments.length; ++j) segmentAt(segments, j).unlock(); &#125; &#125; return overflow ? Integer.MAX_VALUE : size;&#125; Java 8 基于 CAS 的 ConcurrentHashMap数据结构JDK 1.7 中 ConcurrentHashMap 为实现并行访问，引入了 Segment 这一结构，实现了分段锁，理论上最大并发度与 Segment 个数相等。JDK 1.8 中 ConcurrentHashMap 为进一步提高并发性，摒弃了分段锁的方案，而是直接使用一个大的数组。 JDK 1.8 中 ConcurrentHashMap 参考了 JDK 1.8 HashMap 的实现，采用了数组 + 链表 + 红黑树的实现方式进行数据存储，提高哈希碰撞下的寻址性能，在链表长度超过一定阈值（8）时将链表（寻址时间复杂度为 O(N)）转换为红黑树（寻址时间复杂度为 O(log(N))）；进一步提高并发性，取消了基于 Segment 的分段锁思想，改用 CAS[2] + synchronized 控制并发操作，在某些方面提升了性能。对于读操作，通过 Key 的哈希值与数组长度取模确定该 Key 在数组中的索引。对于写操作，如果 Key 对应的数组元素为 null，则通过 CAS 操作将其设置为当前值。如果 Key 对应的数组元素不为 null，则对该元素使用 synchronized 关键字申请锁，然后进行操作。 存储结构Node ：Node 是最核心的内部类，它包装了 key-value 键值对，所有插入 ConcurrentHashMap 的数据都包装在这里面。它与 HashMap 中的定义很相似，但是但是有一些差别它对 value 和 next 属性设置了 volatile 同步锁，它不允许调用 setValue 方法直接改变 Node 的 value 域，它增加了 find 方法辅助 map.get() 方法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; // 带有同步锁的 value[value 和 next 都会在扩容时发生变化，所以加上 volatile 来保持可见性和禁止重排序] volatile V val; // 带有同步锁的 next 指针 volatile ConcurrentHashMap.Node&lt;K,V&gt; next; Node(int hash, K key, V val, ConcurrentHashMap.Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.val = val; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return val; &#125; public final int hashCode() &#123; return key.hashCode() ^ val.hashCode(); &#125; public final String toString()&#123; return key + "=" + val; &#125; // 不允许直接改变 value 的值 public final V setValue(V value) &#123; throw new UnsupportedOperationException(); &#125; public final boolean equals(Object o) &#123; Object k, v, u; Map.Entry&lt;?,?&gt; e; return ((o instanceof Map.Entry) &amp;&amp; (k = (e = (Map.Entry&lt;?,?&gt;)o).getKey()) != null &amp;&amp; (v = e.getValue()) != null &amp;&amp; (k == key || k.equals(key)) &amp;&amp; (v == (u = val) || v.equals(u))); &#125; /** * Virtualized support for map.get(); overridden in subclasses. */ ConcurrentHashMap.Node&lt;K,V&gt; find(int h, Object k) &#123; ConcurrentHashMap.Node&lt;K,V&gt; e = this; if (k != null) &#123; do &#123; K ek; if (e.hash == h &amp;&amp; ((ek = e.key) == k || (ek != null &amp;&amp; k.equals(ek)))) return e; &#125; while ((e = e.next) != null); &#125; return null; &#125;&#125; TreeNode：树节点类，当链表长度过长的时候，会转换为 TreeNode。但是与 HashMap 不相同的是，它并不是直接转换为红黑树，而是把这些结点包装成 TreeNode 放在 TreeBin 对象中，由 TreeBin 完成对红黑树的包装。而且 TreeNode 在 ConcurrentHashMap 集成自 Node 类，而并非 HashMap 中的集成自 LinkedHashMap.Entry&lt;K,V&gt; 类，也就是说 TreeNode 带有 next 指针，这样做的目的是方便基于 TreeBin 的访问。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * Nodes for use in TreeBins */static final class TreeNode&lt;K,V&gt; extends ConcurrentHashMap.Node&lt;K,V&gt; &#123; // 存储当前节点的父节点 ConcurrentHashMap.TreeNode&lt;K,V&gt; parent; // red-black tree links // 存储当前节点的左孩子 ConcurrentHashMap.TreeNode&lt;K,V&gt; left; // 存储当前节点的右孩子 ConcurrentHashMap.TreeNode&lt;K,V&gt; right; // 存储当前节点的前一个节点 ConcurrentHashMap.TreeNode&lt;K,V&gt; prev; // needed to unlink next upon deletion // 存储当前节点的颜色（红、黑） boolean red; TreeNode(int hash, K key, V val, ConcurrentHashMap.Node&lt;K,V&gt; next, ConcurrentHashMap.TreeNode&lt;K,V&gt; parent) &#123; super(hash, key, val, next); this.parent = parent; &#125; ConcurrentHashMap.Node&lt;K,V&gt; find(int h, Object k) &#123; return findTreeNode(h, k, null); &#125; /** * Returns the TreeNode (or null if not found) for the given key * starting at given root. */ final ConcurrentHashMap.TreeNode&lt;K,V&gt; findTreeNode(int h, Object k, Class&lt;?&gt; kc) &#123; if (k != null) &#123; ConcurrentHashMap.TreeNode&lt;K,V&gt; p = this; do &#123; int ph, dir; K pk; ConcurrentHashMap.TreeNode&lt;K,V&gt; q; ConcurrentHashMap.TreeNode&lt;K,V&gt; pl = p.left, pr = p.right; if ((ph = p.hash) &gt; h) p = pl; else if (ph &lt; h) p = pr; else if ((pk = p.key) == k || (pk != null &amp;&amp; k.equals(pk))) return p; else if (pl == null) p = pr; else if (pr == null) p = pl; else if ((kc != null || (kc = comparableClassFor(k)) != null) &amp;&amp; (dir = compareComparables(kc, k, pk)) != 0) p = (dir &lt; 0) ? pl : pr; else if ((q = pr.findTreeNode(h, k, kc)) != null) return q; else p = pl; &#125; while (p != null); &#125; return null; &#125;&#125; 构造方法concurrencyLevel，能够同时更新 ConccurentHashMap 且不产生锁竞争的最大线程数，在 JDK 1.8 之前实际上就是 ConcurrentHashMap 中的分段锁个数，即 Segment[] 的数组长度。在 JDK 1.8 里，仅仅是为了兼容旧版本而保留，唯一的作用就是保证构造 map 时初始容量不小于 concurrencyLevel。 12345678public ConcurrentHashMap(int initialCapacity) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(); int cap = ((initialCapacity &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(initialCapacity + (initialCapacity &gt;&gt;&gt; 1) + 1)); this.sizeCtl = cap;&#125; 基本属性12345678910111213141516171819202122232425262728293031323334353637// node 数组最大容量：2^30=1073741824private static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;// 默认初始值，必须是 2 的幕数private static final int DEFAULT_CAPACITY = 16;// 数组可能最大值，需要与 toArray（）相关方法关联static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;// 并发级别，遗留下来的，为兼容以前的版本private static final int DEFAULT_CONCURRENCY_LEVEL = 16;// 负载因子private static final float LOAD_FACTOR = 0.75f;// 链表转红黑树阀值 &gt; 8 链表转换为红黑树static final int TREEIFY_THRESHOLD = 8;// 树转链表阀值，小于等于 6（tranfer 时，lc、hc=0 两个计数器分别 ++ 记录原 bin、新 binTreeNode 数量，&lt;=UNTREEIFY_THRESHOLD 则 untreeify(lo)）static final int UNTREEIFY_THRESHOLD = 6;static final int MIN_TREEIFY_CAPACITY = 64;private static final int MIN_TRANSFER_STRIDE = 16;private static int RESIZE_STAMP_BITS = 16;// 2^15-1，help resize 的最大线程数private static final int MAX_RESIZERS = (1 &lt;&lt; (32 - RESIZE_STAMP_BITS)) - 1;// 32-16=16，sizeCtl 中记录 size 大小的偏移量private static final int RESIZE_STAMP_SHIFT = 32 - RESIZE_STAMP_BITS;// forwarding nodes 的 hash 值static final int MOVED = -1;// 树根节点的 hash 值static final int TREEBIN = -2;// ReservationNode 的 hash 值static final int RESERVED = -3;// 可用处理器数量static final int NCPU = Runtime.getRuntime().availableProcessors();// 存放 node 的数组transient volatile Node&lt;K, V&gt;[] table;/* 控制标识符，用来控制 table 的初始化和扩容的操作，不同的值有不同的含义 * 当为负数时：-1 代表正在初始化，-N 代表有 N-1 个线程正在 进行扩容 * 当为 0 时：代表当时的 table 还没有被初始化 * 当为正数时：表示初始化或者下一次进行扩容的大小 */private transient volatile int sizeCtl; 寻址方式JDK 1.8 的 ConcurrentHashMap 同样是通过 Key 的哈希值与数组长度取模确定该 Key 在数组中的索引。同样为了避免不太好的 Key 的 hashCode 设计，它通过如下方法计算得到 Key 的最终哈希值。不同的是，JDK 1.8 的 ConcurrentHashMap 作者认为引入红黑树后，即使哈希冲突比较严重，寻址效率也足够高，所以作者并未在哈希值的计算上做过多设计，只是将 Key 的 hashCode 值与其高 16 位作异或并保证最高位为 0（从而保证最终结果为正整数）。 123static final int spread(int h) &#123; return (h ^ (h &gt;&gt;&gt; 16)) &amp; HASH_BITS;&#125; 同步方式对于写操作，如果 Key 对应的数组元素为 null，则通过 CAS 操作将其设置为当前值。如果 Key 对应的数组元素（也即链表表头或者树的根元素）不为 null，则对该元素使用 synchronized 关键字申请锁，然后进行操作。如果该 put 操作使得当前链表长度超过一定阈值，则将该链表转换为树，从而提高寻址效率。 对于读操作，由于数组被 volatile 关键字修饰，因此不用担心数组的可见性问题。同时每个元素是一个 Node 实例（Java 7 中每个元素是一个 HashEntry），它的 Key 值和 hash 值都由 final 修饰，不可变更，无须关心它们被修改后的可见性问题。而其 Value 及对下一个元素的引用由 volatile 修饰，可见性也有保障。 123456static class Node&lt;K, V&gt; implements Map.Entry&lt;K, V&gt; &#123; final int hash; final K key; volatile V val; volatile Node&lt;K, V&gt; next;&#125; 对于 Key 对应的数组元素的可见性，由 Unsafe 的 getObjectVolatile 方法保证。它是对 tab[i] 进行原子性的读取，因为我们知道 putVal 等对 table 的桶操作是有加锁的，那么一般情况下我们对桶的读也是要加锁的，但是我们这边为什么不需要加锁呢？因为我们用了 Unsafe[3] 的 getObjectVolatile，因为 table 是 volatile 类型，所以对 tab[i] 的原子请求也是可见的。因为如果同步正确的情况下，根据 happens-before 原则，对 volatile 域的写入操作 happens-before 于每一个后续对同一域的读操作。所以不管其他线程对 table 链表或树的修改，都对 get 读取可见。 1234// 获得数组中位置i上的节点static final &lt;K,V&gt; Node&lt;K,V&gt; tabAt(Node&lt;K,V&gt;[] tab, int i) &#123; return (Node&lt;K,V&gt;)U.getObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE);&#125; size 操作put 方法和 remove 方法都会通过 addCount 方法维护 Map 的 size。size 方法通过 sumCount 获取由 addCount 方法维护的 Map 的 size。 12345678910111213141516171819public int size() &#123; long n = sumCount(); return ((n &lt; 0L) ? 0 : (n &gt; (long)Integer.MAX_VALUE) ? Integer.MAX_VALUE : (int)n);&#125;final long sumCount() &#123; // 变化的数量 CounterCell[] as = counterCells; CounterCell a; long sum = baseCount; if (as != null) &#123; for (int i = 0; i &lt; as.length; ++i) &#123; if ((a = as[i]) != null) sum += a.value; &#125; &#125; return sum;&#125; 源码分析 ConcurrentHashMapinitTable()方法该方法的核心思想就是，只允许一个线程对表进行初始化，如果不巧有其他线程进来了，那么会让其他线程交出 CPU 等待下次系统调度。这样，保证了表同时只会被一个线程初始化。 12345678910111213141516171819202122232425262728293031private final Node&lt;K,V&gt;[] initTable() &#123; Node&lt;K,V&gt;[] tab; int sc; // 如果表为空才进行初始化操作 while ((tab = table) == null || tab.length == 0) &#123; // sizeCtl 小于零说明已经有线程正在进行初始化操作，当前线程应该放弃 CPU 的使用 if ((sc = sizeCtl) &lt; 0) Thread.yield(); // lost initialization race; just spin // 否则说明还未有线程对表进行初始化，那么本线程就来做这个工作，CAS 方法把 sizectl 置为-1，表示本线程正在进行初始化 else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; // 保险起见，再次判断下表是否为空 try &#123; if ((tab = table) == null || tab.length == 0) &#123; // sc 大于零说明容量已经初始化了，否则使用默认容量 16 int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; @SuppressWarnings("unchecked") // 根据容量构建数组 Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; // 将这个数组赋值给 table，table 是 volatile 的 table = tab = nt; // 计算阈值，等效于 n*0.75 sc = n - (n &gt;&gt;&gt; 2); &#125; &#125; finally &#123; // 设置阈值 sizeCtl = sc; &#125; break; &#125; &#125; return tab;&#125; get(Object key)方法get 方法比较简单，给定一个 key 来确定 value 的时候，必须满足两个条件 key 相同 、hash 值相同，对于节点可能在链表或树上的情况，需要分别去查找。 123456789101112131415161718192021222324public V get(Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; // 计算 hash 值 int h = spread(key.hashCode()); // 根据 hash 值确定节点位置 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) &#123; // 如果搜索到的节点 key 与传入的 key 相同且不为 null, 直接返回这个节点 if ((eh = e.hash) == h) &#123; if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; &#125; // 如果 eh&lt;0 说明这个节点在树上，调用树的 find 方法寻找 else if (eh &lt; 0) return (p = e.find(h, key)) != null ? p.val : null; // 否则遍历链表 找到对应的值并返回 while ((e = e.next) != null) &#123; if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; &#125; &#125; return null;&#125; ConcurrentHashMap 的 get 操作的流程很简单，可以分为三个步骤来描述： 计算 hash 值，定位到该 table 索引位置，如果是首节点符合就返回 如果遇到扩容的时候，会调用标志正在扩容节点 ForwardingNode 的 find 方法，查找该节点，匹配就返回 以上都不符合的话，就往下遍历节点，匹配就返回，否则最后就返回 null put(K key, V value)方法假设 table 已经初始化完成，put 操作采用 CAS+synchronized 实现并发插入或更新操作，具体实现如下。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788public V put(K key, V value) &#123; return putVal(key, value, false);&#125;/** * Implementation for put and putIfAbsent */final V putVal(K key, V value, boolean onlyIfAbsent) &#123; // 不允许 key 或 value 为 null if (key == null || value == null) throw new NullPointerException(); // 计算 hash 值，两次 hash，减少 hash 冲突，可以均匀分布 int hash = spread(key.hashCode()); int binCount = 0; // 死循环 何时插入成功 何时跳出；因为如果其他线程正在修改 tab，那么尝试就会失败，所以这边要加一个 for 循环，不断的尝试 for (Node&lt;K, V&gt;[] tab = table; ; ) &#123; Node&lt;K, V&gt; f; int n, i, fh; // 如果 table 为空的话，初始化 table，属于懒汉模式初始化 if (tab == null || (n = tab.length) == 0) tab = initTable(); // 根据 hash 值计算出在 table 里面的位置 else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; // 如果这个位置没有值，通过 CAS 操作将其设置为当前值，不需要加锁 if (casTabAt(tab, i, null, new Node&lt;K, V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin &#125; // 当遇到表连接点时，需要进行整合表的操作 else if ((fh = f.hash) == MOVED) // 由于检测到当前哈希表正在扩容，于是让当前线程去协助扩容 tab = helpTransfer(tab, f); else &#123; V oldVal = null; // 这个地方设计非常的巧妙，内置锁 synchronized 锁住了 f,因为 f 是指定特定的tab[i]的；结点上锁，这里的结点可以理解为 hash 值相同组成的链表的头结点 synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; // fh &gt;= 0 说明这个节点是一个链表的节点 不是树的节点 if (fh &gt;= 0) &#123; binCount = 1; // 在这里遍历链表所有的结点 for (Node&lt;K, V&gt; e = f; ; ++binCount) &#123; K ek; // 如果 hash 值和 key 值相同 则修改对应结点的 value 值[put 操作和 putIfAbsent 操作业务实现] if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; &#125; Node&lt;K, V&gt; pred = e; // 如果遍历到了最后一个结点，那么就证明新的节点需要插入，使用尾插法把它插入在链表尾部 if ((e = e.next) == null) &#123; pred.next = new Node&lt;K, V&gt;(hash, key, value, null); break; &#125; &#125; &#125; // 如果这个节点是树节点，就按照树的方式插入值 else if (f instanceof TreeBin) &#123; Node&lt;K, V&gt; p; binCount = 2; if ((p = ((TreeBin&lt;K, V&gt;) f).putTreeVal(hash, key, value)) != null) &#123; oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; if (binCount != 0) &#123; // 如果链表长度已经达到临界值 8 就需要把链表转换为树结构 if (binCount &gt;= TREEIFY_THRESHOLD) // 转化为红黑树 treeifyBin(tab, i); if (oldVal != null) return oldVal; break; &#125; &#125; &#125; // 将当前 ConcurrentHashMap 的元素数量 + 1，CAS 式更新 baseCount，并判断是否需要扩容 addCount(1L, binCount); return null;&#125; ConcurrentHashMap 的 put 操作的流程就是对当前的 table 进行无条件自循环直到 put 成功，可以分成以下七步流程来概述： 如果为 null 直接抛空指针异常 如果没有初始化就先调用 initTable() 方法来进行初始化过程 如果没有 hash 冲突就直接 casTabAt() 无锁插入 如果还在进行扩容操作就 helpTransfer() 帮助其扩容 如果存在 hash 冲突，就加锁来保证线程安全，这里有两种情况，一种是链表形式就直接遍历到尾端插入，一种是红黑树就按照红黑树结构插入， 如果该链表的数量大于阈值 8，就要先treeifyBin()转换成黑红树的结构[若table length &lt; 64，直接 tryPresize，两倍table.length，不转树]，break 跳出循环 如果添加成功就调用 addCount() 方法统计 size，并且检查是否需要扩容 总结ConcurrentHashMap 与 HashMap 的区别 线程安全性不同：ConcurrentHashMap 线程安全，而 HashMap 非线程安全 key 和 value 是否允许 null 值：HashMap 允许 Key 和 Value 为 null，而 ConcurrentHashMap 不允许 Key 和 Value 为 null 是否允许遍历时修改集合：HashMap 不允许通过 Iterator 遍历的同时通过 HashMap 修改，而 ConcurrentHashMap 允许该行为，并且该更新对后续的遍历可见。 JDK 1.7 中 ConcurrentHashMap 和 JDK 1.8 中 ConcurrentHashMap 的实现区別？其实可以看出 JDK 1.8 版本的 ConcurrentHashMap 的数据结构已经接近 HashMap，相对而言，ConcurrentHashMap 只是增加了同步的操作来控制并发，从 JDK 1.7 版本的 ReentrantLock + Segment + HashEntry，到 JDK 1.8 版本中 synchronized + CAS + HashEntry + 红黑树，相对而言，JDK 1.7 中 ConcurrentHashMap 和 JDK 1.8 中 ConcurrentHashMap 的实现区別总结如下： 底层数据结构不同：JDK 1.7 中 ConcurrentHashMap 采用 Segment 分段锁的数据结构，JDK 1.8 中 ConcurrentHashMap 采用数组 + 链表 + 红黑树的数据结构。 保证线程安全机制不同：JDK 1.7 中 ConcurrentHashMap 采用 segment 的分段锁机制实现线程安全，其中 segment 继承自 ReentrantLock。JDK 1.8 中 ConcurrentHashMap 采用 CAS+Synchronized 机制实现线程安全。 锁的粒度不同：JDK 1.7 中 ConcurrentHashMap 是对需要进行数据操作的 Segment 加锁，JDK 1.8 中 ConcurrentHashMap 是对每个数组元素加锁（Node）。 查询的时间复杂度不同：定位结点的 hash 算法简化会带来弊端，Hash 冲突加剧，因此在链表节点数量大于 8 时，会将链表转化为红黑树进行存储。查询元素的时间复杂度从原来的遍历链表 O(n)，变成遍历红黑树 O(logN)。 参考博文[1]. Java 进阶（六）从 ConcurrentHashMap 的演进看 Java 多线程核心技术[2]. ConcurrentHashMap 原理分析（1.7 与 1.8）[3]. 为并发而生的 ConcurrentHashMap（Java 8） 注脚[1]. 分离锁（lock striping）：分拆锁 (lock spliting) 就是若原先的程序中多处逻辑都采用同一个锁，但各个逻辑之间又相互独立，就可以拆 (Spliting) 为使用多个锁，每个锁守护不同的逻辑。分拆锁有时候可以被扩展，分成可大可小加锁块的集合，并且它们归属于相互独立的对象，这样的情况就是分离锁(lock striping)。（摘自《Java 并发编程实践》） [2]. CAS：CAS 是 compare and swap 的缩写，即我们所说的比较并替换，是用于实现多线程同步的原子指令。CAS 是一种基于锁的操作，是乐观锁。在 Java 中锁分为乐观锁和悲观锁。悲观锁是将资源锁住，线程一旦得到锁，会导致其它所有需要锁的线程挂起，等一个之前获得锁的线程释放锁之后，下一个线程才可以访问。而乐观锁采取了一种宽泛的态度，通过某种方式不加锁来处理资源，比如通过给记录加 version 来获取数据，性能较悲观锁有很大的提高。CAS 操作包含三个基本操作数 —— 内存位置（V）、预期原值（A）和新值 (B)。更新一个变量的时候，只有当变量的预期原值（A）和内存位置（V）当中的实际值相同时，才会将内存位置（V）对应的值修改为新值 (B)。CAS 是通过无限循环来获取数据的，若果在第一轮循环中，a 线程获取地址里面的值被 b 线程修改了，那么 a 线程需要自旋，到下次循环才有可能机会执行。CAS 可以有效的提升并发的效率，但同时也会引入 ABA 问题。如线程 1 从内存位置（V）中取出预期原值（A），这时候另一个线程 2 也从内存位置（V）中取出预期原值（A），并且线程 2 进行了一些操作将内存位置（V）中的值变成了新值 (B)，然后线程 2 又将内存位置（V）中的数据变成预期原值（A），这时候线程 1 进行 CAS 操作发现内存位置（V）中仍然是预期原值（A），然后线程 1 操作成功。虽然线程 1 的 CAS 操作成功，但是整个过程就是有问题的。比如链表的头在变化了两次后恢复了原值，但是不代表链表就没有变化。解决方式，在对象中额外再增加一个标记来标识对象是否有过变更【AtomicMarkableReference(通过引入一个 boolean 来反映中间有没有变过)、AtomicStampedReference（通过引入一个 int 来累加来反映中间有没有变过）】。 [3]. Unsafe：Unsafe 是 Java 留给开发者的后门，用于直接操作系统内存且不受 Jvm 管辖，实现类似 C++ 风格的操作。Java 不能直接访问操作系统底层，而是通过本地方法来访问，Unsafe 类提供了硬件级别的原子操作。Oracle 官方一般不建议开发者使用 Unsafe 类，因为正如这个类的类名一样，它并不安全，使用不当会造成内存泄露。Unsafe 类在 sun.misc 包下，不属于 Java 标准。很多 Java 的基础类库，包括一些被广泛使用的高性能开发库都是基于 Unsafe 类开发，比如 Netty、Hadoop、Kafka 等。]]></content>
      <categories>
        <category>集合</category>
      </categories>
      <tags>
        <tag>Java8</tag>
        <tag>集合</tag>
        <tag>Java7</tag>
        <tag>ConcurrentHashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java8 那些事儿（五）：函数式接口]]></title>
    <url>%2Farchives%2F43810ae.html</url>
    <content type="text"><![CDATA[前言函数式接口 (Functional Interface) 就是一个有且仅有一个抽象方法，但是可以有多个非抽象方法的接口。函数式接口可以被隐式转换为 lambda 表达式。Java 允许利用 Lambda 表达式创建这些接口的实例。java.util.function 包是 Java 8 增加的一个新技术点 “函数式接口”，此包共有 43 个接口。别指望能够全部记住他们，但是如果能记住其中 6 个基础接口，必要时就可以推断出其余接口了。这些接口是为了使 Lamdba 函数表达式使用的更加简便，当然你也可以自己自定义接口来应用于 Lambda 函数表达式。 JDK 1.8 API 包含了很多内建的函数式接口，比如 Comparator 或者 Runnable 接口，这些接口都增加了 @FunctionalInterface 注解以便能用在 Lamdba 上。现如今，我们则从 Function 常用函数入口，真正了解一下函数式接口。 Java 8 中函数式接口 接口 描述 函数签名 范例 UnaryOperator&lt;T> 接收 T 对象，返回 T 对象 T apply(T t) String::toLowerCase BinaryOprator&lt;T> 接收两个 T 对象，返回 T 对象 T apply(T t1, T t2) BigInteger::add Predicate&lt;T> 接收 T 对象，返回 boolean boolean test(T t) Collection::isEmpty Function&lt;T, R&gt; 接收 T 对象，返回 R 对象 R apply(T t) Arrays::asList Supplier&lt;T> 提供 T 对象（例如工厂），不接收值 T get() Instant::new Consumer&lt;T> 接收 T 对象，不返回值 void accept(T t) System.out::println 标注为 @FunctionalInterface 的接口被称为函数式接口，该接口有且仅有一个抽象方法，但是可以有多个非抽象方法的接口。是否是一个函数式接口，需要注意的有以下几点： 该注解只能标记在“有且仅有一个抽象方法”的接口上。 Java 8 接口中的静态方法和默认方法，都不算是抽象方法。 接口默认继承 java.lang.Object，所以如果接口显示声明覆盖了 Object 中方法，那么也不算抽象方法。 该注解不是必须的，如果一个接口符合 “函数式接口” 定义，那么加不加该注解都没有影响。加上该注解能够更好地让编译器进行检查。如果编写的不是函数式接口，但是加上了 @FunctionInterface，那么编译器会报错。 在一个接口中定义两个自定义的方法，就会产生 Invalid ‘@FunctionalInterface’ annotation; FunctionalInterfaceTest is not a functional interface 错误。 1234@FunctionalInterfacepublic interface Runnable &#123; public abstract void run();&#125; Consumer：消费型接口（void accept(T t)） 函数式接口 描述 Consumer&lt;T> 提供一个 T 类型的输入参数，不返回执行结果 BiConsumer&lt;T, U&gt; 提供两个自定义类型的输入参数，不返回执行结果 DoubleConsumer 提供一个 double 类型的输入参数，不返回执行结果 IntConsumer 提供一个 int 类型的输入参数，不返回执行结果 LongConsumer 提供一个 long 类型的输入参数，不返回执行结果 ObjDoubleConsumer&lt;T> 提供一个 double 类型的输入参数和一个 T 类型的输入参数，不返回执行结果 ObjIntConsumer&lt;T> 提供一个 int 类型的输入参数和一个 T 类型的输入参数，不返回执行结果 ObjLongConsumer&lt;T> 提供一个 long 类型的输入参数和一个 T 类型的输入参数，不返回执行结果 （1）作用：消费某个对象 （2）使用场景：Iterable 接口的 forEach 方法需要传入 Consumer，大部分集合类都实现了该接口，用于返回 Iterator 对象进行迭代。 （3）主要方法 方法 描述 void accept(T t) 对给定的参数执行操作 default Consumer&lt;T> andThen(Consumer&lt; ? super T&gt; after) 返回一个组合函数，after 将会在该函数执行之后应用 （4）代码示例 Consumer&lt;T>：提供一个 T 类型的输入参数，不返回执行结果 12345678910111213@Testpublic void testConsumer() &#123; // Consumer&lt;T&gt;:accept(T t) Consumer&lt;String&gt; consumer = System.out::println; consumer.accept("hello world!"); // hello world! // Consumer&lt;T&gt;:andThen(Consumer&lt;? super T&gt; after) -&gt; 返回一个组合函数，after将会在该函数执行之后应用 StringBuilder sb = new StringBuilder("Hello "); Consumer&lt;StringBuilder&gt; consumer_accept = (str) -&gt; str.append("Jack! "); Consumer&lt;StringBuilder&gt; consumer_andThen = (str) -&gt; str.append("Bob!"); consumer_accept.andThen(consumer_andThen).accept(sb); System.out.println(sb.toString()); // Hello Jack! Bob!&#125; BiConsumer&lt;T, U&gt; ：提供两个自定义类型的输入参数，不返回执行结果 123456@Testpublic void testBiConsumer() &#123; // BiConsumer&lt;T, U&gt;:accept(T t, U u) BiConsumer&lt;String, String&gt; biConsumer = (a, b) -&gt; System.out.printf("%s %s!", a, b); biConsumer.accept("hello", "world"); // hello world!&#125; DoubleConsumer ：提供一个 double 类型的输入参数，不返回执行结果 123456@Testpublic void testDoubleConsumer() &#123; // DoubleConsumer:accept(double value) DoubleConsumer doubleConsumer = System.out::println; doubleConsumer.accept(9.12D); // 9.12&#125; ObjDoubleConsumer&lt;T> ： 提供一个 double 类型的输入参数和一个 T 类型的输入参数，不返回执行结果 123456@Testpublic void testObjDoubleConsumer() &#123; // ObjDoubleConsumer&lt;T&gt;:accept(T t, double value) ObjDoubleConsumer&lt;String&gt; stringObjDoubleConsumer = (s, value) -&gt; System.out.println(s + value); stringObjDoubleConsumer.accept("金额：", 9.12D); // 金额：9.12&#125; Predicate：断言型接口（boolean test(T t)） 函数式接口 描述 Predicate&lt;T> 提供一个 T 类型的输入参数，返回一个 boolean 类型的结果 BiPredicate&lt;T,U&gt; 提供两个自定义类型的输入参数，返回一个 boolean 类型的结果 DoublePredicate 提供一个 double 类型的输入参数，返回一个 boolean 类型的结果 IntPredicate 提供一个 int 类型的输入参数，返回一个 boolean 类型的结果 LongPredicate 提供一个 long 类型的输入参数，返回一个 boolean 类型的结果 （1）作用：判断对象是否符合某个条件 （2）使用场景：ArrayList 的 removeIf(Predicate)：删除符合条件的元素。如果条件硬编码在 ArrayList 中，它将提供无数的实现，但是如果让调用者传入条件，这样 ArrayList 就可以从复杂和无法猜测的业务中解放出来。 （3）主要方法 方法 描述 boolean test(T t) 根据给定的参数进行判断 Predicate&lt;T> and(Predicate&lt; ? super T&gt; other) 返回一个组合判断，将 other 以短路并且的方式加入到函数的判断中 Predicate&lt;T> or(Predicate&lt; ? super T&gt; other) 返回一个组合判断，将 other 以短路或的方式加入到函数的判断中 Predicate&lt;T> negate() 将函数的判断取反 （4）代码示例 Predicate&lt;T> ： 提供一个 T 类型的输入参数，返回一个 boolean 类型的结果 12345678910111213141516171819202122@Testpublic void testPredicate() &#123; // Predicate&lt;T&gt;:boolean test(T t) Predicate&lt;List&lt;String&gt;&gt; listPredicate = Collection::isEmpty; System.out.println(listPredicate.test(Arrays.asList("Hello", "World"))); // false // Predicate&lt;T&gt;:boolean test(T t) Predicate&lt;Integer&gt; predicate = integer -&gt; integer != 0; System.out.println(predicate.test(10)); // true // Predicate&lt;T&gt;:Predicate&lt;T&gt; and(Predicate&lt;? super T&gt; other) predicate = predicate.and(integer -&gt; integer &gt;= 10); System.out.println(predicate.test(10)); // true // Predicate&lt;T&gt;:Predicate&lt;T&gt; or(Predicate&lt;? super T&gt; other) predicate = predicate.or(integer -&gt; integer != 10); System.out.println(predicate.test(10)); // true // Predicate&lt;T&gt;:Predicate&lt;T&gt; negate() predicate = predicate.negate(); System.out.println(predicate.test(10)); // false&#125; Function：函数型接口（R apply(T t)） 函数式接口 描述 Function&lt;T, R&gt; 提供一个 T 类型的输入参数，返回一个 R 类型的结果 BiFunction&lt;T, U, R&gt; 提供两个自定义类型的输入参数，返回一个 R 类型的结果 DoubleFunction&lt;R> 提供一个 double 类型的输入参数，返回一个 R 类型的结果 DoubleToIntFunction 提供一个 double 类型的输入参数，返回一个 int 类型的结果 DoubleToLongFunction 提供一个 double 类型的输入参数，返回一个 long 类型的结果 IntFunction&lt;R> 提供一个 int 类型的输入参数，返回一个 R 类型的结果 IntToDoubleFunction 提供一个 int 类型的输入参数，返回一个 double 类型的结果 IntToLongFunction 提供一个 int 类型的输入参数，返回一个 long 类型的结果 LongFunction&lt;R> 提供一个 long 类型的输入参数，返回一个 R 类型的结果 LongToDoubleFunction 提供一个 long 类型的输入参数，返回一个 double 类型的结果 LongToIntFunction 提供一个 long 类型的输入参数，返回一个 int 类型的结果 ToDoubleBiFunction&lt;T, U&gt; 提供两个自定义类型的输入参数，返回一个 double 类型的结果 ToDoubleFunction&lt;T> 提供一个 T 类型的输入参数，返回一个 double 类型的结果 ToIntBiFunction&lt;T, U&gt; 提供两个自定义类型的输入参数，返回一个 int 类型的结果 ToIntFunction&lt;T> 提供一个 T 类型的输入参数，返回一个 int 类型的结果 ToLongBiFunction&lt;T, U&gt; 提供两个自定义类型的输入参数，返回一个 long 类型的结果 ToLongFunction&lt;T> 提供一个 T 类型的输入参数，返回一个 long 类型的结果 （1）作用：实现一个”一元函数“，即传入一个值经过函数的计算返回另一个值。 （2）使用场景：V HashMap.computeIfAbsent(K , Function&lt;K, V&gt;)：如果指定的 key 不存在或相关的 value 为 null 时，设置 key 与关联一个计算出的非 null 值，计算出的值为 null 的话什么也不做(不会去删除相应的 key)。如果 key 存在并且对应 value 不为 null 的话什么也不做。 （3）主要方法 方法 描述 R apply(T t) 将此参数应用到函数中 Function&lt;T, V&gt; andThen(Function&lt; ? super R, ? extends V&gt; after) 返回一个组合函数，该函数结果应用到 after 函数中 Function&lt;V, R&gt; compose(Function&lt; ? super V, ? extends T&gt; before) 返回一个组合函数，首先将入参应用到 before 函数，再将 before 函数结果应用到该函数中 （4）代码示例 Function&lt;T, R&gt; ： 提供一个 T 类型的输入参数，返回一个 R 类型的结果 12345678910111213@Testpublic void testFunction() &#123; // Function&lt;T, R&gt;:R apply(T t) Function&lt;String[], List&lt;String&gt;&gt; asList = Arrays::asList; System.out.println(asList.apply(new String[]&#123;"Hello", "World"&#125;)); // [Hello, World] Function&lt;String, String&gt; function = s -&gt; String.format("%s, Jack!", s); Function&lt;String, String&gt; compose = s -&gt; StringUtils.isEmpty(s) ? "Hello" : s; Function&lt;String, String&gt; andThen = String::toUpperCase; String s = function.compose(compose).andThen(andThen).apply(""); System.out.println(s); // HELLO, JACK!&#125; Supplier：供给型接口（R apply(T t)） 函数式接口 描述 Supplier&lt;T> 不提供输入参数，返回一个 T 类型的结果 BooleanSupplier 不提供输入参数，返回一个 boolean 类型的结果 DoubleSupplier 不提供输入参数，返回一个 double 类型的结果 IntSupplier 不提供输入参数，返回一个 int 类型的结果 LongSupplier 不提供输入参数，返回一个 long 类型的结果 （1）作用：创建一个对象（工厂类） （2）使用场景：Optional.orElseGet(Supplier&lt; ? extends T&gt;)：当 this 对象为 null，就通过传入 supplier 创建一个 T 返回。 （3）主要方法 方法 描述 T get() 获取结果值 （4）代码示例 Supplier&lt;T> ： 不提供输入参数，返回一个 T 类型的结果 123456@Testpublic void testSupplier() &#123; // Supplier&lt;T&gt;:T get(); Supplier&lt;String&gt; supplier = () -&gt; "Hello Jack!"; System.out.println(supplier.get()); // Hello Jack!&#125; Operator：操作型接口（T apply(T t)） 函数式接口 描述 UnaryOperator&lt;T> 提供一个 T 类型的输入参数，返回一个 T 类型的结果 BinaryOperator&lt;T> 提供两个 T 类型的输入参数，返回一个 T 类型的结果 DoubleBinaryOperator 提供两个 double 类型的输入参数，返回两个 double 类型的结果 DoubleUnaryOperator 提供一个 double 类型的输入参数，返回一个 double 类型的结果 IntBinaryOperator 提供两个 int 类型的输入参数，返回一个 int 类型的结果 IntUnaryOperator 提供一个 int 类型的输入参数，返回一个 int 类型的结果 LongBinaryOperator 提供两个 long 类型的输入参数，返回一个 long 类型的结果 LongUnaryOperator 提供一个 long 类型的输入参数，返回一个 long 类型的结果 （1）作用：实现一个”一元函数“，即传入一个值经过函数的计算返回另一个同类型的值。 （2）使用场景：UnaryOperator 继承了 Function，与 Function 作用相同，不过 UnaryOperator，限定了传入类型和返回类型必需相同。 （3）主要方法 方法 描述 T apply(T t) 将给定参数应用到函数中 Function&lt;T, V&gt; andThen(Function&lt; ? super T, ? extends V&gt; after) 返回一个组合函数，该函数结果应用到 after 函数中 Function&lt;V, T&gt; compose(Function&lt; ? super V, ? extends T&gt; before) 返回一个组合函数，首先将入参应用到 before 函数，再将 before 函数结果应用到该函数中 （4）代码示例 UnaryOperator&lt;T> ： 提供一个 T 类型的输入参数，返回一个 T 类型的结果 123456@Testpublic void testOperator() &#123; // UnaryOperator&lt;T&gt;:T apply(T t) UnaryOperator&lt;String&gt; unaryOperator = String::toUpperCase; System.out.println(unaryOperator.apply("Hello World!")); // HELLO WORLD!&#125; BinaryOperator&lt;T> ：提供两个 T 类型的输入参数，返回一个 T 类型的结果 123456@Testpublic void testBinaryOperator() &#123; // BinaryOperator&lt;T&gt;:T apply(T t1, T T2); BinaryOperator&lt;BigInteger&gt; binaryOperator = BigInteger::add; System.out.println(binaryOperator.apply(BigInteger.ONE, BigInteger.TEN));&#125; 总结java.util.function 包已经为大家提供了大量标注的函数接口。只要标准的函数接口能够满足需求，通常应该优先考虑，而不是专门再构建一个新的函数接口。这样会使 API 更加容易学习，通过减少它的概念内容，显著提升互操作性优势，因为许多标准的函数接口都提供了有用的默认方法。 参考博文[1]. JDK8 新特性 - java.util.function-Function 接口[2]. JAVA8 的 java.util.function 包 Java8 那些事儿系列 Java8 那些事儿（一）：Stream 函数式编程 Java8 那些事儿（二）：Optional 类解决空指针异常 Java8 那些事儿（三）：Date/Time API(JSR 310) Java8 那些事儿（四）：增强的 Map 集合 Java8 那些事儿（五）：函数式接口]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Function</tag>
        <tag>Lambda</tag>
        <tag>函数式接口</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[了不起的消息队列（一）：浅谈消息队列及常见的分布式消息队列中间件]]></title>
    <url>%2Farchives%2F1c55560e.html</url>
    <content type="text"><![CDATA[背景分布式消息队列中间件是是大型分布式系统不可缺少的中间件，通过消息队列，应用程序可以在不知道彼此位置的情况下独立处理消息，或者在处理消息前不需要等待接收此消息。所以消息队列主要解决应用耦合、异步消息、流量削锋等问题，实现高性能、高可用、可伸缩和最终一致性架构。消息队列已经逐渐成为企业应用系统内部通信的核心手段，当前使用较多的消息队列有 RabbitMQ、RocketMQ、ActiveMQ、Kafka、ZeroMQ、MetaMQ 等，而部分数据库如 Redis、MySQL 以及 PhxSQL 也可实现消息队列的功能。 在日常学习与开发过程中，消息队列作为系统不可缺少的中间件，显得十分的重要。在现代云架构中，应用程序被分解为多个规模较小且更易于开发、部署和维护的独立构建块。消息队列可为这些分布式应用程序提供通信和协调。而本人也在工作的过程中，前前后后后接触到了 Kafka、RabbitMQ 两款消息队列。所以，本系列文章也主要以 RabbitMQ 和 Kafka 两款典型的消息中间件来做分析。本文是该系列的开篇，主要讲解消息队列的概述、特点等，然后对消息队列使用场景进行分析，最后对市面上比较常见的消息队列产品进行技术对比。 消息队列概述消息队列（Message Queue，简称 MQ）是指利用高效可靠的消息传递机制进行与平台无关的数据交流，并基于数据通信来进行分布式系统的集成。通过提供消息传递和消息排队模型，它可以在分布式环境下提供应用解耦、弹性伸缩、冗余存储、流量削峰、异步通信、数据同步等等功能，其作为分布式系统架构中的一个重要组件，有着举足轻重的地位。消息队列是构建分布式互联网应用的基础设施，通过 MQ 实现的松耦合架构设计可以提高系统可用性以及可扩展性，是适用于现代应用的最佳设计方案。 消息队列特点为什么要用消息队列？通过异步处理提高系统性能讲解该特点之前，我们先了解一下同步架构和异步架构的区别： 同步调用：是指从请求的发起一直到最终的处理完成期间，请求的调用方一直在同步阻塞等待调用的处理完成。 异步调用：是指在请求发起的处理过程中，客户端的代码已经返回了，它可以继续进行自己的后续操作，而不需要等待调用处理完成，这就叫做异步调用。 如上图，在不使用消息队列服务器的时候，用户的请求数据直接写入数据库，在高并发的情况下数据库压力剧增，使得响应速度变慢。但是在使用消息队列之后，用户的请求数据发送给消息队列之后立即 返回，再由消息队列的消费者进程从消息队列中获取数据，异步写入数据库。由于消息队列服务器处理速度快于数据库（消息队列也比数据库有更好的伸缩性），因此响应速度得到大幅改善。 通过以上分析我们可以得出消息队列具有很好的削峰作用的功能——即通过异步处理，将短时间高并发产生的事务消息存储在消息队列中，从而削平高峰期的并发事务。 举例：在电子商务一些秒杀、促销活动中，合理使用消息队列可以有效抵御促销活动刚开始大量订单涌入对系统的冲击。如下图所示： 因为用户请求数据写入消息队列之后就立即返回给用户了，但是请求数据在后续的业务校验、写数据库等操作中可能失败。因此使用消息队列进行异步处理之后，需要适当修改业务流程进行配合，比如用户在提交订单之后，订单数据写入消息队列，不能立即返回用户订单提交成功，需要在消息队列的订单消费者进程真正处理完该订单之后，甚至出库后，再通过电子邮件或短信通知用户订单成功，以免交易纠纷。这就类似我们平时手机订火车票和电影票。 降低系统耦合性我们知道如果模块之间不存在直接调用，那么新增模块或者修改模块就对其他模块影响较小，这样系统的可扩展性无疑更好一些。 我们最常见的事件驱动架构类似生产者消费者模式，在大型网站中通常用利用消息队列实现事件驱动结构。如下图所示： 消息队列使利用发布 - 订阅模式工作，消息发送者（生产者）发布消息，一个或多个消息接受者（消费者）订阅消息。 从上图可以看到消息发送者（生产者）和消息接受者（消费者）之间没有直接耦合，消息发送者将消息发送至分布式消息队列即结束对消息的处理，消息接受者从分布式消息队列获取该消息后进行后续处理，并不需要知道该消息从何而来。对新增业务，只要对该类消息感兴趣，即可订阅该消息，对原有系统和业务没有任何影响，从而实现网站业务的可扩展性设计。 消息接受者对消息进行过滤、处理、包装后，构造成一个新的消息类型，将消息继续发送出去，等待其他消息接受者订阅该消息。因此基于事件（消息对象）驱动的业务架构可以是一系列流程。 另外为了避免消息队列服务器宕机造成消息丢失，会将成功发送到消息队列的消息存储在消息生产者服务器上，等消息真正被消费者服务器处理后才删除消息。在消息队列服务器宕机后，生产者服务器会选择分布式消息队列服务器集群中的其他服务器发布消息。 使用消息队列带来的一些问题？ 系统可用性降低： 系统可用性在某种程度上降低，为什么这样说呢？在加入 MQ 之前，你不用考虑消息丢失或者说 MQ 挂掉等情况，但是，引入 MQ 之后你就需要如何保证消息队列的高可用。 系统复杂性提高： 加入 MQ 之后，你需要保证消息没有被重复消费、处理消息丢失的情况、保证消息传递的顺序性等等问题，系统复发性提高。 一致性问题： 消息队列带来的异步确实可以提高系统响应速度。但是，万一消息的真正消费者并没有正确消费消息怎么办？这样就会导致数据不一致的情况。 JMS VS AMQPJMSJava 消息服务（Java Message Service，JMS）应用程序接口是一个 Java 平台中关于面向消息中间件（MOM）的 API，用于在两个应用程序之间，或分布式系统中发送消息，进行异步通信。JMS 的客户端之间可以通过 JMS 服务进行异步的消息传输。JMS PI 是一个消息服务的标准或者说是规范，允许应用程序组件基于 JavaEE 平台创建、发送、接收和读取消息。它使分布式通信耦合度更低，消息服务更加可靠以及异步性。点对点与发布订阅最初是由 JMS 定义的。这两种模式主要区别或解决的问题就是发送到队列的消息能否重复消费。 JMS 规范目前支持两种消息模型：点对点（point to point，queue）和发布 / 订阅（publish/subscribe，topic）。 点对点（P2P）模型消息生产者向消息队列中发送了一个消息之后，只能被一个消费者消费一次。点对点（P2P）使用队列（Queue）作为消息通信载体；满足生产者与消费者模式，一条消息只能被一个消费者使用，未被消费的消息在队列中保留直到被消费或超时。 Queue 实现了负载均衡，一个消息只能被一个消费者接受，当没有消费者可用时，这个消息会被保存直到有 一个可用的消费者，一个 queue 可以有很多消费者，他们之间实现了负载均衡， 所以 Queue 实现了一个可靠的负载均衡。 特点： 每个消息只用一个消费者； 发送者和接受者没有时间依赖； 接受者确认消息接受和处理成功。 发布 / 订阅（Pub/Sub）模型消息生产者向频道发送一个消息之后，多个消费者可以从该频道订阅到这条消息并消费。发布订阅模型（Pub/Sub） 使用主题（Topic）作为消息通信载体，类似于广播模式；发布者发布一条消息，该消息通过主题传递给所有的订阅者，在一条消息广播之后才订阅的用户则是收不到该条消息的。 Topic 实现了发布和订阅，当你发布一个消息，所有订阅这个 Topic 的服务都能得到这个消息，所以从 1 到 N 个订阅者都能得到一个消息的拷贝， 只有在消息代理收到消息时有一个有效订阅时的订阅者才能得到这个消息的拷贝。 特点： 每个消息可以有多个订阅者； 客户端只有订阅后才能接收到消息； 持久订阅和非持久订阅。 注意： 发布者和订阅者有时间依赖：接受者和发布者只有建立订阅关系才能收到消息； 持久订阅：订阅关系建立后，消息就不会消失，不管订阅者是否都在线； 非持久订阅：订阅者为了接受消息，必须一直在线。 当只有一个订阅者时约等于点对点模。 点对点（P2P）模型与发布 / 订阅（Pub/Sub）模型应用 点对点模型：主要用于一些耗时较长的、逻辑相对独立的业务。 比如说发送邮件这样一个操作。因为发送邮件比较耗时，而且应用程序其实也并不太关心邮件发送是否成功，发送邮件的逻辑也相对比较独立，所以它只需要把邮件消息丢到消息队列中就可以返回了，而消费者也不需要关心是哪个生产者去发送的邮件，它只需要把邮件消息内容取出来以后进行消费，通过远程服务器将邮件发送出去就可以了。而且每个邮件只需要被发送一次。所以消息只被一个消费者消费就可以了。 发布订阅模型：如新用户注册这样一个消息，需要使用按主题发布的方式。 比如新用户注册，一个新用户注册成功以后，需要给用户发送一封激活邮件，发送一条欢迎短信，还需要将用户注册数据写入数据库，甚至需要将新用户信息发送给关联企业的系统，比如淘宝新用户信息发送给支付宝，这样允许用户可以一次注册就能登录使用多个关联产品。一个新用户注册，会把注册消息发送给一个主题，多种消费者可以订阅这个主题。比如发送邮件的消费者、发送短信的消费者、将注册信息写入数据库的消费者，跨系统同步消息的消费者等。 AMQPAMQP（advanced message queuing protocol）在 2003 年时被提出，最早用于解决金融领不同平台之间的消息传递交互问题。顾名思义，AMQP 是一种协议，更准确的说是一种 binary wire-level protocol（链接协议）。这是其和 JMS 的本质差别，AMQP 不从 API 层进行限定，而是直接定义网络交换的数据格式。这使得实现了 AMQP 的 provider 天然性就是跨平台的。意味着我们可以使用 Java 的 AMQP provider，同时使用一个 python 的 producer 加一个 rubby 的 consumer。 在 AMQP 中，消息路由（message routing）和 JMS 存在一些差别，在 AMQP 中增加了 Exchange 和 binding 的角色。producer 将消息发送给 Exchange，binding 决定 Exchange 的消息应该发送到那个 queue，而 consumer 直接从 queue 中消费消息。 AMQP 提供五种消息模型：①Direct Exchange；②Fanout Exchange；③Topic Exchange；④Headers Exchange；⑤System Exchange。本质来讲，后四种和 JMS 的 pub/sub 模型没有太大差别，仅是在路由机制上做了更详细的划分。 JMS 与 AMQP 对比 总结： AMQP 为消息定义了线路层（wire-level protocol）的协议，而 JMS 所定义的是 API 规范。在 Java 体系中，多个 client 均可以通过 JMS 进行交互，不需要应用修改代码，但是其对跨平台的支持较差。而 AMQP 天然具有跨平台、跨语言特性。 JMS 支持 TextMessage、MapMessage 等复杂的消息类型；而 AMQP 仅支持 byte[] 消息类型（复杂的类型可序列化后发送）。 由于 Exchange 提供的路由算法，AMQP 可以提供多样化的路由方式来传递消息到消息队列，而 JMS 仅支持 队列 和 主题 / 订阅 方式两种。 消息队列推拉模型Push 推消息模型：消息生产者将消息发送给消息队列，消息队列又将消息推给消息消费者。 Pull 拉消息模型：消费者请求消息队列接受消息，消息生产者从消息队列中拉该消息。 RabbitMQRabbitMQ 实现了 AQMP 协议，AQMP 协议定义了消息路由规则和方式。生产端通过路由规则发送消息到不同 queue，消费端根据 queue 名称消费消息。此外 RabbitMQ 是向消费端推送消息，订阅关系和消费状态保存在服务端。 KafkaKafka 只支持消息持久化，消费端为拉模型，消费状态和订阅关系由客户端端负责维护，消息消费完后不会立即删除，会保留历史消息。因此支持多订阅时，消息只会存储一份就可以了。同一个订阅组会消费 topic 所有消息，每条消息只会被同一个订阅组的一个消费节点消费，同一个订阅组内不同消费节点会消费不同消息。 消息队列使用场景异步处理：实现异步处理，提升处理性能对一些比较耗时的操作，可以把处理过程通过消息队列进行异步处理。这样做可以推迟耗时操作的处理，使耗时操作异步化，而不必阻塞客户端的程序，客户端的程序在得到处理结果之前就可以继续执行，从而提高客户端程序的处理性能。非核心流程异步化，减少系统响应时间，提高吞吐量。 例如：短信通知、终端状态推送、App 推送、用户注册等。 解耦：可以使生产者和消费者的代码实现解耦合可以多个生产者发布消息，多个消费者处理消息，共同完成完整的业务处理逻辑，但是它们的不需要直接的交互调用，没有代码的依赖耦合。在传统的同步调用中，调用者代码必须要依赖被调用者的代码，也就是生产者代码必须要依赖消费者的处理逻辑代码，代码需要直接的耦合，而使用消息队列，这两部分的代码不需要进行任何的耦合。因为耦合程度越低的代码越容易维护，也越容易进行扩展。 比如新用户注册，如果用传统同步调用的方式，那么发邮件、发短信、写数据库、通知关联系统这些代码会和用户注册代码直接耦合起来，整个代码看起来就是完成用户注册逻辑后，后面必然跟着发邮件、发短信这些代码。如果要新增一个功能，比如将监控用户注册情况，将注册信息发送到业务监控系统，就必须要修改前面的代码，至少增加一行代码，发送注册信息到监控系统，我们知道，任何代码的修改都可能会引起 bug。 而使用分布式消息队列实现生产者和消费者解耦合以后，用户注册以后，不需要调用任何后续处理代码，只需要将注册消息发送到分布式消息队列就可以了。如果要增加新功能，只需要写个新功能的消费者程序，在分布式消息队列中，订阅用户注册主题就可以了，不需要修改原来任何一行代码。 流量削峰和流控：可以平衡流量峰值，削峰填谷当上下游系统处理能力存在差距的时候，利用消息队列做一个通用的 “漏斗”，进行限流控制。在下游有能力处理的时候，再进行分发。 使用消息队列，即便是访问流量持续的增长，系统依然可以持续地接收请求。这种情况下，虽然生产者发布消息的速度比消费者消费消息的速度快，但是可以持续的将消息纳入到消息队列中，用消息队列作为消息的缓冲，因此短时间内，发布者不会受到消费处理能力的影响。 在访问高峰，用户的并发访问数可能超过了系统的处理能力，所以在高峰期就可能会导致系统负载过大，响应速度变慢，更严重的可能会导致系统崩溃。这种情况下，通过消息队列将用户请求的消息纳入到消息队列中，通过消息队列缓冲消费者处理消息的速度。 消息的生产者它有高峰有低谷，但是到了消费者这里，只会按照自己的最佳处理能力去消费消息。高峰期它会把消息缓冲在消息队列中，而在低谷期它也还是使用自己最大的处理能力去获取消息，将前面缓冲起来、来不及及时处理的消息处理掉。那么，通过这种手段可以实现系统负载消峰填谷，也就是说将访问的高峰消掉，而将访问的低谷填平，使系统处在一个最佳的处理状态之下，不会对系统的负载产生太大的冲击。 举个例子：用户在支付系统成功结账后，订单系统会通过短信系统向用户推送扣费通知。短信系统可能由于短板效应，速度卡在网关上（每秒几百次请求），跟前端的并发量不是一个数量级。于是，就造成支付系统和短信系统的处理能力出现差异化。 然而用户晚上个半分钟左右收到短信，一般是不会有太大问题的。如果没有消息队列，两个系统之间通过协商、滑动窗口等复杂的方案也不是说不能实现。但系统复杂性指数级增长，势必在上游或者下游做存储，并且要处理定时、拥塞等一系列问题。而且每当有处理能力有差距的时候，都需要单独开发一套逻辑来维护这套逻辑。所以，利用中间系统转储两个系统的通信内容，并在下游系统有能力处理这些消息的时候，再处理这些消息，是一套相对较通用的方式。 易伸缩：可以让系统获得更好的伸缩性耗时的任务可以通过分布式消息队列，向多台消费者服务器并行发送消息，然后在很多台消费者服务器上并行处理消息，也就是说可以在多台物理服务器上运行消费者。那么当负载上升的时候，可以很容易地添加更多的机器成为消费者。 例如：用户上传文件后，通过发布消息的方式，通知后端的消费者获取数据、读取文件，进行异步的文件处理操作。那么当前端发布更多文件的时候，或者处理逻辑比较复杂的时候，就可以通过添加后端的消费者服务器，提供更强大的处理能力。 隔离失效机器以及自我修复：失败隔离和自我修复因为发布者不直接依赖消费者，所以分布式消息队列可以将消费者系统产生的错误异常与生产者系统隔离开来，生产者不受消费者失败的影响。当在消息消费过程中出现处理逻辑失败的时候，这个错误只会影响到消费者自身，而不会传递给消息的生产者，也就是应用程序可以按照原来的处理逻辑继续执行。 所以，这也就意味着在任何时候都可以对后端的服务器执行维护和发布操作。可以重启、添加或删除服务器，而不影响生产者的可用性，这样简化了部署和服务器管理的难度。 日志处理日志处理是指将消息队列用在日志处理中，比如 Kafka 的应用，解决大量日志传输和缓冲的问题。日志采集客户端，负责日志数据采集，定时写受写入 Kafka 队列；Kafka 消息队列，负责日志数据的接收，存储和转发；日志处理应用，订阅并消费 kafka 队列中的日志数据。 消息队列技术对比 ActiveMQ 是 Apache 出品的、采用 Java 语言编写的完全基于 JMS1.1 规范的面向消息的中间件，为应用程序提供高效的、可扩展的、稳定的和安全的企业级消息通信。不过由于历史原因包袱太重，目前市场份额没有后面三种消息中间件多，其最新架构被命名为 Apollo，号称下一代 ActiveMQ，有兴趣的同学可行了解。 RabbitMQ 是采用 Erlang 语言实现的 AMQP 协议的消息中间件，最初起源于金融系统，用于在分布式系统中存储转发消息。RabbitMQ 发展到今天，被越来越多的人认可，这和它在可靠性、可用性、扩展性、功能丰富等方面的卓越表现是分不开的。主要特点是性能好，社区活跃，但是 RabbitMQ 用 Erlang 开发，我们的应用很少用 Erlang，所以不便于二次开发和维护。 Kafka 是由 LinkedIn 公司采用 Scala 语言开发的一个分布式、多分区、多副本且基于 zookeeper 协调的分布式消息系统，现已捐献给 Apache 基金会。它是一种高吞吐量的分布式发布订阅消息系统，以可水平扩展和高吞吐率而被广泛使用。目前越来越多的开源分布式处理系统如 Cloudera、Apache Storm、Spark、Flink 等都支持与 Kafka 集成。 RocketMQ 是阿里开源的消息中间件，目前在 Apache 孵化，使用纯 Java 开发，具有高吞吐量、高可用性、适合大规模分布式系统应用的特点。RocketMQ 思路起源于 Kafka，但并不是简单的复制，它对消息的可靠传输及事务性做了优化，目前在阿里集团被广泛应用于交易、充值、流计算、消息推送、日志流式处理、binglog 分发等场景，支撑了阿里多次双十一活动。 ZeroMQ 是基于 C 语言开发，号称史上最快的消息队列。ZeroMQ 是一个消息处理队列库，可在多线程、多内核和主机之间弹性伸缩，虽然大多数时候我们习惯将其归入消息队列家族之中，但是其和前面的几款有着本质的区别，ZeroMQ 本身就不是一个消息队列服务器，更像是一组底层网络通讯库，对原有的 Socket API 上加上一层封装而已。 总结： ActiveMQ 的社区算是比较成熟，但是较目前来说，ActiveMQ 的性能比较差，而且版本迭代很慢，不推荐使用。 RabbitMQ 在吞吐量方面虽然稍逊于 Kafka 和 RocketMQ ，但是由于它基于 erlang 开发，所以并发能力很强，性能极其好，延时很低，达到微秒级。但是也因为 RabbitMQ 基于 erlang 开发，所以国内很少有公司有实力做 erlang 源码级别的研究和定制。如果业务场景对并发量要求不是太高（十万级、百万级），那这四种消息队列中，RabbitMQ 一定是你的首选。如果是大数据领域的实时计算、日志采集等场景，用 Kafka 是业内标准的，绝对没问题，社区活跃度很高，绝对不会黄，何况几乎是全世界这个领域的事实性规范。 RocketMQ 阿里出品，Java 系开源项目，源代码我们可以直接阅读，然后可以定制自己公司的 MQ，并且 RocketMQ 有阿里巴巴的实际业务场景的实战考验。RocketMQ 社区活跃度相对较为一般，不过也还可以，文档相对来说简单一些，然后接口这块不是按照标准 JMS 规范走的有些系统要迁移需要修改大量代码。还有就是阿里出台的技术，你得做好这个技术万一被抛弃，社区黄掉的风险，那如果你们公司有技术实力我觉得用 RocketMQ 挺好的 kafka 最初设计时就是针对互联网的分布式、高可用应用场景而设计，所以其特点其实很明显，就是仅仅提供较少的核心功能，但是提供超高的吞吐量，ms 级的延迟，极高的可用性以及可靠性，而且分布式可以任意扩展。同时 kafka 最好是支撑较少的 topic 数量即可，保证其超高吞吐量。kafka 唯一的一点劣势是有可能消息重复消费，那么对数据准确性会造成极其轻微的影响，在大数据领域中以及日志采集中，这点轻微影响可以忽略这个特性天然适合大数据实时计算以及日志收集。 参考博文[1]. 浅谈消息队列及常见的消息中间件[2]. 消息中间件选型分析[3]. 新手也能看懂，消息队列其实很简单[4]. 10 分钟搞懂：95% 的程序员都拎不清的分布式消息队列中间件 了不起的消息队列系列 了不起的消息队列（一）：浅谈消息队列及常见的分布式消息队列中间件 了不起的消息队列（二）：啊哈！RabbitMQ]]></content>
      <categories>
        <category>消息队列</category>
      </categories>
      <tags>
        <tag>RabbitMQ</tag>
        <tag>Kafka</tag>
        <tag>JMS</tag>
        <tag>AMQP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 并发编程之美（六）：J.U.C 之线程同步辅助工具类]]></title>
    <url>%2Farchives%2Ff4f43ede.html</url>
    <content type="text"><![CDATA[前言Java 并发编程是整个 Java 开发体系中最难以理解但也是最重要的知识点，也是各类开源分布式框架（如 ZooKeeper、Kafka、Spring Cloud、Netty 等）中各个并发组件实现的基础。J.U.C 并发包，即 java.util.concurrent 包，大大提高了并发性能，是 JDK 的核心工具包，是 JDK 1.5 之后，由 Doug Lea 实现并引入。而 AQS 被认为是 J.U.C 的核心。 AQS 是一个抽象类，并没有对并发类提供了一个统一的接口定义，而是由子类根据自身的情况实现相应的方法，AQS 中一般包含两个方法 acquire(int)、release(int)，获取同步状态和释放同步状态，AQS 根据其状态是否独占分为独占模式和共享模式。 独占模式：同一时刻最多只有一个线程获取同步状态，处于该模式下，其他线程试图获取该锁将无法获取成功。 共享模式：同一时刻会有多个线程获取共享同步状态，处于该模式下，其他线程试图获取该锁可能会获取成功。 同步器根据同步状态分为独占模式和共享模式，独占模式包括类：ReentrantLock、ReentrantReadWriteLock.WriteLock，共享模式包括：Semaphore、CountDownLatch、ReentrantReadWriteLock.ReadLock，本文将着重介绍一下 java.util.concurrent 包下一些辅助同步器类：CountDownLatch、CyclicBarrier、Semaphore、Exchanger、Phaser。 CountDownLatch - 闭锁简介CountDownLatch 是一个同步辅助工具类，通过它可以完成类似于阻塞当前线程的功能，也就是一个或多个线程一直等待直到其他线程执行完成。即允许一个或多个线程一直等待，直到其他线程执行完后再执行。例如，应用程序的主线程希望在负责启动框架服务的线程已经启动所有框架服务之后执行。 CountDownLatch 用了一个给定的计数器 cnt 来进行初始化，该计数器的操作是原子操作，即同时只能有一个线程操作该计数器，调用该类 await 方法的线程会一直处于阻塞状态，直到其他线程调用 countDown 方法时计数器的值变成 0，每次调用 countDown 时计数器的值会减 1，当计数器的值为 0 时所有因 await 方法而处于等待状态的线程就会继续执行。计数器 cnt 是闭锁需要等待的线程数量，只能被设置一次，且 CountDownLatch 没有提供任何机制去重新设置计数器 count，如果需要重置，可以考虑使用 CyclicBarrier。 使用场景（1）开启多个线程分块下载一个大文件，每个线程只下载固定的一截，最后由另外一个线程来拼接所有的分段。 （2）应用程序的主线程希望在负责启动框架服务的线程已经启动所有的框架服务之后再执行。 （3）确保一个计算不会执行，直到所需要的资源被初始化。 （4）并行计算，处理量很大时可以将运算任务拆分成多个子任务，当所有子任务都完成之后，父任务再将所有子任务都结果进行汇总。 主要接口分析CountDownLatch 内部依赖 Sync 实现，而 Sync 继承 AQS。CountDownLatch 关键接口如下： countDown() 如果当前计数器的值大于 1，则将其减 1；若当前值为 1，则将其置为 0 并唤醒所有通过 await 等待的线程；若当前值为 0，则什么也不做直接返回。 await() 等待计数器的值为 0，若计数器的值为 0 则该方法返回；若等待期间该线程被中断，则抛出 InterruptedException 并清除该线程的中断状态。 await(long timeout, TimeUnit unit) 在指定的时间内等待计数器的值为 0，若在指定时间内计数器的值变为 0，则该方法返回 true；若指定时间内计数器的值仍未变为 0，则返回 false；若指定时间内计数器的值变为 0 之前当前线程被中断，则抛出 InterruptedException 并清除该线程的中断状态。 getCount() 读取当前计数器的值，一般用于调试或者测试。 Coding 演示（1）作为一个开关 / 入口 将初始计数值为 1 的 CountDownLatch 作为一个的开关或入口，在调用 countDown() 的线程打开入口前，所有调用 await 的线程都一直在入口处等待。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class Driver &#123; private static final Integer WORK_COUNT = 10; private static final ExecutorService executorService = Executors.newCachedThreadPool(); public static void main(String[] args) &#123; // 初始化计数器为 10 的 CountDownLatch CountDownLatch countDownLatch = new CountDownLatch(1); for (int i = 0; i &lt; WORK_COUNT; i++) &#123; executorService.execute(new Worker(countDownLatch)); &#125; // 主线程执行 doSomething(); // 主线程开启开关 countDownLatch.countDown(); // 平滑地关闭 ExecutorService executorService.shutdown(); &#125; private static void doSomething() &#123; // ... System.out.print("start.."); &#125;&#125;class Worker implements Runnable &#123; private final CountDownLatch countDownLatch; Worker(CountDownLatch countDownLatch) &#123; this.countDownLatch = countDownLatch; &#125; @Override public void run() &#123; try &#123; // 所有执行线程在此处等待开关开启 [多个子线程同时执行] countDownLatch.await(); // 子线程执行 doWork(); &#125; catch (InterruptedException ignored) &#123; &#125; &#125; private void doWork() &#123; // ... System.out.print("run.."); &#125;&#125;// Output// start..run..run..run..run..run..run..run..run..run..run.. （2）作为一个完成信号 将初始计数值为 N 的 CountDownLatch 作为一个完成信号点，使某个线程在其它 N 个线程完成某项操作之前一直等待。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class Driver &#123; private static final Integer WORK_COUNT = 10; private static final ExecutorService executorService = Executors.newCachedThreadPool(); public static void main(String[] args) throws InterruptedException &#123; // 初始化计数器为 10 的 CountDownLatch CountDownLatch countDownLatch = new CountDownLatch(WORK_COUNT); for (int i = 0; i &lt; WORK_COUNT; i++) &#123; executorService.execute(new Worker(countDownLatch)); &#125; // 主线程等待其它 N 个线程完成 countDownLatch.await(); // 主线程执行 doSomething(); // 平滑地关闭 ExecutorService executorService.shutdown(); &#125; private static void doSomething() &#123; // ... System.out.println("end"); &#125;&#125;class Worker implements Runnable &#123; private final CountDownLatch countDownLatch; Worker(CountDownLatch countDownLatch) &#123; this.countDownLatch = countDownLatch; &#125; @Override public void run() &#123; // 子线程执行 doWork(); // 每个线程做完自己的事情后, 就将计数器减去 1 countDownLatch.countDown(); &#125; private void doWork() &#123; // ... System.out.print("run.."); &#125;&#125;// Output// run..run..run..run..run..run..run..run..run..run..end CyclicBarrier - 循环栅栏简介CyclicBarrier 和 CountDownLatch 是非常类似的，CyclicBarrier 核心的概念是在于设置一个等待线程的数量边界，到达了此边界之后进行执行。CyclicBarrier 也是一个同步辅助工具类，它允许一组线程相互等待直到到达某个公共的屏障点（Common Barrier Point），通过它可以完成多个线程之间相互等待时，只有当每个线程都准备就绪后才能各自继续执行后面的操作。 CyclicBarrier 也是通过计数器来实现，当某个线程调用 await 方法后就进入等待状态，计数器执行加一操作。当计数器的值达到了设置的初始值时等待状态的线程会被唤醒继续执行。通过调用 CyclicBarrier 对象的 await() 方法，两个线程可以实现互相等待。一旦 N 个线程在等待 CyclicBarrier 达成，所有线程将被释放掉去继续执行。由于 CyclicBarrier 在释放等待线程后可以重用，所以可以称之为循环栅栏。 使用场景CyclicBarrier 特别适用于并行迭代计算，每个线程负责一部分计算，然后在栅栏处等待其他线程完成，所有线程到齐后，交换数据和计算结果，再进行下一次迭代。 主要接口分析CyclicBarrier 并没有自己去实现 AQS 框架的 API，而是利用了 ReentrantLock 和 Condition。 CyclicBarrier 提供的关键方法如下： await() 等待其它参与方的到来（调用 await()）。如果当前调用是最后一个调用，则唤醒所有其它的线程的等待并且如果在构造 CyclicBarrier 时指定了 action，当前线程会去执行该 action，然后该方法返回该线程调用 await 的次序（getParties()-1 说明该线程是第一个调用 await 的，0 说明该线程是最后一个执行 await 的），接着该线程继续执行 await 后的代码；如果该调用不是最后一个调用，则阻塞等待；如果等待过程中，当前线程被中断，则抛出 InterruptedException；如果等待过程中，其它等待的线程被中断，或者其它线程等待超时，或者该 barrier 被 reset，或者当前线程在执行 barrier 构造时注册的 action 时因为抛出异常而失败，则抛出 BrokenBarrierException。 await(long timeout, TimeUnit unit) 与 await() 唯一的不同点在于设置了等待超时时间，等待超时时会抛出 TimeoutException。 reset() 该方法会将该 barrier 重置为它的初始状态，并使得所有对该 barrier 的 await 调用抛出 BrokenBarrierException。 CyclicBarrier 提供的两个构造函数： CyclicBarrier(int parties)：parties 表示拦截线程的数量。创建一个新的 CyclicBarrier，它将在给定数量的参与者（线程）处于等待状态时启动，但它不会在启动 barrier 时执行预定义的操作。 CyclicBarrier(int parties, Runnable barrierAction) ：barrierAction 为 CyclicBarrier 接收的 Runnable 命令，用于在线程到达屏障时，优先执行 barrierAction ，用于处理更加复杂的业务场景。创建一个新的 CyclicBarrier，它将在给定数量的参与者（线程）处于等待状态时启动，并在启动 barrier 时执行给定的屏障操作，该操作由最后一个进入 barrier 的线程执行。 Coding 演示（1）简单例子 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class Solver &#123; private static final Integer WORK_COUNT = 10; private static final ExecutorService executorService = Executors.newCachedThreadPool(); public static void main(String[] args) &#123; // 初始化计数器为 10 的 CyclicBarrier CyclicBarrier cyclicBarrier = new CyclicBarrier(WORK_COUNT); for (int i = 0; i &lt; WORK_COUNT; i++) &#123; executorService.execute(new Worker(cyclicBarrier)); &#125; // 平滑地关闭 ExecutorService executorService.shutdown(); &#125;&#125;class Worker implements Runnable &#123; private final CyclicBarrier cyclicBarrier; Worker(CyclicBarrier cyclicBarrier) &#123; this.cyclicBarrier = cyclicBarrier; &#125; @Override public void run() &#123; System.out.print("before.."); try &#123; // 多个线程之间相互等待时，只有当每个线程都准备就绪后才能各自继续执行后面的操作 cyclicBarrier.await(); &#125; catch (InterruptedException | BrokenBarrierException e) &#123; e.printStackTrace(); &#125; // 子线程执行 doWork(); &#125; private void doWork() &#123; // ... System.out.print("after.."); &#125;&#125;// Output// before..before..before..before..before..before..before..before..before..before..after..after..after..after..after..after..after..after..after..after.. （2）执行 barrierAction 在 ready 状态时日志是每秒输出一条，当有 5 条 ready 时会一次性输出 5 条 continue。这就是前面讲的全部线程准备就绪后同时开始执行。在初始化 CyclicBarrier 时还可以在等待线程数后指定一个 runnable，含义是当线程到达这个屏障时优先执行这里的 runnable。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public class Solver &#123; private static final Integer WORK_COUNT = 10; private static final ExecutorService executorService = Executors.newCachedThreadPool(); public static void main(String[] args) throws InterruptedException &#123; // 初始化计数器为 5 的 CyclicBarrier CyclicBarrier cyclicBarrier = new CyclicBarrier(5, () -&gt; System.out.println(String.format("%s call back is ready.", Thread.currentThread().getName()))); for (int i = 0; i &lt; WORK_COUNT; i++) &#123; Thread.sleep(1000); executorService.execute(new Worker(cyclicBarrier)); &#125; // 平滑地关闭 ExecutorService executorService.shutdown(); &#125;&#125;class Worker implements Runnable &#123; private final CyclicBarrier cyclicBarrier; Worker(CyclicBarrier cyclicBarrier) &#123; this.cyclicBarrier = cyclicBarrier; &#125; @Override public void run() &#123; try &#123; System.out.println(String.format("%s is ready", Thread.currentThread().getName())); cyclicBarrier.await(); System.out.println(String.format("%s continue", Thread.currentThread().getName())); &#125; catch (InterruptedException | BrokenBarrierException e) &#123; e.printStackTrace(); &#125; &#125;&#125;// Output// pool-1-thread-1 is ready// pool-1-thread-2 is ready// pool-1-thread-3 is ready// pool-1-thread-4 is ready// pool-1-thread-5 is ready// pool-1-thread-5 call back is ready.// pool-1-thread-5 continue// pool-1-thread-3 continue// pool-1-thread-4 continue// pool-1-thread-1 continue// pool-1-thread-2 continue// pool-1-thread-5 is ready// pool-1-thread-1 is ready// pool-1-thread-4 is ready// pool-1-thread-2 is ready// pool-1-thread-3 is ready// pool-1-thread-3 call back is ready.// pool-1-thread-3 continue// pool-1-thread-5 continue// pool-1-thread-4 continue// pool-1-thread-2 continue// pool-1-thread-1 continue CyclicBarrier 与 CountDownLatch 区别CyclicBarrier 与 CountDownLatch 可能容易混淆，我们强调下其区别： CountDownLatch 的参与线程是有不同角色的，有的负责倒计时，有的在等待倒计时变为 0，负责倒计时和等待倒计时的线程都可以有多个，它用于不同角色线程间的同步。 CyclicBarrier 的参与线程角色是一样的，用于同一角色线程间的协调一致。 CountDownLatch 是一次性的，而 CyclicBarrier 是可以重复利用的。 Semaphore - 信号量简介Semaphore，又名信号量，这个类的作用有点类似于 “许可证”。信号量 Semaphore 是一个控制访问多个共享资源的计数器，和 CountDownLatch 一样，其本质上是一个 “共享锁”。从源码角度来看，Semaphore 的实现方式和 CountDownLatch 非常相似，基于 AQS 做了一些定制。通过维持 AQS 的锁全局计数 state 字段来实现定量锁的加锁和解锁操作。Semaphore 通常用于限制可以访问某些资源（物理或逻辑的）的线程数目。 有时，我们因为一些原因需要控制同时访问共享资源的最大线程数量，比如出于系统性能的考虑需要限流，或者共享资源是稀缺资源，我们需要有一种办法能够协调各个线程，以保证合理的使用公共资源。当有线程想要访问共享资源时，需要先获取 (acquire) 的许可；如果许可不够了，线程需要一直等待，直到许可可用。当线程使用完共享资源后，可以归还 (release) 许可，以供其它需要的线程使用；然而，实际上并没有真实的许可证对象供线程使用，Semaphore 只是对可用的数量进行管理维护。 使用场景Semaphore 可以用于做流量控制，特别公用资源有限的应用场景，比如数据库连接。 主要接口分析Semaphore 内部包含公平锁（FairSync）和非公平锁（NonfairSync），继承内部类 Sync，其中 Sync 继承 AQS，作为 Semaphore 的公平锁和非公平锁的基类。 CyclicBarrier 提供的关键方法如下： isFair()：是否公平模式 FIFO availablePermits()：获取当前可用的许可证数量 acquire()：当前线程尝试去阻塞的获取 1 个许可证。此过程是阻塞的，它会一直等待许可证，直到发生以下任意一件事：当前线程获取了 1 个可用的许可证，则会停止等待，继续执行；当前线程被中断，则会抛出 InterruptedException 异常，并停止等待，继续执行。 acquire(permits)：当前线程尝试去阻塞的获取 permits 个许可证。此过程是阻塞的，它会一直等待许可证，直到发生以下任意一件事：当前线程获取了 n 个可用的许可证，则会停止等待，继续执行；当前线程被中断，则会抛出 InterruptedException 异常，并停止等待，继续执行。 acquierUninterruptibly()：当前线程尝试去阻塞的获取 1 个许可证 (不可中断的)。此过程是阻塞的，它会一直等待许可证，直到发生以下任意一件事：当前线程获取了 1 个可用的许可证，则会停止等待，继续执行。 acquireUninterruptibly(permits)：当前线程尝试去阻塞的获取 permits 个许可证。此过程是阻塞的，它会一直等待许可证，直到发生以下任意一件事：当前线程获取了 n 个可用的许可证，则会停止等待，继续执行。 tryAcquire()：当前线程尝试去获取 1 个许可证。此过程是非阻塞的，它只是在方法调用时进行一次尝试。如果当前线程获取了 1 个可用的许可证，则会停止等待，继续执行，并返回 true。如果当前线程没有获得这个许可证，也会停止等待，继续执行，并返回 false。 tryAcquire(permits)：当前线程尝试去获取 permits 个许可证。此过程是非阻塞的，它只是在方法调用时进行一次尝试。如果当前线程获取了 permits 个可用的许可证，则会停止等待，继续执行，并返回 true。如果当前线程没有获得 permits 个许可证，也会停止等待，继续执行，并返回 false。 tryAcquire(timeout, TimeUnit)：当前线程在限定时间内，阻塞的尝试去获取 1 个许可证。此过程是阻塞的，它会一直等待许可证，直到发生以下任意一件事：当前线程获取了可用的许可证，则会停止等待，继续执行，并返回 true；当前线程等待时间 timeout 超时，则会停止等待，继续执行，并返回 false；当前线程在 timeout 时间内被中断，则会抛出 InterruptedException 一次，并停止等待，继续执行。 tryAcquire(permits, timeout, TimeUnit)：当前线程在限定时间内，阻塞的尝试去获取 permits 个许可证。此过程是阻塞的，它会一直等待许可证，直到发生以下任意一件事：当前线程获取了可用的 permits 个许可证，则会停止等待，继续执行，并返回 true；当前线程等待时间 timeout 超时，则会停止等待，继续执行，并返回 false；当前线程在 timeout 时间内被中断，则会抛出 InterruptedException 一次，并停止等待，继续执行。 release()：当前线程释放 1 个可用的许可证。 release(permits)：当前线程释放 permits 个可用的许可证。 drainPermits()：当前线程获得剩余的所有可用许可证。 hasQueuedThreads()：判断当前 Semaphore 对象上是否存在正在等待许可证的线程。 getQueueLength()：获取当前 Semaphore 对象上是正在等待许可证的线程数量。 Semaphore 提供了两个构造函数： Semaphore(int permits)：创建具有给定的许可数和非公平的公平设置的 Semaphore，Semaphore 默认选择非公平锁。 Semaphore(int permits, boolean fair)：创建具有给定的许可数和给定的公平设置的 Semaphore。Semaphore 有两种模式，公平模式和非公平模式。公平模式就是调用 acquire 的顺序就是获取许可证的顺序，遵循 FIFO；而非公平模式是抢占式的，也就是有可能一个新的获取线程恰好在一个许可证释放时得到了这个许可证，而前面还有等待的线程，简单的说就是随机选取新线程来运行。 Coding 演示12345678910111213141516171819202122232425public class SemaphoreExample &#123; public static void main(String[] args) &#123; final int clientCount = 3; final int totalRequestCount = 10; Semaphore semaphore = new Semaphore(clientCount); ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; totalRequestCount; i++) &#123; executorService.execute(()-&gt;&#123; try &#123; semaphore.acquire(); System.out.print(semaphore.availablePermits() + " "); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; semaphore.release(); &#125; &#125;); &#125; executorService.shutdown(); &#125;&#125;// Output// 2 1 2 2 2 2 2 1 2 2 Exchanger - 交换器简介Exchanger（交换器）是一个用于线程间协作的工具类，是 JDK 1.5 开始提供的一个用于两个工作线程之间交换数据的封装工具类。Exchanger 有点类似于 CyclicBarrier，我们知道 CyclicBarrier 是一个栅栏，到达栅栏的线程需要等待其它一定数量的线程到达后，才能通过栅栏，Exchanger 可以看成是一个双向栅栏。它提供一个同步点，在这个同步点两个线程可以交换彼此的数据。 可简单地将 Exchanger 对象理解为一个包含两个格子的容器，通过 exchanger 方法可以向两个格子中填充信息。当两个格子中的均被填充时，该对象会自动将两个格子的信息交换，然后返回给线程，从而实现两个线程的信息交换。这两个线程通过 exchange 方法交换数据，如果第一个线程先执行 exchange 方法，它会一直等待第二个线程也执行 exchange，当两个线程都到达同步点时，这两个线程就可以交换数据，将本线程生产出来的数据传递给对方。 主要接口分析Exchanger 是最简单的也是最复杂的，简单在于 API 非常简单，就一个构造方法和两个 exchange() 方法，最复杂在于它的实现是最复杂的。 Exchanger 提供的关键方法如下： exchange(V x) ：当前线程跟另外一个线程交换数据 x，如果另外一个线程的数据准备好，那么当前线程会立刻返回，并获得另外一个线程的数据；否则当前线程会进入等待状态。 V exchange(V x, long timeout, TimeUnit unit)：当前线程跟另外一个线程交换数据 x，有一个指定的超时时间，如果在等待时间超时了，而且还没有收到对方的数据的话，则会抛出 TimeoutException 异常。 可以看出，当一个线程到达 exchange 调用点时，如果其他线程此前已经调用了此方法，则其他线程会被调度唤醒并与之进行对象交换，然后各自返回；如果其他线程还没到达交换点，则当前线程会被挂起，直至其他线程到达才会完成交换并正常返回，或者当前线程被中断或超时返回。 Coding 演示1234567891011121314151617181920212223242526public class ExchangerExample &#123; private static final Integer WORK_COUNT = 2; public static void main(String[] args) &#123; Exchanger&lt;String&gt; exchanger = new Exchanger&lt;&gt;(); ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; WORK_COUNT; i++) &#123; executorService.execute(() -&gt; &#123; String beforeObj = Thread.currentThread().getName(); try &#123; String afterObj = exchanger.exchange(Thread.currentThread().getName()); System.out.println(String.format("currentThread %s , before exchange %s , after exchange %s", Thread.currentThread().getName(), beforeObj, afterObj)); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); &#125; executorService.shutdown(); &#125;&#125;// Output// currentThread pool-1-thread-1 , before exchange pool-1-thread-1 , after exchange pool-1-thread-2 // currentThread pool-1-thread-2 , before exchange pool-1-thread-2 , after exchange pool-1-thread-1 Phaser - 多阶段栅栏简介CountDownLatch 和 CyclicBarrier 都是 JDK 1.5 引入的，而 Phaser 是 JDK 1.7 引入的。Phaser 的功能与 CountDownLatch 和 CyclicBarrier 有部分重叠，它几乎可以取代 CountDownLatch 和 CyclicBarrier， 其功能更灵活，更强大，支持动态调整需要控制的线程数。 CountDownLatch，闭锁，在完成一组正在其他线程中执行的操作之前，它允许一个或多个线程一直等待，它提供了 await()、countDown() 两个方法来进行操作；CyclicBarrier，循环栅栏，允许一组线程互相等待，直到到达某个公共屏障点，它提供的 await() 可以实现让所有参与者在临界点到来之前一直处于等待状态；Phaser，多阶段栅栏，它把多个线程协作执行的任务划分为多个阶段，编程时需要明确各个阶段的任务，每个阶段都可以有任意个参与者，线程都可以随时注册并参与到某个阶段，当到达的参与者数量满足栅栏设定的数量后，会进行阶段升级（advance）。 Phaser 顾名思义，与阶段相关。Phaser 比较适合这样一种场景，一种任务可以分为多个阶段，现希望多个线程去处理该批任务，对于每个阶段，多个线程可以并发进行，但是希望保证只有前面一个阶段的任务完成之后才能开始后面的任务。这种场景可以使用多个 CyclicBarrier 来实现，每个 CyclicBarrier 负责等待一个阶段的任务全部完成。但是使用 CyclicBarrier 的缺点在于，需要明确知道总共有多少个阶段，同时并行的任务数需要提前预定义好，且无法动态修改。而 Phaser 可同时解决这两个问题。 使用场景Phaser 主要接口如下： 主要接口分析Phaser 提供的关键方法如下： arriveAndAwaitAdvance()：当前线程当前阶段执行完毕，等待其它线程完成当前阶段。如果当前线程是该阶段最后一个未到达的，则该方法直接返回下一个阶段的序号（阶段序号从 0 开始），同时其它线程的该方法也返回下一个阶段的序号。arriveAndAwaitAdvance 方法是不响应中断的，也就是说即使当前线程被中断，arriveAndAwaitAdvance 方法也不会返回或抛出异常，而是继续等待。如果希望能够响应中断，可以参考 awaitAdvanceInterruptibly 方法。 arriveAndDeregister()：该方法立即返回下一阶段的序号，并且其它线程需要等待的个数减一，并且把当前线程从之后需要等待的成员中移除。如果该 Phaser 是另外一个 Phaser 的子 Phaser，并且该操作导致当前 Phaser 的成员数为 0，则该操作也会将当前 Phaser 从其父 Phaser 中移除。 arrive()：该方法不作任何等待，直接返回下一阶段的序号。 awaitAdvance(int phase)：该方法等待某一阶段执行完毕。如果当前阶段不等于指定的阶段或者该 Phaser 已经被终止，则立即返回。该阶段数一般由 arrive() 方法或者 arriveAndDeregister() 方法返回。返回下一阶段的序号，或者返回参数指定的值（如果该参数为负数），或者直接返回当前阶段序号（如果当前 Phaser 已经被终止）。 awaitAdvanceInterruptibly(int phase)：效果与 awaitAdvance(int phase) 相当，唯一的不同在于若该线程在该方法等待时被中断，则该方法抛出 InterruptedException。 awaitAdvanceInterruptibly(int phase, long timeout, TimeUnit unit)：效果与 awaitAdvanceInterruptibly(int phase) 相当，区别在于如果超时则抛出 TimeoutException。 bulkRegister(int parties)：注册多个 party。如果当前 phaser 已经被终止，则该方法无效，并返回负数。如果调用该方法时，onAdvance 方法正在执行，则该方法等待其执行完毕。如果该 Phaser 有父 Phaser 则指定的 party 数大于 0，且之前该 Phaser 的 party 数为 0，那么该 Phaser 会被注册到其父 Phaser 中。 forceTermination()：强制让该 Phaser 进入终止状态。已经注册的 party 数不受影响。如果该 Phaser 有子 Phaser，则其所有的子 Phaser 均进入终止状态。如果该 Phaser 已经处于终止状态，该方法调用不造成任何影响。 Coding 演示（1）通过 Phaser 实现 CyclicBarrier 控制多个线程的执行时机的功能 通过 Phaser 控制多个线程的执行时机：有时候我们希望所有线程到达指定点后再同时开始执行，我们可以利用 CyclicBarrier 来实现，这里给出使用 Phaser 的版本。 123456789101112131415161718192021222324252627282930public class PhaserExample &#123; public static void main(String[] args) &#123; final int totalRequestCount = 10; Phaser phaser = new Phaser(); ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; totalRequestCount; i++) &#123; phaser.register(); executorService.execute(() -&gt; &#123; // 等待其它参与者线程到达 [arriveAndAwaitAdvance 方法是不响应中断的，也就是说即使当前线程被中断，arriveAndAwaitAdvance 方法也不会返回或抛出异常，而是继续等待。如果希望能够响应中断，可以参考 awaitAdvanceInterruptibly 方法] int j = phaser.arriveAndAwaitAdvance(); // do something System.out.println(String.format("currentThread:%s, Executing the task, currentPhase:%s", Thread.currentThread().getName(), j)); &#125;); &#125; executorService.shutdown(); &#125;&#125;// Output// currentThread:pool-1-thread-1, Executing the task, currentPhase:1// currentThread:pool-1-thread-2, Executing the task, currentPhase:1// currentThread:pool-1-thread-7, Executing the task, currentPhase:1// currentThread:pool-1-thread-4, Executing the task, currentPhase:1// currentThread:pool-1-thread-6, Executing the task, currentPhase:1// currentThread:pool-1-thread-3, Executing the task, currentPhase:1// currentThread:pool-1-thread-10, Executing the task, currentPhase:1// currentThread:pool-1-thread-9, Executing the task, currentPhase:1// currentThread:pool-1-thread-5, Executing the task, currentPhase:1// currentThread:pool-1-thread-8, Executing the task, currentPhase:1 （2）通过 Phaser 实现 CyclicBarrier 执行 barrierAction CyclicBarrier 支持 barrier action, Phaser 同样也支持。不同之处是 Phaser 的 barrier action 需要改写 onAdvance 方法来进行定制。 123456789101112131415161718192021222324252627282930313233343536373839public class PhaserExample &#123; public static void main(String[] args) &#123; final int totalRequestCount = 10; Phaser phaser = new Phaser() &#123; @Override protected boolean onAdvance(int phase, int registeredParties) &#123; System.out.println(String.format("%s call back is ready.", Thread.currentThread().getName())); return super.onAdvance(phase, registeredParties); &#125; &#125;; ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; totalRequestCount; i++) &#123; // 注册各个参与者线程 phaser.register(); executorService.execute(() -&gt; &#123; // 等待其它参与者线程到达 [arriveAndAwaitAdvance 方法是不响应中断的，也就是说即使当前线程被中断，arriveAndAwaitAdvance 方法也不会返回或抛出异常，而是继续等待。如果希望能够响应中断，可以参考 awaitAdvanceInterruptibly 方法] int j = phaser.arriveAndAwaitAdvance(); // do something System.out.println(String.format("currentThread:%s, Executing the task, currentPhase:%s", Thread.currentThread().getName(), j)); &#125;); &#125; executorService.shutdown(); &#125;&#125;// Output// pool-1-thread-10 call back is ready.// currentThread:pool-1-thread-10, Executing the task, currentPhase:1// currentThread:pool-1-thread-9, Executing the task, currentPhase:1// currentThread:pool-1-thread-7, Executing the task, currentPhase:1// currentThread:pool-1-thread-8, Executing the task, currentPhase:1// currentThread:pool-1-thread-5, Executing the task, currentPhase:1// currentThread:pool-1-thread-3, Executing the task, currentPhase:1// currentThread:pool-1-thread-1, Executing the task, currentPhase:1// currentThread:pool-1-thread-6, Executing the task, currentPhase:1// currentThread:pool-1-thread-2, Executing the task, currentPhase:1// currentThread:pool-1-thread-4, Executing the task, currentPhase:1 （3）通过 Phaser 实现 CountDownLatch 作为一个开关 / 入口功能 12345678910111213141516171819202122232425262728293031323334353637public class PhaserExample &#123; public static void main(String[] args) throws IOException &#123; final int totalRequestCount = 10; // 注册主线程, 当外部条件满足时, 由主线程打开开关 Phaser phaser = new Phaser(1); ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; totalRequestCount; i++) &#123; // 注册各个参与者线程 phaser.register(); executorService.execute(() -&gt; &#123; // 等待其它参与者线程到达 [arriveAndAwaitAdvance 方法是不响应中断的，也就是说即使当前线程被中断，arriveAndAwaitAdvance 方法也不会返回或抛出异常，而是继续等待。如果希望能够响应中断，可以参考 awaitAdvanceInterruptibly 方法] int j = phaser.arriveAndAwaitAdvance(); // do something System.out.println(String.format("currentThread:%s, Executing the task, currentPhase:%s", Thread.currentThread().getName(), j)); &#125;); &#125; // 打开开关 [parties 共 11 个, 主线程从之后需要等待的成员中移除, 即 parties 还剩 10] phaser.arriveAndDeregister(); System.out.println("主线程打开了开关"); executorService.shutdown(); &#125;&#125;// Output// 主线程打开了开关// currentThread:pool-1-thread-6, Executing the task, currentPhase:1// currentThread:pool-1-thread-7, Executing the task, currentPhase:1// currentThread:pool-1-thread-1, Executing the task, currentPhase:1// currentThread:pool-1-thread-8, Executing the task, currentPhase:1// currentThread:pool-1-thread-3, Executing the task, currentPhase:1// currentThread:pool-1-thread-9, Executing the task, currentPhase:1// currentThread:pool-1-thread-4, Executing the task, currentPhase:1// currentThread:pool-1-thread-2, Executing the task, currentPhase:1// currentThread:pool-1-thread-5, Executing the task, currentPhase:1 （4）通过 Phaser 实现分层 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class PhaserExample &#123; public static void main(String[] args) &#123; final int parties = 3; final int phases = 4; Phaser phaser = new Phaser() &#123; @Override protected boolean onAdvance(int phase, int registeredParties) &#123; System.out.println("====== Phase :" + phase + "======"); return super.onAdvance(phase, registeredParties); &#125; &#125;; ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; parties; i++) &#123; // 注册各个参与者线程 phaser.register(); executorService.execute(() -&gt; &#123; for (int phase = 0; phase &lt; phases; phase++) &#123; // 等待其它参与者线程到达 [arriveAndAwaitAdvance 方法是不响应中断的，也就是说即使当前线程被中断，arriveAndAwaitAdvance 方法也不会返回或抛出异常，而是继续等待。如果希望能够响应中断，可以参考 awaitAdvanceInterruptibly 方法] int j = phaser.arriveAndAwaitAdvance(); // do something System.out.println(String.format("currentThread:%s, Executing the task, currentPhase:%s", Thread.currentThread().getName(), j)); &#125; &#125;); &#125; executorService.shutdown(); &#125;&#125;// Output// ====== Phase : 0 ======// currentThread:pool-1-thread-1, Executing the task, currentPhase:1// currentThread:pool-1-thread-2, Executing the task, currentPhase:1// currentThread:pool-1-thread-3, Executing the task, currentPhase:1// ====== Phase : 1 ======// currentThread:pool-1-thread-3, Executing the task, currentPhase:2// currentThread:pool-1-thread-1, Executing the task, currentPhase:2// currentThread:pool-1-thread-2, Executing the task, currentPhase:2// ====== Phase : 2 ======// currentThread:pool-1-thread-2, Executing the task, currentPhase:3// currentThread:pool-1-thread-1, Executing the task, currentPhase:3// currentThread:pool-1-thread-3, Executing the task, currentPhase:3// ====== Phase : 3 ======// currentThread:pool-1-thread-3, Executing the task, currentPhase:4// currentThread:pool-1-thread-1, Executing the task, currentPhase:4// currentThread:pool-1-thread-2, Executing the task, currentPhase:4 参考博文[1]. 【并发编程】J.U.C 之 AQS 介绍、实现及其子类使用演示[2]. Java 进阶（四）线程间通信剖析[3]. 透彻理解 Java 并发编程[4]. 死磕 Java 并发 Java 并发编程之美系列 Java 并发编程之美（一）：并发队列 Queue 原理剖析 Java 并发编程之美（二）：线程池 ThreadPoolExecutor 原理探究 Java 并发编程之美（三）：异步执行框架 Eexecutor Java 并发编程之美（四）：深入剖析 ThreadLocal Java 并发编程之美（五）：揭开 InheritableThreadLocal 的面纱 Java 并发编程之美（六）：J.U.C 之线程同步辅助工具类]]></content>
      <categories>
        <category>并发</category>
      </categories>
      <tags>
        <tag>Semaphore</tag>
        <tag>CountDownLatch</tag>
        <tag>CyclicBarrier</tag>
        <tag>Phaser</tag>
        <tag>Exchanger</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 进阶讲解（二）：快速生成测试数据以及 EXPLAIN 详解]]></title>
    <url>%2Farchives%2F46faebc7.html</url>
    <content type="text"><![CDATA[前言索引类似大学图书馆建书目索引，可以提高数据检索的效率，降低数据库的 IO 成本。MySQL 在 300w 条记录左右性能开始逐渐下降，虽然官方文档说 500~800w 记录，所以大数据量建立索引是非常有必要的。MySQL 提供了 EXPLAIN，用于显示 SQL 执行的详细信息，可以进行索引的优化。使用 EXPLAIN 关键字可以模拟优化器执行 SQL 查询语句，从而知道 MySQL 是如何处理你的 SQL 语句的，分析你的查询语句或是表结构的性能瓶颈。 可以帮助选择更好的索引和写出更优化的查询语句。 本章首先介绍如何通过存储过程随机生成大量随机数据作为 EXPLIAN 的测试数据，然后通过例子详解 EXPLIAN 用法以及各字段含义，最后对 EXPLIAN 用途进行总结。 EXPLAIN 概述EXPLAIN 命令是查看查询优化器如何决定执行查询的主要方法，使用 EXPLAIN，只需要在查询中的 SELECT 关键字之前增加 EXPLAIN 这个词即可，MYSQL 会在查询上设置一个标记，当执行查询时，这个标记会使其返回关于在执行计划中每一步的信息，而不是执行它，它会返回一行或多行信息，显示出执行计划中的每一部分和执行的次序，从而可以从分析结果中找到查询语句或是表结构的性能瓶颈。 通过 EXPLAIN，我们可以分析出以下结果： 表的读取顺序 数据读取操作的操作类型 哪些索引可以使用 哪些索引被实际使用 表之间的引用 每张表有多少行被优化器查询 随机生成大量测试数据利用 MySQL 内存表插入速度快的特点，先利用函数和存储过程在内存表中生成数据，然后再从内存表插入普通表中。 （1）登录 MySQL 1234567891011# 1. 连接到远程主机上的 MySQL$ mysql -h [host] -u [username] -p [password]# 2. 查看所有的数据库mysql&gt; show databases;# 3. 选择数据库mysql&gt; use [table_name];# 4. 查看数据库中的表 mysql&gt; show tables; （2）创建内存表 如果一条一条插入普通表的话，效率太低下，但内存表插入速度是很快的，可以先建立一张内存表，插入数据后，在导入到普通表中。 123456789101112131415DROP TABLE IF EXISTS `big_data_user_memory`;CREATE TABLE `big_data_user_memory` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT COMMENT '主键ID', `user_id` varchar(20) DEFAULT NULL COMMENT '用户ID', `user_name` varchar(20) DEFAULT NULL COMMENT '用户名称', `age` tinyint(3) DEFAULT NULL COMMENT '年龄', `gender` tinyint(1) DEFAULT NULL COMMENT '性别 [0: 男性; 1: 女性]', `phone` varchar(20) DEFAULT NULL COMMENT '手机', `group_id` int(11) DEFAULT NULL COMMENT '分组ID', `join_time` datetime DEFAULT NULL COMMENT '加入时间', `gmt_create` datetime DEFAULT NULL COMMENT '创建时间', `gmt_modified` datetime DEFAULT NULL COMMENT '更新时间', PRIMARY KEY (`id`), KEY `index_user_id` (`user_id`) USING HASH) ENGINE=MEMORY DEFAULT CHARSET=utf8mb4; （3）创建普通表 创建普通表，参数设置和内存表相同，否则从内存表往普通标导入数据会报错。 123456789101112131415DROP TABLE IF EXISTS `big_data_user`;CREATE TABLE `big_data_user` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT COMMENT '主键ID', `user_id` varchar(20) DEFAULT NULL COMMENT '用户ID', `user_name` varchar(20) DEFAULT NULL COMMENT '用户名称', `age` tinyint(3) DEFAULT NULL COMMENT '年龄', `gender` tinyint(1) DEFAULT NULL COMMENT '性别 [0: 男性; 1: 女性]', `phone` varchar(20) DEFAULT NULL COMMENT '手机', `group_id` int(11) DEFAULT NULL COMMENT '分组ID', `join_time` datetime DEFAULT NULL COMMENT '加入时间', `gmt_create` datetime DEFAULT NULL COMMENT '创建时间', `gmt_modified` datetime DEFAULT NULL COMMENT '更新时间', PRIMARY KEY (`id`), KEY `index_user_id` (`user_id`) USING BTREE) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; （4）创建存储函数 123456789101112131415-- 生成随机 UserIdCREATE DEFINER=`root`@`localhost` FUNCTION `generateCode`( n int ) RETURNS varchar(20) CHARSET utf8 DETERMINISTICBEGIN DECLARE chars_str VARCHAR ( 100 ) DEFAULT 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'; DECLARE return_str VARCHAR ( 255 ) DEFAULT ''; DECLARE i INT DEFAULT 0; WHILE i &lt; n DO SET return_str = concat(return_str, SUBSTRING(chars_str, FLOOR( 1 + RAND() * 62 ), 1 )); SET i = i + 1; END WHILE; RETURN return_str;END 123456789101112131415161718192021# 生成随机中文名CREATE DEFINER=`root`@`localhost` FUNCTION `generateUserName`() RETURNS varchar(255) CHARSET utf8 DETERMINISTICBEGIN #Routine body goes here... DECLARE last_name varchar(2056) DEFAULT '赵钱孙李周郑王冯陈楮卫蒋沈韩杨朱秦尤许何吕施张孔曹严华金魏陶姜戚谢喻柏水窦章云苏潘葛奚范彭郎鲁韦昌马苗凤花方俞任袁柳酆鲍史唐费廉岑薛雷贺倪汤滕殷罗毕郝邬安常乐于时傅皮齐康伍余元卜顾孟平黄和穆萧尹姚邵湛汪祁毛禹狄米贝明臧计伏成戴谈宋茅庞熊纪舒屈项祝董梁杜阮蓝闽席季麻强贾路娄危江童颜郭梅盛林刁锺徐丘骆高夏蔡田樊胡凌霍虞万支柯昝管卢莫经裘缪干解应宗丁宣贲邓郁单杭洪包诸左石崔吉钮龚程嵇邢滑裴陆荣翁'; DECLARE first_name varchar(2056) DEFAULT '嘉懿煜城懿轩烨伟苑博伟泽熠彤鸿煊博涛烨霖烨华煜祺智宸正豪昊然明杰诚立轩立辉峻熙弘文熠彤鸿煊烨霖哲瀚鑫鹏致远俊驰雨泽烨磊晟睿天佑文昊修洁黎昕远航旭尧鸿涛伟祺轩越泽浩宇瑾瑜皓轩擎苍擎宇志泽睿渊楷瑞轩弘文哲瀚雨泽鑫磊梦琪忆之桃慕青问兰尔岚元香初夏沛菡傲珊曼文乐菱痴珊恨玉惜文香寒新柔语蓉海安夜蓉涵柏水桃醉蓝春儿语琴从彤傲晴语兰又菱碧彤元霜怜梦紫寒妙彤曼易南莲紫翠雨寒易烟如萱若南寻真晓亦向珊慕灵以蕊寻雁映易雪柳孤岚笑霜海云凝天沛珊寒云冰旋宛儿绿真盼儿晓霜碧凡夏菡曼香若烟半梦雅绿冰蓝灵槐平安书翠翠风香巧代云梦曼幼翠友巧听寒梦柏醉易访旋亦玉凌萱访卉怀亦笑蓝春翠靖柏夜蕾冰夏梦松书雪乐枫念薇靖雁寻春恨山从寒忆香觅波静曼凡旋以亦念露芷蕾千兰新波代真新蕾雁玉冷卉紫山千琴恨天傲芙盼山怀蝶冰兰山柏翠萱乐丹翠柔谷山之瑶冰露尔珍谷雪乐萱涵菡海莲傲蕾青槐冬儿易梦惜雪宛海之柔夏青亦瑶妙菡春竹修杰伟诚建辉晋鹏天磊绍辉泽洋明轩健柏煊昊强伟宸博超君浩子骞明辉鹏涛炎彬鹤轩越彬风华靖琪明诚高格光华国源宇晗昱涵润翰飞翰海昊乾浩博和安弘博鸿朗华奥华灿嘉慕坚秉建明金鑫锦程瑾瑜鹏经赋景同靖琪君昊俊明季同开济凯安康成乐语力勤良哲理群茂彦敏博明达朋义彭泽鹏举濮存溥心璞瑜浦泽奇邃祥荣轩'; DECLARE return_str varchar(2056) DEFAULT ''; # 一个中文的长度是 3 位 DECLARE first_name_length int DEFAULT LENGTH(first_name) / 3; DECLARE last_name_length int DEFAULT LENGTH(last_name) / 3; SET return_str = CONCAT(return_str, SUBSTRING(last_name, FLOOR(1 + RAND() * last_name_length), 1)); SET return_str = CONCAT(return_str, SUBSTRING(first_name, FLOOR(1 + RAND() * first_name_length), 1)); IF RAND() &gt; 0.400 THEN SET return_str = CONCAT(return_str, SUBSTRING(first_name, FLOOR(1 + RAND() * first_name_length), 1)); END IF; RETURN return_str;END 123456789101112131415161718# 生成随机手机号CREATE DEFINER=`root`@`localhost` FUNCTION `generatePhone`() RETURNS char(11) CHARSET utf8 DETERMINISTICBEGIN #Routine body goes here... DECLARE head VARCHAR(256) DEFAULT '133、153、180、181、189、177、173、149、130、131、132、155、156、145、185、186、176、175、134、135、136、137、138、139、150、151、152、157、158、159、182、183、184、187、188、147、178'; DECLARE content CHAR(10) DEFAULT '0123456789'; DECLARE phone CHAR(11) DEFAULT substring( head, 1 + ( FLOOR(( RAND() * 37 ))* 4 ), 3 ); DECLARE i int DEFAULT 1; DECLARE len int DEFAULT LENGTH(content); WHILE i &lt;= 8 DO SET i = i + 1; SET phone = CONCAT(phone, substring( content, floor( 1 + RAND() * len ), 1 )); END WHILE; RETURN phone;END 1234567891011121314# 生成随机'yyyy-MM-dd'至'yyyy-MM-dd'时间CREATE DEFINER=`root`@`localhost` FUNCTION `generateDateTime`(begin_time char(10), end_time char(10)) RETURNS datetimeBEGIN #Routine body goes here... DECLARE date_time VARCHAR ( 255 ) DEFAULT ''; DECLARE local_date VARCHAR ( 255 ) DEFAULT ''; DECLARE local_time VARCHAR ( 255 ) DEFAULT ''; SET local_date = DATE(FROM_UNIXTIME( UNIX_TIMESTAMP( begin_time ) + FLOOR( RAND() * ( UNIX_TIMESTAMP( end_time ) - UNIX_TIMESTAMP( begin_time ) + 1 ) ))); SET local_time = CONCAT(local_time, FLOOR(RAND() * 24), ':', FLOOR(RAND() * 60), ':', FLOOR(RAND() * 60)); SET date_time = CONCAT(local_date, ' ', local_time); RETURN date_time;END （5）创建存储过程 1234567891011121314CREATE DEFINER=`root`@`localhost` PROCEDURE `generateBigDataUser`(IN num INT)BEGIN #Routine body goes here... #申明变量i,默认为1 DECLARE i INT DEFAULT 1; #当i小于传入的参数时执行循环插入 WHILE i &lt;= num DO INSERT INTO `big_data_user_memory`(`user_id`, `user_name`, `age`, `gender`, `phone`, `group_id`, `join_time`, `gmt_create`, `gmt_modified`) VALUES (generateCode(20), generateUserName(), 18 + FLOOR(RAND() * 50), FLOOR(RAND() * 2), generatePhone(), FLOOR(RAND() * 100), generateDateTime('1990-01-01', '2019-08-15'), DATE(NOW() - INTERVAL (FLOOR(rand() * 1000 )) DAY), NOW()); SET i = i + 1; END WHILE;END （6）调用存储过程 1CALL generateBigDataUser(1000000); 在调用存储过程的过程中内存表大小的问题抛出 “The table ‘big_data_memory’ is full”，这是就需要我们修改一下 MySQL 的配置信息。 1234567891011# 1. 查看 tmp_table_size 大小, tmp_table_size: 控制内存临时表的最大值, 超过限值后就往硬盘写, 写的位置由变量 tmpdir 决定 mysql&gt; SHOW variables like '%tmp_table_size%';# 2. 查看 max_heap_table_size 大小, max_heap_table_size: 用户可以创建的内存表 (memory table) 的大小. 这个值用来计算内存表的最大行数值mysql&gt; SHOW VARIABLES LIKE '%max_heap_table_size%';# 3. 修改 tmp_table_size 大小mysql&gt; SET SESSION tmp_table_size = 1024 * 1024 * 1024;# 4. 修改 max_heap_table_size 大小mysql&gt; SET SESSION max_heap_table_size = 1024 * 1024 * 1024; （7）将内存表中的数据导入普通表 1mysql&gt; INSERT INTO big_data_user SELECT * FROM big_data_user_memory; 以上，我们通过存储过程快速产生百万条随机测试数据的工作就大功告成了。接下来，我们将用我们产生的数据为基础详解 EXPLIAN 用法以及各字段含义。 （8）准备关联查询数据 12345678CREATE TABLE `big_data_group` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT COMMENT '主键ID', `group_code` varchar(20) DEFAULT NULL COMMENT '分组编码', `number_of_people` int(11) DEFAULT NULL COMMENT '人数', `gmt_create` datetime DEFAULT NULL COMMENT '创建时间', `gmt_modified` datetime DEFAULT NULL COMMENT '更新时间', PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; 123456789101112CREATE DEFINER=`root`@`localhost` PROCEDURE `generateBigDataGroup`(IN num INT)BEGIN #Routine body goes here... DECLARE i INT DEFAULT 1; #当i小于传入的参数时执行循环插入 WHILE i &lt;= num DO INSERT INTO `big_data_group`(`group_code`, `number_of_people`, `gmt_create`, `gmt_modified`) VALUES (generateCode(15), FLOOR(RAND() * 10000), DATE(NOW() - INTERVAL (FLOOR(rand() * 1000 )) DAY), NOW()); SET i = i + 1; END WHILE;END 1mysql&gt; CALL generateBigDataGroup(100) EXPLIAN 用法以及各字段含义EXPLIAN 模拟优化器执行 SQL 语句，在 5.6 以及以后的版本中，除过 SELECT，其他比如 INSERT，UPDATE 和 DELETE 均可以使用 EXPLIAN 查看执行计划，从而知道 MySQL 是如何处理 SQL 语句，分析查询语句或者表结构的性能瓶颈。 本次 EXPLIAN 以根据手机号码过滤测试数据中手机号码重复的、保留 ID 最小数据的滤重 SQL 语句为例子。 123456EXPLAINDELETE FROM big_data_user WHERE phone IN (SELECT phone FROM (SELECT phone FROM big_data_user bdu1 GROUP BY bdu1.phone HAVING count(*)&gt; 1) p) AND id NOT IN (SELECT id FROM (SELECT min(bdu2.id) FROM big_data_user bdu2 GROUP BY bdu2.phone HAVING count(*)&gt; 1) b); EXPLIAN 出来的信息有 12 列，分别是 id、select_type、table、partitions、type、possible_keys、key、key_len、ref、rows、filtered、Extra id - 查询标识查询标识，表示 SQL 语句中执行 SELECT 子句或者是操作的顺序。 id 相同时执行顺序从上至下。 id 不同时，如果是子查询，id 的序号会递增，序号越大的越先执行。 id 相同，不同都存在时，id 相同的可以认为是一组查询按从上至下的顺序执行，id 值越大越优先执行。 id 为 NULL，如果行引用其他行的联合结果，则值可以为 NULL。在这种情况下，表列显示像 &lt;unionM,N&gt; 这样的值，以指示该行引用 id 值为 M 和 N 的行的并。 select_type - 查询类型查询类型，主要是用于区分普通查询、联合查询、子查询等复杂的查询。 SIMPLE：简单的 select 查询，查询中不包含子查询或者 UNION 1EXPLAIN SELECT * FROM big_data_user WHERE user_id='Jt2BHyxQqsPBoZAO9adp'; PRIMARY：查询中若包含任何复杂的子部分，最外层查询则被标记为 PRIMARY 1EXPLAIN SELECT *, (SELECT group_code FROM big_data_group WHERE id=group_id) AS group_code FROM big_data_user WHERE user_id='Jt2BHyxQqsPBoZAO9adp'; SUBQUERY：在 SELECT 或 WHERE 列表中包含了子查询 1EXPLAIN SELECT * FROM big_data_user WHERE group_id = (SELECT id FROM big_data_group WHERE group_code='cqlhc1nBKNAlOTQ'); DEPENDENT SUBQUERY：在 SELECT 或 WHERE 列表中包含了子查询，该子查询依赖外层查询。 1EXPLAIN SELECT *, (SELECT group_code FROM big_data_group WHERE id=group_id) AS group_code FROM big_data_user WHERE user_id='Jt2BHyxQqsPBoZAO9adp'; DERIVED：在 FROM 列表中包含的子查询被标记为 DERIVED（衍生），MySQL 会递归执行这些子查询，把结果放在临时表中 1EXPLAIN SELECT * FROM (SELECT * FROM big_data_user LIMIT 5) AS bdu UNION：若第二个 SELECT 出现在 UNION 之后，则被标记为 UNION；若 UNION 包含在 FROM 子句的子查询中，外层 SELECT 将被标记为DERIVED 1EXPLAIN SELECT * FROM big_data_user WHERE user_id = 'Jt2BHyxQqsPBoZAO9adp' UNION SELECT * FROM big_data_user WHERE phone = '13982711661'; UNION RESULT：从 UNION 表获取结果的 SELECT 1EXPLAIN SELECT * FROM big_data_user WHERE user_id = 'Jt2BHyxQqsPBoZAO9adp' UNION SELECT * FROM big_data_user WHERE phone = '13982711661'; table - 查询涉及表查询涉及表，显示这一行的数据是关于哪张表的。这也可以是下列值之一： &lt;unionM,N&gt;：输出行引用了 id 值为 M 和 N 的行的 UNION 结果。 &lt; derivedN &gt;：该行引用了一个 id 值为 n 的行的派生表结果。 &lt; subqueryN &gt;：输出行引用了 id 值为 N 的行的物化子查询的结果。 partitions - 匹配到的分区信息匹配到的分区信息，由查询匹配记录的分区。对于非分区表，值为 NULL。 type - 连接类型连接类型，对表访问方式，表示 MySQL 在表中找到所需行的方式，又称 “访问类型”。常用的类型有： ALL、index、range、 ref、eq_ref、const、system、NULL（从左到右，性能从差到好）。SQL 性能优化的目标：至少要达到 range 级别，要求是 ref 级别，如果可以是 consts最好。 system: 表中只有一条数据， 这个类型是特殊的 const 类型。 const: 针对主键或唯一索引的等值查询扫描，最多只返回一行数据。 const 查询速度非常快， 因为它仅仅读取一次即可。 1EXPLAIN SELECT * FROM big_data_user WHERE id = 1; eq_ref: 此类型通常出现在多表的 join 查询，表示对于前表的每一个结果，都只能匹配到后表的一行结果。并且查询的比较操作通常是 =，查询效率较高。 1EXPLAIN SELECT * FROM big_data_user bdu LEFT JOIN big_data_group bdg ON bdu.group_id = bdg.id; ref: 此类型通常出现在多表的 join 查询，针对于非唯一或非主键索引，或者是使用了最左前缀规则索引的查询。 1EXPLAIN SELECT * FROM big_data_user WHERE user_id='Jt2BHyxQqsPBoZAO9adp'; range: 表示使用索引范围查询，通过索引字段范围获取表中部分数据记录。这个类型通常出现在 =, &lt;&gt;, &gt;, &gt;=, &lt;, &lt;=, IS NULL, &lt;=&gt;, BETWEEN, IN() 操作中。 1EXPLAIN SELECT * FROM big_data_user WHERE id BETWEEN 2 AND 8; index: 表示全索引扫描 (full index scan)，和 ALL 类型类似，只不过 ALL 类型是全表扫描，而 index 类型则仅仅扫描所有的索引，而不扫描数据。index 类型通常出现在：所要查询的数据直接在索引树中就可以获取到, 而不需要扫描数据。当是这种情况时，Extra 字段 会显示 Using index。例如下面这个例子：EXPLAIN SELECT id FROM big_data_user; 1EXPLAIN SELECT id FROM big_data_user; ALL: 表示全表扫描，这个类型的查询是性能最差的查询之一。通常来说， 我们的查询不应该出现 ALL 类型的查询，因为这样的查询在数据量大的情况下，对数据库的性能是巨大的灾难。 如一个查询是 ALL 类型查询， 那么一般来说可以对相应的字段添加索引来避免。 1EXPLAIN SELECT * FROM big_data_user; possible_keys - 可能选择的索引可能选择的索引，它表示 MySQL 在查询时，可能使用到的索引。 注意，即使有些索引在 possible_keys 中出现，但是并不表示此索引会真正地被 MySQL 使用到。 MySQL 在查询时具体使用了哪些索引，由 key 字段决定。 key - 实际使用的索引实际使用的索引，实际使用的索引，如果为null，则没有使用索引，因此会出现possible_keys列有可能被用到的索引，但是key列为null，表示实际没用索引。 key_len - 实际使用的索引的长度实际使用的索引的长度，表示索引中使用的字节数，而通过该列计算查询中使用的索引长度，在不损失精确性的情况下，长度越短越好，key_len显示的值为索引字段的最大可能长度，并非实际使用长度，即key_len是根据表定义计算而得而不是通过表内检索出的。 ref - 和索引进行比较的列和索引进行比较的列，表示哪些列或常量与键列中命名的索引相比较，以从表中选择行。 1EXPLAIN SELECT * FROM big_data_user bdu LEFT JOIN big_data_group bdg ON bdu.group_id = bdg.id; rows - 需要被检索的大致行数需要被检索的大致行数，根据表统计信息及索引选用情况，大只估算出找到所需的记录所需要读取的行数。 filtered - 按表条件过滤的行百分比按表条件过滤的行百分比，该列表示将被表条件过滤的表行的估计百分比。 最大值为100，这意味着没有发生行过滤。值从100下降表明过滤量增加。 Extra - 额外信息额外信息，不适合在其他字段中显示，但是十分重要的额外信息。 123456EXPLAINDELETE FROM big_data_user WHERE phone IN (SELECT phone FROM (SELECT phone FROM big_data_user bdu1 GROUP BY bdu1.phone HAVING count(*)&gt; 1) p) AND id NOT IN (SELECT id FROM (SELECT min(bdu2.id) FROM big_data_user bdu2 GROUP BY bdu2.phone HAVING count(*)&gt; 1) b); Using filesort : 表示 MySQL 需额外的排序操作，不能通过索引顺序达到排序效果，MySQL Query Optimizer 不得不选择相应的排序算法来实现。一般有 using filesort 都建议优化去掉，因为这样的查询 cpu 资源消耗大。 Using temporary : 使用了临时表保存中间结果，MySQL 在对查询结果排序时使用了临时表。常见于 order by， group by， join 操作，查询效率不高，建议优化。 Using index : 发生了索引覆盖 ， 查询时在索引树上取到了需要查询的数据，不需要再进行回行操作。 Using join buffer : 使用了连接缓存，Block Nested Loop，连接算法是块嵌套循环连接；Batched Key Access，连接算法是批量索引连接。 Using where : 表示 MySQL 服务器从存储引擎收到查询数据，再进行 “后过滤”（Post-filter）。所谓 “后过滤”，就是先读取整行数据，再检查此行是否符合 where 句的条件，符合就留下，不符合便丢弃。因为检查是在读取行后才进行的，所以称为 “后过滤”。注意：Extra 列出现 Using where 表示 MySQL 服务器将存储引擎返回服务层以后再应用 WHERE 条件过滤。 Impossible WHERE : where 子句的值总是 false，不能用来获取任何数据。 distinct : 查找 distinct 值，当 MySQL 找到了第一条匹配的结果时，将停止该值的查询，转为后面其他值查询。 SQL 执行顺序想要优化 SQL，必须清楚知道 SQL 的执行顺序，这样再配合 explain 才能事半功倍！ 完整 SQL 语句： 1234567891011121314select distinct &lt;select_list&gt;from &lt;left_table&gt; &lt;join_type&gt;join &lt;right_table&gt; on &lt;join_condition&gt;where &lt;where_condition&gt;group by &lt;group_by_list&gt;having &lt;having_condition&gt;order by &lt;order_by_condition&gt;limit &lt;limit number&gt; SQL 执行顺序： 1、from &lt;left_table&gt;&lt;join_type&gt;2、on &lt;join_condition&gt;3、&lt;join_type&gt; join &lt;right_table&gt;4、where &lt;where_condition&gt;5、group by &lt;group_by_list&gt;6、having &lt;having_condition&gt;7、select8、distinct &lt;select_list&gt;9、order by &lt;order_by_condition&gt;10、limit &lt;limit_number&gt; 总结我们使用 EXPLAIN 解析 SQL 执行计划时，如果有下面几种情况，就需要特别关注下了： 首先看下 type 这列的结果，如果有类型是 ALL 时，表示预计会进行全表扫描（full table scan）。通常全表扫描的代价是比较大的，建议创建适当的索引，通过索引检索避免全表扫描。 再来看下 Extra 列的结果，如果有出现 Using temporary 或者 Using filesort 则要多加关注：Using temporary，表示需要创建临时表以满足需求，通常是因为 GROUP BY 的列没有索引，或者 GROUP BY 和 ORDER BY 的列不一样，也需要创建临时表，建议添加适当的索引；Using filesort，表示无法利用索引完成排序，也有可能是因为多表连接时，排序字段不是驱动表中的字段，因此也没办法利用索引完成排序，建议添加适当的索引；Using where，通常是因为全表扫描或全索引扫描时（type 列显示为 ALL 或 index），又加上了 WHERE 条件，建议添加适当的索引；其他状态例如：Using index、Using index condition、Using index for group-by 则都还好，不用紧张。 参考博文[1]. MySQL 的索引是什么？怎么优化？[2]. EXPLAIN Output Format[3]. MySQL 快速生成 100W 条测试数据[4]. MySQL EXPLAIN 详解[5]. MySQL 高级 之 explain 执行计划详解 MySQL 进阶讲解系列 Git 在团队中的最佳实践（一）：Git 备忘清单 MySQL 进阶讲解（二）：快速生成测试数据以及 EXPLAIN 详解]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>索引</tag>
        <tag>EXPLAIN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis 深度探险（三）：Redis 单机环境搭建以及配置说明]]></title>
    <url>%2Farchives%2F14348371.html</url>
    <content type="text"><![CDATA[前言经过 Redis 深度探险系列的学习相信大家对 Redis 的数据结构、对象、持久化机制、过期键删除策略等知识有了大致的了解，本篇博文主要讲述 Redis 的安装步骤，然后介绍一下 Redis 配置说明，最后对 Redis 集群搭建进行详细的讲解。 单机环境搭建Mac 安装 RedisMac 中使用 brew 安装 Redis 的方法 1$ brew install redis Linux 安装 RedisUbuntu 中使用 apt-get 安装 Redis 的方法 1$ sudo apt-get install redis-server CentOS7 中使用 yum 安装 Redis 的方法 检查 Redis 版本 12// 通过 yum info redis 可知 Centos7 中 Redis 源的版本为3.2.12$ yum info redis 安装 Redis 数据库 1$ yum install -y redis 启动 Redis 数据库 1$ systemctl start redis 查看 Redis 运行状况 1$ systemctl status redis 检测 Redis 服务器是否开启 1$ ps -ef | grep redis 通过上述方法安装的 Redis 版本较低，如果想安装最新的 Redis 请执行如下命令 1234// 安装 Remi 的软件源$ yum install -y http://rpms.famillecollet.com/enterprise/remi-release-7.rpm// 更换 yum 源，通过 remi 源安装 Redis$ yum --enablerepo=remi install -y redis Redis 简单配置以及配置说明Rdis设置为开启启动1$ systemctl enable redis.service 开启远程连接，Redis 默认只能 localhost 访问修改 /etc/redis.conf 配置文件，执行 systemctl restart redis 重启 Redis 即生效。 输入命令 vim /etc/redis.conf 进入编辑模式 bind 127.0.0.1 修改为 bind 0.0.0.0 protected-mode yes 修改为 protected-mode no（保护模式，是否只允许本地访问） 设置密码修改 /etc/redis.conf 配置文件，在 requirepass foobared 前面去掉注释，将 foobared 改为自己的密码，我在这里改为 requirepass foobared，执行 systemctl restart redis 重启 Redis 即生效。 在线变更配置12345678// 通过 redis-cli 连接 redis 客户端，使用 AUTH 进行认证127.0.0.1:6379&gt; auth foobared// 获取当前配置127.0.0.1:6379&gt; CONFIG GET *// 修改密码为 root127.0.0.1:6379&gt; CONFIG SET requirepass root// 将修改后的配置写入配置文件127.0.0.1:6379&gt; CONFIG REWRITE 配置文件详解 Redis 配置文件 redis.conf 以及Redis 配置文件 redis.conf 中文详解： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749750751752753754755756757758759760761762763764765766767768769770771772773774775776777778779780781782783784785786787788789790791792793794795796797798799800801802803804805806807808809810811812813814815816817818819820821822823824825826827828829830831832833834835836837838839840841842843844845846847848849850851852853854855856857858859860861862863864865866867868869870871872873874875876877878879880881882883884885886887888889890891892893894895896897898899900901902903904905906907908909910911912913914915916917918919920921922923924925926927928929930931932933934935936937938939940941942943944945946947948949950951952953954955956957958959960961962963964965966967968969970971972973974975976977978979980981982983984985986987988989990# Redis 配置文件例子.## 注意: 为了能读取到配置文件, Redis 服务必须以配置文件的路径作为第一个参数启动:## ./redis-server /path/to/redis.conf# 单位说明: 当需要指定内存大小时, 可能会使用到不同的单位, 如 1k、5GB、4M 等, 这里给出其单位含义:## 1k =&gt; 1000 bytes# 1kb =&gt; 1024 bytes# 1m =&gt; 1000000 bytes# 1mb =&gt; 1024*1024 bytes# 1g =&gt; 1000000000 bytes# 1gb =&gt; 1024*1024*1024 bytes## 单位是大小写不敏感的 所以 1GB 1Gb 1gB 是一样的.################################## INCLUDES #################################### 如果你拥有一个标准的配置模板, 并且希望在该模板之上做一些个性化的修改, 你可以使用 include 指令来引入其他的配置文件.## 注意:"include" 不会被 admin 或者 Redis Sentinel "CONFIG REWRITE" 命令覆盖.# 由于 redis 以最终的配置作为实际配置, 因此我们希望你将 include 命令放置在配置文件的最前面以防配置被覆盖.# # 如果你打算使用另外的 conf 文件来覆盖当前文件的配置, 那么最好将 include 指令放置到该文件的末尾. # 即最后生效原则, 最后被解析的配置将作为最后的配置.## include /path/to/local.conf# include /path/to/other.conf################################## MODULES ###################################### 启动时加载模块. 如果服务器无法加载模块, 它将中止. 可以使用多个 loadmodule 指令.## loadmodule /path/to/my_module.so# loadmodule /path/to/other_module.so################################## NETWORK ###################################### 默认情况下 redis 会在所有的可用网络接口中进行监听, 如果你想让 redis 在指定的网络接口中# 监听, 那么可以使用 bind 命令来指定 redis 的监听接口.## 例如:## bind 192.168.1.100 10.0.0.1# bind 127.0.0.1 ::1## ~~~ 警告 ~~~ 如果允许所有的网络接口访问 Redis, 这样做是很危险的, 如果你只是需要本机访问 # 可以指定特定的 127.0.0.1, 如果需要外网访问, 请配置防火墙策略.## IF YOU ARE SURE YOU WANT YOUR INSTANCE TO LISTEN TO ALL THE INTERFACES# JUST COMMENT THE FOLLOWING LINE.# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~bind 127.0.0.1# 是否开启保护模式, 默认开启. 要是配置里没有指定 bind 和密码.# 开启该参数后, redis 只会本地进行访问, 拒绝外部访问.# 要是开启了密码和 bind, 可以开启. 否则最好关闭, 设置为 no.protected-mode yes# 在指定的端口上进行监听, 默认是 6379.# 如果端口设置为 0, 那么 redis 就不会在 TCP socket 上进行监听.port 6379# TCP listen() backlog.## 在一个并发量高的环境中, 你需要指定一个比较大的 backlog 值来避免慢连接的情况.# 注意, linux 内核会默认 使用 / proc/sys/net/core/somaxconn 的值来削减 backlog 的实际值,# 因此你需要确保提升 somaxconn 和 tcp_max_syn_backlog 这两个值来确保此处的 backlog 生效.tcp-backlog 511# Unix socket.## 指定 unix sock 的路径来进行连接监听, 默认是不指定, 因此 redis 不会在 unix socket 上进行监听.## unixsocket /tmp/redis.sock# unixsocketperm 700# 关闭掉空闲 N 秒的连接 (0 则是不处理空闲连接)timeout 0# TCP keepalive.## 如果该值不为 0, 将使用 SO_KEEPALIVE 这一默认的做法来向客户端连接发送 TCP ACKs ## 这样的好处有以下两个原因:# 1) 检测已经死亡的对端 (译者注: TCP 的关闭会存在无法完成 4 次握手的情况, 如断电, 断网, 数据丢失等等)# 2) 保持连接在网络环境中的存活## 在 Linux 中, 该指定时间是一次发送 ACKs 的时间片.# 对于其他内核系统, 其时间片大小与内核配置有关.# 一个比较合理的值是 300 seconds.Redis 3.2.1 版本之后默认指定该值为 300 seconds.#tcp-keepalive 300################################# GENERAL ###################################### redis 默认不是以一个守护进程来运行的, 使用 yes, 可以让 redis 作为守护进程来运行.# 注意: 当 redis 作为守护进程的时候 /var/run/redis.pid 作为 pid 文件.daemonize no# 如果需要在机器启动 (upstart 模式 或 systemd 模式) 时就启动 Redis 服务器, 可以通过该选项来配置 Redis.# 支持的模式:# supervised no – 无, 不会与 supervised tree 进行交互# supervised upstart – 将 Redis 服务器添加到 SIGSTOP 模式中# supervised systemd – 将 READY=1 写入 $NOTIFY_SOCKET# supervised auto - 根据环境变量 UPSTART_JOB 或 NOTIFY_SOCKET 检测 upstart 还是 systemd# # 注意, 上述 supervision 方法 (upstart 或 systemd) 仅发出 “程序已就绪” 信号, 不会继续给 supervisor 返回 ping 回复.supervised no# 如果指定了 pid 文件, Redis 会在启动时写该 pid 文件, 在退出时删除该文件.# 当 Redis 服务器已守护进程启动时, 如果指定了配置文件, 则直接使用, 如果没有指定, 则创建 / var/run/redis.pid 作为配置文件.pidfile /var/run/redis_6379.pid# 指定日志的记录级别的.# 可以是如下的几个值之一:# debug (尽可能多的日志信息, 用于开发和测试之中)# verbose (少但是有用的信息, 没有 debug 级别那么混乱)# notice (适量的信息, 用于生产环境)# warning (只有非常重要和关键的信息会被记录)loglevel notice# 指定日志文件的位置. 为空时将输出到标准输出设备.# 如果你在 demo 模式下使用标准输出的日志, 日志将会输出到 /dev/null .logfile /var/log/redis/redis.log# 当设置'syslog-enabled'为 yes 时, 允许记录日志到系统日志中.# 以及你可以使用更多的日志参数来满足你的要求.# syslog-enabled no# 指定在系统日志中的身份.# syslog-ident redis# 指定系统日志的能力. 必须是 LOCAL0 到 LOCAL7 之间 (闭区间).# syslog-facility local0# 设置数据库的数量. 默认的数据库是 DB 0 使得你可以在每一个连接的基础之上使用 # SELECT &lt;dbid&gt; 来指定另外的数据库, 但是这个值必须在 0 到'database'-1 之间.databases 16# 默认情况下, Redis 只会在交互模式才显示 ASCII 版本的 logo, 可以通过设置此选项为 yes,# 来使 Redis 在任何情况下都显示 logo, 使得 Redis 的此行为和 4.0 之前的版本行为保持一致.#always-show-logo yes################################ SNAPSHOTTING ################################## 将 DB 数据保存到磁盘:## save &lt;seconds&gt; &lt;changes&gt;## 将会在 &lt; seconds&gt; 和 &lt;changes &gt; 两个值同时满足时, 将 DB 数据保存到硬盘中# 其中 &lt; seconds&gt; 每多少秒,&lt;changes &gt; 是改变的 key 的数量## 在以下的例子中, 将会存在如下的行为:# 当存在最少一个 key 变更时, 900 秒 (15 分钟) 后保存到硬盘# 当存在最少 10 个 key 变更时, 300 秒后保存到硬盘# 当存在最少 1000 个 key 变更时, 60 秒后保存到硬盘## 提示: 你可以禁用如下的所有 save 行.## 你可以删除所有的 save 然后设置成如下这样的情况.## save ""save 900 1save 300 10save 60 10000# 默认情况下, 在发生 RDB 快照或 BGSAVE 执行失败的那一刻, Redis 执行接收写请求.# 这会使用户察觉 (通常比较困难) 到数据没有正确的持久化到磁盘. 否则有可能出现不被察觉的灾难性后果.# # 当后台 BGSAVE 程序可以再次开始工作时, Reidis 会再次自动允许写入.# # 如果已经对 Server 和服务器持久化建立了正确的监控, 那么当你禁用该功能后, 即使磁盘、持久化等出现问题, Redis 也能继续提供服务.stop-writes-on-bgsave-error yes# 是否在 dump 到 rdb 数据库的时候使用 LZF(一种高效的压缩算法) 来压缩字符串.# 默认是 yes, 因为这是一个优良的做法.# 如果你不想耗费你的 CPU 处理能力, 你可以设置为 no, 但是这会导致你的数据会很大.rdbcompression yes# 从 RDB 的版本 5 开始, CRC64 校验值会写入到文件的末尾.# 这会使得格式化过程中, 使得文件的完整性更有保障,# 但是这会在保存和加载的时候损失不少的性能 (大概在 10%).# 你可以关闭这个功能来获得最高的性能.## RDB 文件会在校验功能关闭的时候, 使用 0 来作为校验值, 这将告诉加载代码来跳过校验步骤.rdbchecksum yes# DB 的文件名称.dbfilename dump.rdb# 工作目录.## DB 将会使用上述'dbfilename'指定的文件名写入到该目录中.# # 追加的文件也会在该目录中创建.# # 注意, 你应该在这里输入的是一个目录而不是一个文件名.dir /var/lib/redis################################# REPLICATION ################################## Redis 主从复制. 单机模式下, Redis 支持使用 slaveof 命令从另一个 Redis 服务器的拷贝中来创建一个实例.# 集群模式下则使用 cluster replicate &lt;master-id &gt; 命令. Redis 复制使用前须知:## +------------------+ +---------------+# | Master | ---&gt; | Replica |# | (receive writes) | | (exact copy) |# +------------------+ +---------------+## 1)Redis 复制是异步复制, 但是可以配置连接的从节点数量.# 2)当连接断开, Redis 从节点支持部分重同步 (psync) 功能来保证主从节点数据同步.# 3)复制过程是一个自动化过程, 无需人工干预. 当出现网络分区后, 从节点会自动尝试建立与主节点的连接, 并尝试同步.## replicaof &lt;masterip&gt; &lt;masterport&gt;# 如果主服务器开启了密码保护 (使用下面的 "requirepass" 配置).# 这个配置就是告诉从服务在发起向主服务器的异步复制的请求之前使用如下的密码进行认证,# 否则主服务器会拒绝这个请求.## masterauth &lt;master-password&gt;# 如果从服务器失去了和主服务器之间的连接, 或者当复制仍然处于处理状态的时候# 从服务器做出如下的两个行为:## 1) 如果 slave-serve-stale-data 被设置为 yes(默认值), 从服务器将会持续# 回复来自客户端的请求, 可能会回复已经过期的数据,# 或者返回空的数据, 当从服务器第一次异步请求数据时.## 2) 如果 slave-serve-stale-data 被设置为 no ,# 从服务器就会返回 "SYNC with master in progress"# 这个错误, 来应答所有命令除了 INFO 和 SLAVEOF.#replica-serve-stale-data yes# 你可以配置一个从服务器的实例是否接受写请求,# 从服务器在存储一些短暂的数据的的时候, 接收写请求是一件非常正确的事情# (因为数据在向主服务器同步之后非常容易擦除) 但是会因为配置不正确而导致一些问题.## 从 redis 2.6 开始默认从服务器是只读的服务器.## 提示: 只读的从服务器并不是设计用来公开给不受信任的互联网客户端的, 它# 仅仅是一个用来防止对实例进行误操作的保护层. 只读从服务器默认用来输出管理命令# 例如 CONFIG, DEBUG 和其他. 如果你想限制它的规模, 你可以使用'rename-command'来# 提高它的安全性, 使得她作为一个影子来执行管理或者危险的命令.replica-read-only yes# 是否使用 socket 方式复制数据. 目前 redis 复制提供两种方式, disk 和 socket.# # 对于新连接的 slaves 或断开重连的 slaves 将无法执行 “部分同步”, 需要进行一次完全同步. 当进行完全同步时, 主节点将传播一个 RDB 文件给从节点. 该 RDB 文件的传播方式有两种:# 1)基于磁盘: Redis 主节点创建一个新进程将 RDB 文件写到磁盘, 然后将生成的 RDB 文件传播给从节点.# 2)无磁盘: Redis 主节点创建一个新进程直接将 RDB 文件写到 slaves 的套接字中, RDB 文件无需落盘.# # 基于磁盘的复制, 一旦 RDB 文件生成, 多个 slaves 将排队等待并可以共享该文件. 而无磁盘复制一旦开始传输数据, 新 slaves 到来后将会排队等待.# # 在使用无磁盘复制时, 主节点在开始传输同步数据前将根据配置的时间进行等待, 从而实现多个从节点的并发传输.# # 在磁盘速度缓慢且网络速度很快 (高带宽) 时, 无磁盘复制效率更高. 默认情况下, 无磁盘复制同步关闭.repl-diskless-sync no# 无磁盘复制前, 主节点需要等待的时间. 该配置在启用无磁盘复制时将生效.# 由于一旦开启一次数据传输, 其余 slaves 将排队等待, 所以最好让主节点等待一段时间, 这样主节点就可对多个 slaves 并发传播数据.# 等待的单位是秒(second), 默认是 5 秒. 一旦将其设置为 0, 主节点将会马上开始数据传输.repl-diskless-sync-delay 5# slave 根据指定的时间间隔向服务器发送 ping 请求.# 时间间隔可以通过 repl_ping_slave_period 来设置, 默认 10 秒.## repl-ping-replica-period 10# 复制连接超时时间. master 和 slave 都有超时时间的设置.# master 检测到 slave 上次发送的时间超过 repl-timeout, 即认为 slave 离线, 清除该 slave 信息.# slave 检测到上次和 master 交互的时间超过 repl-timeout, 则认为 master 离线.# 需要注意的是 repl-timeout 需要设置一个比 repl-ping-slave-period 更大的值,# 不然会经常检测到超时.## 以下情境将使用到复制超时阈值:# 1) 从节点在执行 SYNC 期间, 检测块文件传输超时.# 2) 从节点检测主节点离线(data、pings).# 3) 主节点检测从节点离线(REPLCONF ACK).# # 必须要确保复制超时阈值 (repl-timeout) 大于 slaves 定时向 master 发送 PING 的时间片(repl-ping-slave-period),# 否则将总会检测到复制超时(当 slave 发送 PING 的时间片大于复制超时阈值时, slave 还未发送 ping 就会被定性为复制超时).# repl-timeout 60# 执行完 SYNC 后, 是否要禁用 TCP_NODELAY.## 当禁用该功能后, Redis 会使用占用更少带宽的小 TCP 包向从节点发送数据.# 但是这样做将会增大从节点端数据传输延时. 在 Linux 下禁用 TCP_NODELAY 功能将导致 40 微秒的延迟.# # 当启动该功能后, 在进行复制时将会减少数据传输延迟, 但是会占用更大的带宽.# # 默认情况下, 我们优先选择低延迟, 但是在高速网络或主从节点存在多 hops 路径时, 建议禁用 TCP_NODELAY 功能.repl-disable-tcp-nodelay no# 设置复制积压缓冲区 (replication backlog) 大小. 当 slaves 断开与节点连接后, Redis 使用复制积压缓冲区记录需要未发送给 slave 的数据.# 当从节点重连后, 仅需执行一次部分同步, 将从节点缺失数据补全.# # 复制积压缓冲区 (replication backlog) 越大, Redis 可以支持的 slave 离线时间就越长. 复制积压缓冲区用于部分重同步.# # 复制缓冲区只有在有 slave 连接时才分配内存. 没有 slave 时, 该内存会被释放出来, 默认大小为 1m.## repl-backlog-size 1mb# 当主节点不再有新连接的从节点后, 复制积压缓冲区将会被释放.# 为避免因从节点频繁掉线后上线而频繁的进行复制积压缓冲区的释放与申请, Redis 提供复制积压缓冲区释放时间片 (repl-backlog-ttl) 参数, 保证主节点在检测到从节点掉线后的规定时间内不会释放该缓冲区.# # 值为零时表示不会释放该复制积压缓冲区, 单位为秒.# 单位为秒, 配置如下:# repl-backlog-ttl 3600# 使用整数表示从节点优先级.# # 当主节点无法正常工作后, Sentinel 将使用该优先级在从节点中推选出新的主节点.# # 优先级对应的整数值越小, 被推选成主节点的可能性更大. 但是当优先级的值为零时表示该从节点不具备成为主节点的身份.# # 默认优先级为 100.slave-priority 100# 当主节点的已连接从节点数小于 N 且这些从节点延迟均大于 M 秒, 该主节点将停止接收写请求.# # 从节点处于 “online” 状态, 当且仅当延迟 (通过计算距离上一次接收从节点的 ping 消息的时间间隔获得) 小于指定的阈值.# 这个选项配置不是用来保证 N 个部分接收写信息, 而是为了在没有足够的从节点可用时, 限制写丢失.# # 如需要至少需要 3 个从节点并在 10s 内可用, 则设置:# # 延迟小于 min-slaves-max-lag 秒的 slave 才认为是健康的 slave.# min-slaves-to-write 3# 设置 1 或另一个设置为 0 禁用这个特性.# min-slaves-max-lag 10# # 一旦对这两个中的一个赋值为零, 则该功能失效.# 默认 min-slaves-to-write 参数设置为 0, 即该功能默认不启用.# Redis 主节点可以通过多种途径显示已连接从节点的 IP 和 port. 如 Sentinel 可以使用 “INFO replication” 命令来发现从节点实例；# Master 可以使用 “ROLE” 命令显示从节点 IP 和 port 信息等.# # slave 获取 IP 和 port 的方式是:# IP: 自动检测获取. 当从节点连接主节点时, 通过检查对应套接字地址获取.## Port: 从节点在复制中和主节点握手时需要使用到 port. 通常情况下, port 即为连接时的 port.# # 但是, 当发生端口转发(port forwarding, 转发一个网络端口从一个网络节点到另一个网络节点的行为)# 或使用 NAT(Network Address Translation, 网络地址转换)技术是, 从节点需要被分配不同 IP 和 port 后才能被访问.# 接下来的两个配置用来设置从节点的 IP 和 port, 用来告知主节点所指定的 IP 和 port, 这样 INFO 和 ROLE 才能继续返回结果.# # 当需要重写 IP 和 port 时, 则无需配置该选项.# slave-announce-ip 5.5.5.5# slave-announce-port 1234################################## SECURITY #################################### Server 在处理客户端命令前, 该客户端需要提供提供认证密码. 这在非可信网络环境中很有用.# # 为减少后台执行复杂度, 这个选项一般都会被注释掉. 因为大多数用户不需要授权.(如用户使用自己的服务器)# # Warning: 由于 Redis 执行高效, 所以外部用户每秒可以尝试认证 15w 次.# 也就是说, 为避免密码被快送攻破, 用户需要使用一个极其复杂的密码.## requirepass foobared# 重命名命令.# # 在一个共享环境中有必要对危险命令进行重命令, 从而避免危险命令的滥用、无用.# 如给 CONFIG 命令重新设置一个难以猜测的命令, 这样这个命令就很难被普通用户使用的, 但仍能被内部工具使用.# # 例如:# # rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52## 设置成一个空的值, 可以禁止一个命令.# rename-command CONFIG ""# # 注意, 一定要避免重命名那些写 AOF 文件或传输数据给 slaves 的命令, 否则将会导致各种难以预料的错误.################################### CLIENTS ##################################### 设置能连上 redis 的最大客户端连接数量. 默认是 10000 个客户端连接.# 由于 redis 不区分连接是客户端连接还是内部打开文件或者和 slave 连接等,# 所以 maxclients 最小建议设置到 32. 如果超过了 maxclients,# redis 会给新的连接发送’max number of clients reached’, 并关闭连接.## maxclients 10000############################## MEMORY MANAGEMENT ################################# redis 配置的最大内存容量. 不要再内存超过指定限制时仍然使用内存.# 当达到内存上限时, Redis 会根据选定的过期键策略移除一些 key.# # 如果根据过期键策略仍不能移除一些键或者过期键策略设置成 “noeviction”(不启用过期键策略),# 那么 Redis 会向如 SET、LPUSH 等使用内存的命令返回错误, 向诸如 GET 等读命令正常返回结果.# # 这个配置通常在将 Redis 当过 LRU 缓存或对以设置硬性的内存上限的 Redis 很适用.# # WARNING: slaves 的输出缓冲区不在主节点的 maxmemory 计算中, 所以设置的 maxmemory 不宜过大.# 如果过大, 可能导致主机的剩余内存过小, 从而不能预留足够的内存用于创建 slaves 的输出缓冲区.# # 简言之, 如果当前节点存在已连接的从节点, 建立设置一个较小的 maxmemory 上限, 这样系统就可以有多余的 RAM 用与创建从节点输出缓存.(当过期键策略设置成'noeviction'时, 则没有必要这么做)## maxmemory &lt;bytes&gt;# 内存容量超过 maxmemory 后的处理策略. Redis 提供五种内存淘汰策略:## volatile-lru -&gt; 利用 LRU 算法移除设置过过期时间的 key.# allkeys-lru -&gt; 利用 LRU 算法移除任何 key.# volatile-lfu -&gt; 利用 LFU 算法移除设置过过期时间的 key.# allkeys-lfu -&gt; 利用 LFU 算法移除任何 key.# volatile-random -&gt; 随机移除设置过过期时间的 key.# allkeys-random -&gt; 随机移除任何 key.# volatile-ttl -&gt; 移除即将过期的 key, 根据最近过期时间来删除 (辅以 TTL)# noeviction -&gt; 不移除任何 key, 只是返回一个写错误.# # 上面的这些驱逐策略, 如果 redis 没有合适的 key 驱逐, 对于写命令, 还是会返回错误.# redis 将不再接收写请求, 只接收 get 请求. 写命令包括: set setnx## maxmemory-policy noeviction# lru 检测的样本数. 使用 lru 或者 ttl 淘汰算法, 从需要淘汰的列表中随机选择 sample 个 key,# 选出闲置时间最长的 key 移除.# # 默认情况下, Redis 会从五个键中选择一个最近最久未使用的键进行淘汰. 可以通过配置该选择设置检测基数.# 一般情况下, 五个检测样本可以获得足够好的结果. 10 个样本的结果更接近 LRU 算法, 但是会消耗更多的 CPU.# 3 个样本可以获得较高的执行速度但是不够精确.## maxmemory-samples 5# 从 Redis 5 开始, 默认情况下, replica 节点会忽略 maxmemory 设置(除非在发生 failover 后, 此节点被提升为 master 节点).# 这意味着只有 master 才会执行过期删除策略, 并且 master 在删除键之后会对 replica 发送 DEL 命令.## 这个行为保证了 master 和 replicas 的一致性, 并且这通常也是你需要的, 但是若你的 replica 节点是可写的,# 或者你希望 replica 节点有不同的内存配置, 并且你确保所有到 replica 写操作都幂等的, 那么你可以修改这个默认的行为(请确保你明白你在做什么).## 需要注意的是默认情况喜爱 replica 节点不会执行过期策略, 它有可能使用了超过 maxmemory 设定的值的内存.# 因此你需要监控 replicas 节点所在的机器并且确保在 master 节点到达配置的 maxmemory 大小时, replicas 节点不会超过物理内存的大小.## replica-ignore-maxmemory yes############################# LAZY FREEING ##################################### Redis 有两种方式删除键. 一种是使用如 DEL 这样的命令进行的同步删除. # 同步删除意味着删除过程中服务端会停止处理新进来的命令. 若要删除的 key 关联了一个小的 object 删除耗时会很短.# 若要删除的 key 管理了一个很大的 object, 比如此对象有上百万个元素, 服务端会阻塞相同长一段时间(甚至超过一秒).## 由于以上原因, Redis 同时提供了一种非阻塞的方式用于删除,# 比如 UNLINK(非阻塞的 DEL)以及用于 FLUSHALL 和 FLUSHDB 的 ASYNC 选项, 这些命令能在后台回收内存.# 这些命令能在常数时间内执行完毕. 其他线程会在后台尽快回收内存.# # DEL,UNLINK 以及用于 FLUSHALL 和 FLUSHDB 的 ASYNC 选项是用户可以控制的. # 根据应用的设计, 用户可以选择使用阻塞或者非阻塞的方式. # 但是作为某些命令的副作用 Redis 服务端有时会删除某些 object 或者 flush 整个数据库. # 特别是以下独立于用户操作的情形: # 1. 由于 maxmemory 和 maxmemory policy 配置导致的内存回收动作 # 2. 由于过期, 当一个 key 过期后(可以查看 EXPIRE 命令获取相关信息), 必须回收其内存 # 3. 由于某些命令的副作用, 比如 STORE 命令, 执行 STORE 命令可能需要删除已有的键. SET 命令需要删除已有的旧内容.# 4. 在复制过程中, 当一个 replica 节点执行一个全量同步时, replica 需要删除整个数据库的内容以加载传输过来的 RDB 文件.## 在上述所有情形中, 删除 object 的默认行为都是以阻塞方式删除. 当然你可以配置上述四个选项来改变这种默认行为:lazyfree-lazy-eviction nolazyfree-lazy-expire nolazyfree-lazy-server-del noreplica-lazy-flush no############################## APPEND ONLY MODE ################################ 默认 redis 使用的是 rdb 方式持久化, 这种方式在许多应用中已经足够用了.# 但是 redis 如果中途宕机, 会导致可能有几分钟的数据丢失, 根据 save 来策略进行持久化,# Append Only File 是另一种持久化方式, 可以提供更好的持久化特性.# Redis 会把每次写入的数据在接收后都写入 appendonly.aof 文件,# 每次启动时 Redis 都会先把这个文件的数据读入内存里, 先忽略 RDB 文件.# # 默认情况下, Redis 会异步将数据集快照到磁盘上. 尽管这种模式对许多应用友好, 但是当 Redis 进程崩溃或发生掉电时,# 几分钟内的写信息将丢失(根据快照执行的粒度).# # AOF 作为一种可替换的持久化策略, 能够提供更好的耐久性.# 如使用默认的 fsync 策略, Redis 仅会丢失 1s 的写信息, 当发生突发事件(服务器掉电、服务器进程崩溃但 OS 运行正常).# # AOF 持久化和 RDB 持久化可以同时开启. 如果在启动 Redis 时已经存在 AOF 文件, 则会直接加载 AOF 文件(考虑到 AOF 文件相比 RDB 文件有更好的耐久性).# 关于 AOF 的更多讯息见: http://redis.io/topics/persistence appendonly no# 指定 AOF 文件名称, 默认是 appendonly.aof.appendfilename "appendonly.aof"# 调用 fsync() 系统函数用来告知 OS 将数据写入磁盘, 而不是在输出缓冲区等待数据. 有些 OS 将直接 flush 数据到磁盘, 有些其他的 OS 仅会尝试去 flush.# # Redis 支持三种 fsync() 调用模式:# no: 不执行 fsync, 由 OS 决定 flush 数据的频率. 高效. Linux 下默认是每 30s 执行一次 flush.# always: 每写入一次 AOF 就调用一次 fsync. 慢, 最安全.# everysec: 每秒调用一次 fsync, 可能会导致丢失这 1s 数据.# # 默认 fsync 的调用频率是 “everysec”, 这种策略在执行速度和数据安全进行折中.# 当可以容忍一定程度数据丢失并期望更高的性能时, 可以使用 “no” 策略(由操作系统决定 flush 的频率).# 相反的, 如果不能容忍数据丢失, 可以使用 “always” 获得更好的安全性, 尽管执行更慢.# # 在无法确定 fsync 调用频率时, 推荐使用 “everysec” 策略.# 在开启 AOF 持久化功能后, 该配置才会生效.# appendfsync alwaysappendfsync everysec# appendfsync no# 在 aof 重写或者写入 rdb 文件的时候, 会执行大量 IO, 此时对于 everysec 和 always 的 aof 模式来说,# 执行 fsync 会造成阻塞过长时间, no-appendfsync-on-rewrite 字段设置为默认设置为 no.# 如果对延迟要求很高的应用, 这个字段可以设置为 yes, 否则还是设置为 no,# 这样对持久化特性来说这是更安全的选择. 设置为 yes 表示 rewrite 期间对新写操作不 fsync,# 暂时存在内存中, 等 rewrite 完成后再写入, 默认为 no, 建议 yes.# Linux 的默认 fsync 策略是 30 秒. 可能丢失 30 秒数据.# 当 AOF 执行 fsync 的策略是 always 和 everysec 时, 如果此时有一个后台进程 (BGSAVE 进程或 AOF rewrite 进程) 正在执行大量的 I/O 操作到磁盘,# 在一些 Linux 系统中, 执行 fsync 会造成较长的阻塞. 当前对这种情况还没有很好的解决策略, 即使在不同的线程中执行 fsync 也会导致调用同步 write(2) 阻塞.# # 为了缓解上述问题, 可以通过配置下述选项来避免在主线程调用 fsync() 时执行 BGSAVE 或 BGREWRITEAOF 带来的阻塞.# # 也就是说, 默认情况下, 当子进程执行 BGSAVE 或 BGREWRITEAOF 时, Redis 的耐久性将默认转变成 "appendfsync none".# 在实际的应用中就意味着在最坏的场景下将丢失 30s 的数据, 即使配置了 fsync 调用频率为 always 或 everysec.(默认情况下, Linux 每 30s 自动调用一次 fsync 将缓存数据 flush 到磁盘)# # 如果当前应用已考虑延迟问题, 则将该配置设置成 “yes”. 否则使用默认配置(“no”), 这是从耐久性角度考虑的最安全的选择.no-appendfsync-on-rewrite no # aof 自动重写配置. 当目前 aof 文件大小超过上一次重写的 aof 文件大小的百分之多少进行重写,# 即当 aof 文件增长到一定大小的时候 Redis 能够调用 bgrewriteaof 对日志文件进行重写.# 当前 AOF 文件大小是上次日志重写得到 AOF 文件大小的二倍 (设置为 100) 时,# 自动启动新的日志重写过程.auto-aof-rewrite-percentage 100# 设置允许重写的最小 aof 文件大小, 避免了达到约定百分比但尺寸仍然很小的情况还要重写auto-aof-rewrite-min-size 64mb# 设置 AOF 重写的触发条件.# 当 AOF 日志按照指定的比例增长时, 可以通过调用 BGREWRITEAOF 执行自动的 AOF rewrite.# # 工作原理: Redis 通过对比上一次执行 rewrite 时 AOF 文件的大小与当前 AOF 文件大小(在重启时将没有上一次执行 rewrite 的记录, 这时将使用 startup 时的 AOF 文件大小), 决定是否进行 rewrite.# 如果当前 AOF 对于上一次执行 rewrite 的 AOF 文件的增长比率大于指定的比率, 将会触发一次 rewrite.# 当然, 还需指定一个 AOF 进行重写的最小单位. 这样做可以避免增长比率已经达到要求, 但对应的 AOF 仍很小的情况 (这种情况下没有必要进行 rewrite) 的发生.# # 如果想要关闭自动 AOF rewrite 功能, 可将进行 rewrite 要求的增长比率设置 0.# # 默认当 AOF 大于 64MB 且相比于上一次 rewrite,AOF 以扩充了两倍时会触发一次 rewrite 执行. 默认配置如下:auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb# aof 文件可能在尾部是不完整的, 当 redis 启动的时候, aof 文件的数据被载入内存.# 重启可能发生在 redis 所在的主机操作系统宕机后,# 尤其在 ext4 文件系统没有加上 data=ordered 选项 (redis 宕机或者异常终止不会造成尾部不完整现象.)# 出现这种现象, 可以选择让 redis 退出, 或者导入尽可能多的数据. 如果选择的是 yes,# 当截断的 aof 文件被导入的时候, 会自动发布一个 log 给客户端然后 load.# 如果是 no, 用户必须手动 redis-check-aof 修复 AOF 文件才可以.aof-load-truncated yes# 在将 AOF 文件加载到内存时(重启 Redis), 可能会出现 AOF 被截断的情况.# # 如当 Redis 运行所在系统突然崩溃(当 ext4 文件系统在安装时没有配置成数据按序存储), 会出现 AOF 被截断情况.# 如果 Redis 程序发生崩溃或异常, 但操作系统仍能正常工作, 则不会出现 AOF 被截断的情况.# # 出现 AOF 被截断后, Redis 要么直接退出并返回错误, 要么加载被截断 AOF 中尽可能多的数据(当前默认方式).# 可以通过 aof-load-truncated 选项进行配置:# 当 aof-load-truncated 设置成 “yes”,Redis 仍会加载一个被截断的 AOF 文件, 同时向用户报告 AOF 文件被截断.# 如果设置成 “no”,Redis 会直接返回错误并拒绝启动, 这时用户需要使用 "redis-check-aof" 程序修复 AOF, 只有这样才能重启 Server.# # 注意, 如果 AOF 文件在执行一半时就出现问题, 即使设置 aof-load-truncated 为 “yes”,Redis 也会直接退出并返回错误.# 这个配置仅在 Redis 尝试从 AOF 文件读更多数据但发现没有足够字计数存在时有意义.aof-load-truncated yes# 当重写 AOF 文件时, Redis 可以使用 RDB 文件作为 AOF 文件的前导, 这样可以更快地进行重写和恢复. # 当启用这个功能时 AOF 文件由两部分组成:## [RDB file][AOF tail]## 当加载 AOF 文件时, Redis 通过以 “REDIS” 字符串开头的 AOF 文件识别出此文件是由 RDB 和 AOF 组合而成的,# Redis 会先加载 RDB 部分, 然后再加载 AOF 部分.aof-use-rdb-preamble yes################################ LUA SCRIPTING ################################ 设置 Lua 脚本执超时的时间上限, 单位是毫秒, milliseconds.## 当 Lua 脚本执行超时, Redis 会记录脚本执行之后的结果 (超时后) 并向查询返回错误.# 当一个长时脚本执行时间超过最大执行时间时, 只有 SCRIPT KILL 和 SHUTDOWN NOSAVE 命令可用.# 停止这类脚本运行的第一个方法是调用一个非写命令. 第二种方法是 shut down 这个 server, 如果已经发送了一个写命令但用户并不想等待脚本自然终止.# # 如果想不限制脚本的执行时间并且不需要返回 warning, 可以将该参数设置成 0 或负数.lua-time-limit 5000################################ REDIS CLUSTER ################################ 正常情况下, 启动的 Redis 实例为非集群模式. 只有当节点配置成集群模式时才能成为集群节点.# 如需以集群模式启动, 取消下述配置的注释即可:## cluster-enabled yes# 集群配置文件的名称, 每个节点都有一个集群相关的配置文件, 持久化保存集群的信息.# 这个文件并不需要手动配置, 这个配置文件有 Redis 生成并更新,# 每个 Redis 集群节点需要一个单独的配置文件, 请确保与实例运行的系统中配置文件名称不冲突.## cluster-config-file nodes-6379.conf# 集群节点超时阈值用来作为节点不可达并被标记为失效状态的超时上限, 单位是毫秒(millisecond)# 大部分其他内部时间将其基本参考数.## cluster-node-timeout 15000# 在进行故障转移的时候, 全部 slave 都会请求申请为 master,# 但是有些 slave 可能与 master 断开连接一段时间了, 导致数据过于陈旧,# 这样的 slave 不应该被提升为 master. 该参数就是用来判断 slave 节点与 master 断线的时间是否过长.# # 没有简单的方式可以直接准确判定从节点的”data age”, 可以通过下面两个方面的检测实现:# # 1) 如果有多个从节点可以发起故障转移, 可以让他们交换信息以选出数据状态最节点主节点的从节点.# 从节点可以通过 offset 进行排名并通过该排名延迟发起故障转移的时机.# # 2) 每个独立的从节点计算最近一次与主节点交互的时间.# 这里的交互可以是最近一次 PING、最近一次接收到来自主节点的命令、与主节点断开连接的时间(当复制链接已经 down 掉时).# 如果最近一次与主节点的交互已经足够久远, 那么这个从节点将放弃进行故障转移.# # 在 2) 中的时间阈值可以由用户设定. 特别地, 当从节点的最近一次与主节点的交互远大于 (node-timeout * slave-validity-factor) + repl-ping-slave-period 时, 这个从节点将不会执行故障转移.# # 例如假设 node-timeout 为 30 秒, slave-validity-factor 参数为 10,repl-ping-slave-period 为 10 秒, 当从节点距离最近一次与主节点的交互时间大于 310 秒 (30*10+10) 时, 该从节点不能进行故障转移.# # 一个过大的从节点有效因子 (slave-validity-factor) 会允许存储过旧数据的从节点进行故障转移, 而一个过小的从节点有效因子将会妨碍集群选择从节点成为新的主节点.# 所以合理的设置从节点有效因子很重要.# # 为了获得最大的可用性, 可以将从节点有效因子 (slave-validity-factor) 赋值为 0.# 也就是说, 从节点忽略距离最近一次与主节点交互的时间段, 则是直接点尝试发起故障转移.(但是这种策略下, 这些从节点仍会根据 offset 的排名来推迟发起故障转移的时间)# # 从节点有效因子 (slave-validity-factor) 值为 0 是唯一可以保证网络分区消失后, 集群仍继续工作的值.## cluster-replica-validity-factor 10# Redis 集群支持将从节点迁移到孤立主节点(orphaned masters), 没有可以工作从节点的主节点.# 该功能减少了集群孤立主节点故障但没有可工作从节点进行故障转移的情况的发生.# # 从节点可以迁移到孤立主节点当且仅当原来的主节点的剩余可工作从节点个数大于等于指定的可工作从节点数.# 这个数称为从节点迁移屏蔽因子(migration barrier).# 当迁移屏蔽因子设置为 1 时, 当且仅当主节点拥有至少两个可工作的从节点才允许其中从节点迁移到孤立主节点(orphaned masters).# 该因子通常用来表明使用者需要为集群中的主节点配置从节点个数.# # 默认迁移屏蔽因子是 1(从节点可以执行迁移当前仅当其主节点在该节点迁移后仍保有至少一个从节点).# 如果想关闭该功能, 只需将该参数设置成一个极大值即可.# 允许将迁移屏蔽因子置零. 这种行为仅在调试时有用, 且在生产环境中存在极大风险.# cluster-migration-barrier 1# 默认情况下, Redis 集群将停止接收客户端请求 (停止服务) 当集群检测到存在哈希槽没有对应负责的节点.# 也就是说, 如果集群部分 down(如有一部分哈希槽没有对应的节点), 整个集群最终将会不可用.(集群信息传播遵循最终一致性)# 当所有的槽都再次有对应负责的节点后, 集群将会自动再次可用.# # 但是有时希望即使集群只有部分槽有对应的节点, 集群也能继续接受客户端请求并处理对应的键空间.# 为了达到上述目的, 将 ecluster-require-full-coverage 设置为 “no” 即可.# cluster-require-full-coverage yes# 这个选项用于控制 master 发生故障时是否自动进行 failover.## 当设置为 yes 后 master 发生故障时不会自动进行 failover, 这时你可以进行手动的 failover 操作.## cluster-replica-no-failover no# In order to setup your cluster make sure to read the documentation# available at http://redis.io web site.########################## CLUSTER DOCKER/NAT support ######################### 在某些部署环境下, Redis 集群的节点地址不能被自动发现, 这是因为这些节点是部署在 NAT 网络或者端口是转发的 (典型的情况就是使用了 Docker 或者其他容器).## 为了能让 Redis 集群工作在这种环境下, 我们需要进行相关配置让各个节点知道相互之间的外部地址,# 这可以通过设置以下选项做到:## * cluster-announce-ip: 表示节点的外部地址# * cluster-announce-port: 表示节点的客户端口# * cluster-announce-bus-port: 表示集群消息总线端口## Each instruct the node about its address, client port, and cluster message# bus port. The information is then published in the header of the bus packets# so that other nodes will be able to correctly map the address of the node# publishing the information.## 若以上选项未配置, 则将会启用正常的 Redis 集群自动检测机制. # 若 bus port 未设置, 则会将其设置为 port + 10000.## Example:## cluster-announce-ip 10.1.1.5# cluster-announce-port 6379# cluster-announce-bus-port 6380################################## SLOW LOG #################################### Redis 慢日志系统用来记录执行时间较长的查询.# 这里的执行时间 (“execution time”) 不包括 IO 操作时间, 如接收客户端的请求, 返回请求结果等,# 而是实际执行命令的时间(此时线程处于阻塞状态, 仅能执行该命令, 不能同时处理其他请求).# # 可以使用两个参数配置慢日志: 一个参数告知 Redis 执行时间超时阈值(单位是微秒, microseconds), 这样一旦某个执行时间超过指定上限, 将会被记录到慢日志中；# 另一个参数是慢日志的长度. 慢日志使用环式结构存储超时命令.(当慢日志满后, 新命令添加进去后, 最老的命令将被踢出)# # 单位是微秒(microsecon). 注意, 负数时间会禁用慢查询日志, 而 0 则会强制记录所有命令.# 默认慢日志功能是开启的.slowlog-log-slower-than 10000# 慢查询日志长度. 当一个新的命令被写进日志的时候, 最老的那个记录会被删掉.# 这个长度没有限制. 只要有足够的内存就行. 你可以通过 SLOWLOG RESET 来释放内存.slowlog-max-len 128################################ LATENCY MONITOR ############################### Redis 延迟监测自系统通过对执行期间的操作的检测来收集与延迟相关的数据.# # 通过使用 LATENCY 命令, Redis 用户可以获得延迟相关的图形、报告等信息.# 延迟系统只会记录大于等于设置的 latency-monitor-threshold 值的操作. 当该值为零时,# 则表明关闭 latency monitor.## 默认情况下, latency monitor 功能是关闭的, 因为大多数场景下并不需要该功能.# latency monitor 可以在 Redis 运行时通过 "CONFIG SET latency-monitor-threshold &lt;milliseconds&gt;" 启动.latency-monitor-threshold 0############################# EVENT NOTIFICATION ############################### Redis 可以通知那些已 Pub/Sub 客户端键空间发生的事件.# # 例如, 如果开启键空间通知功能且一个客户端对 0 号数据库上的 “foo”key 执行 DEL 操作, 那么 Redis 将使用 Pub/Sub 发送两条消息:# PUBLISH __keyspace@0__:foo del# PUBLISH __keyevent@0__:del foo# # Redis 对通知的事件进行了分类, 每一类都使用唯一的字符标记:# K 键空间 (Key) 通知, 前缀为:__keyspace@&lt;db&gt;__ # E 键事件 (Event) 通知, 前缀为:__keyevent@&lt;db&gt;__ 对于所有命令类型和非键事件, 均使用小写字母表示, 且没有前缀.# g 一般命令(Generic commands), 如 DEL, EXPIRE, RENAME 等# $ 字符串 (String) 命令# l 列表 (list) 命令# s 集合 (set) 命令# h 哈希 (hash) 命令# z 有序集合 (sorted set) 命令# x 过期事件(过期键产生的事件)# e 驱逐事件(因 maxmemory 而驱逐的事件)# A g$lshzxe 等类型的别称(Alias), 如此一来就可以使用 "AKE" 代表所有的事件类型## notify-keyspace-events 可以指定多个字符组成的字符串或空串. 其中空串代表关闭通知功能.# # 例 1: 为了开启 List 事件和 Genetic 事件(从事件名称分类来说), 可以使用如下设置:# notify-keyspace-events Elg# # 例 2: 为了获取过期键的信息并发送到订阅的频道, 即__keyevent@0__:expired use 信息, 可设置如下:# notify-keyspace-events Ex# # 默认情况下, 事件通知功能是关闭的, 因为大多数用户并不需要这个功能且这个功能会带来额外的性能开销.# 注意, 如果没有指定键空间 (K) 通知还是键事件 (E) 通知, 那么任何事件通知都不会被传送.notify-keyspace-events ""############################### ADVANCED CONFIG ################################ Hash 数据类型的底层实现有压缩链表 (ziplist) 和哈希表(hash).# 当且仅当存储的数据量小于 hash-max-ziplist-entries 且节点占用的容量小于 hash-max-ziplist-value 时才使用小数据量存储高效的 ziplist 结构存储.# 否则, 使用哈希结构存储.hash-max-ziplist-entries 512hash-max-ziplist-value 64# List 类型也可以通过特殊的方式来节省空间.# 每个内部 list 节点允许存储的 entries 数量可以指定为已修订最大数量或最大元素数.# 如指定 - 5 到 - 1, 其含义是:# -5: max size: 64 Kb &lt;-- 对于普通的工作负载, 不建议使用# -4: max size: 32 Kb &lt;-- 不建议使用# -3: max size: 16 Kb &lt;-- 有时不建议使用# -2: max size: 8 Kb &lt;-- good# -1: max size: 4 Kb &lt;-- good# 整数代表每个 list 节点准确存储指定数量的 elements# 最高效的参数设置是 - 2 (8 Kb size) 或 -1 (4 Kb size)# 但是, 如果需求很特殊, 则应根据需要调整参数:list-max-ziplist-size -2# List 可以实现压缩.## 压缩深度是划定 quicklist、ziplist 等 list 在压缩时的节点范围.# 为了进行快速的 push/pop 操作, 不会对 list 的 head 和 tail 进行压缩, 只会对中间节点进行压缩.# # 参数设置如下:# 0: 关闭 list 压缩功能# 1: 深度为 1 表示只有当 list 添加一个节点 (无论从 head 还是 tail 添加该节点) 后才开始进行压缩.# 所以对于 [head]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[tail]# 只有黑体部分加入才会执行压缩操作.# 2: 深度为 2# 对于链表:[head]-&gt;[next]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[prev]-&gt;[tail]# 不会压缩 head 或 head-&gt;next 或 tail-&gt;prev 或 tail, 而仅压缩剩余部分.# 3: 深度为 3# [head]-&gt;[next]-&gt;[next]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[prev]-&gt;[prev]-&gt;[tail]list-compress-depth 0# Set 数据类型的底层实现默认是 intSet, 也可以是 hash.# Set 使用 hash 编码格式当且仅当字符串组成的 set 变成底数为 10 的整数, 且其值范围在 64 位整数中.# 下面的配置设置用来指定 set 可以使用特殊编码格式的阈值:set-max-intset-entries 512# Sorted Set 默认使用 ziplist 实现, 也可通过 skiplist 编码实现来节省空间.# 当且仅当 Sorted Set 中元素值大于 zset-max-ziplist-value、元素数量大于 zset-max-ziplist-entries 时, 才使用 skiplist 实现 Sorted Set.zset-max-ziplist-entries 128zset-max-ziplist-value 64# value 大小小于等于 hll-sparse-max-bytes 使用稀疏数据结构 (sparse),# 大于 hll-sparse-max-bytes 使用稠密的数据结构 (dense).# 一个比 16000 大的 value 是几乎没用的, 建议的 value 大概为 3000.# 如果对 CPU 要求不高, 对空间要求较高的, 建议设置到 10000 左右.hll-sparse-max-bytes 3000# HyperLogLog 稀疏表示阈值.# 16 位的 header 部分也在 limit 中. 当使用稀疏表示的 HyperLogLog 存储的字节超过了指定的阈值, 它将转变成稠密表示.# 不建议使用大于 16000 的值. 当小于 16000 时能够获得较高存储效率.# 建议的值是 3000, 该值可以在较少 PFADD(在执行稀疏编码时时间复杂度是 O(N))操作执行的同时获得极高空间使用收益. 当 CPU 问题无需考虑时, 可将该值提升到 10000, 但其值不应超过 15000.hll-sparse-max-bytes 3000# Streams macro node max size / items. The stream data structure is a radix# tree of big nodes that encode multiple items inside. Using this configuration# it is possible to configure how big a single node can be in bytes, and the# maximum number of items it may contain before switching to a new node when# appending new stream entries. If any of the following settings are set to# zero, the limit is ignored, so for instance it is possible to set just a# max entires limit by setting max-bytes to 0 and max-entries to the desired# value.# 用于设定 Streams 单个节点的最大大小和最多能保存多个个元素.stream-node-max-bytes 4096stream-node-max-entries 100# 激活的 rehash 会占用每 100 毫秒的 1 millisecond 的 CPU 时间来对 Redis hash table(存储数据库的键值对的 hash table)执行 rehash.# 在 Redis 中 hash table 使用惰性 rehash: 在 rehashing 时, hash table 中执行的操作越多, rehash 执行的步骤也越多.# 所以当 server 很空闲时, rehash 将很简单, hash table 也会有更多的内存可以使用.# # 为了在条件许可的情况下对 hash table 进行 rehash, 从而节省内存空间, 默认情况下, rehash 功能会每 100 毫秒中 1 毫秒被调用一次.# # 如果不确定:# 使用 "activerehashing no" 如果当前应用环境很注重延迟、Redis 仅允许 2 毫秒的延迟应答.# 使用 "activerehashing yes" 如果当前应用环境不是太注重延迟且想要尽可能块的释放内存空间.activerehashing yes# 客户端输出缓冲区可以用来强制断开那些不能够快速读取服务器数据的客户端连接.(如在 Pub/Sub 模式中, 客户端不能够快速的处理 publisher 发送过来的消息)# 客户端类型可以细分为三类:# normal -&gt; 普通客户端(包括 MONITOR 客户端)# slave -&gt; 从节点客户端# pubsub -&gt; 订阅至少一个 pubsub 通道或模式的客户端# # client-output-buffer-limit 设置的通用格式如下:# client-output-buffer-limit &lt;class&gt; &lt;hard limit&gt; &lt;soft limit&gt; &lt;soft seconds&gt;# # 一旦 hard limit 达到, 客户端将直接断开连接. 如果 soft limit 达到, 客户端连接将会持续 soft seconds 后才断开连接.# # 例如, 当 hard limit 是 32 MB(megabytes)、soft limit 在 10 秒内持续超过 16MB,# 如果客户端输出缓冲区 (clients output buffer) 超过 32 MB 或客户客户端输出缓冲区 (clients output buffer) 超过 16MB, 并在接下来的 10 秒都高于 16MB, 客户端连接将马上断开.# # 默认情况下, 不需对 normal 级别的 clients 进行约束因为这些客户端如果没有发起询问就不会接受数据.# 所以, 只需对异步客户端进行约束因为异步客户端会出现请求速度大于 read 速度的情况.# # 因为订阅者和从节点使用推的方式接受数据, 所以需要对 pubsub 客户端和 slave 客户端设置默认的客户端输出缓冲区约束.# 将 hard limit 或 soft limit 置零表示关闭对应的功能.client-output-buffer-limit normal 0 0 0client-output-buffer-limit slave 256mb 64mb 60client-output-buffer-limit pubsub 32mb 8mb 60# 客户端查询缓冲区会累加新的命令. # 默认情况下, 他们会限制在一个固定的数量避免协议同步失效 (比如客户端的 bug) 导致查询缓冲区出现未绑定的内存.# 但是, 如果有类似于巨大的 multi/exec 请求的时候可以修改这个值以满足你的特殊需求.## client-query-buffer-limit 1gb# 在 Redis 协议中, 批量请求通常限制在 512 mb 内, 可以通过修改 proto-max-bulk-len 选项改变这个限制.## proto-max-bulk-len 512mb# Redis 调用一个内部函数来执行后台任务, 如在 timeout 时关闭客户端连接, 清除从未被请求的过期键, 等等.# # 虽然并不是所有的 tasks 都使用同样的频率执行, 但是 Redis 会根据指定的频率值来检测 tasks 的执行.# # 默认设置的值为 10, 即每秒执行 10 次.# 在 Redis 处于空闲提升该值时, 将会消耗更多的 CPU.# 但是, 提升该值也会使 Redis 更精确的处理超时问题, 并检测到更多的过期键.# # 该值设置范围是 1 到 500. 但是不建议将其设置大于 100.# 大多数的用户建议使用默认的值(10), 并根据应用环境的低延迟需求适当提升该值(峰值建议不要大于 100).hz 10# 通常来说根据连接上来的客户端数量对 HZ 的值按比例进行调整是有用的.# 这很有用, 例如, 为了避免每次后台任务处理太多的客户端, 从而避免高延迟峰值.## 默认情况下 HZ 的值为 10, 启用 dynamic-hz 后, 当有大量客户端连接进来时 HZ 的值会临时性地调高.## 启用 dynamic-hz 后, HZ 的配置值将作为基线, 当有大量的客户端连接进来时, Redis 会将 HZ 的实际值设置为 HZ 的配置值的整数倍.# 通过这种方式, 空闲的 Redis 实例只会占用非常小的 CPU 时间, 当实例变得繁忙时 Redis 能更快地进行响应(相对未启用 dynamic-hz 的情况).dynamic-hz yes# 当子进程进行 AOF 的重写时, 如果启用了 aof-rewrite-incremental-fsync, 子进程会每生成 32 MB 数据就进行一次 fsync 操作.# 通过这种方式将数据分批提交到硬盘可以避免高延迟峰值.aof-rewrite-incremental-fsync yes# 当 Redis 保存 RDB 文件时, 如果启用了 rdb-save-incremental-fsync 功能, Redis 会每生成 32 MB 数据就执行一次 fsync 操作. # 通过这种方式将数据分批提交到硬盘可以避免高延迟峰值.rdb-save-incremental-fsync yes# Redis LFU eviction (see maxmemory setting) can be tuned. However it is a good# idea to start with the default settings and only change them after investigating# how to improve the performances and how the keys LFU change over time, which# is possible to inspect via the OBJECT FREQ command.## There are two tunable parameters in the Redis LFU implementation: the# counter logarithm factor and the counter decay time. It is important to# understand what the two parameters mean before changing them.## The LFU counter is just 8 bits per key, it's maximum value is 255, so Redis# uses a probabilistic increment with logarithmic behavior. Given the value# of the old counter, when a key is accessed, the counter is incremented in# this way:## 1. A random number R between 0 and 1 is extracted.# 2. A probability P is calculated as 1/(old_value*lfu_log_factor+1).# 3. The counter is incremented only if R &lt; P.## The default lfu-log-factor is 10. This is a table of how the frequency# counter changes with a different number of accesses with different# logarithmic factors:## +--------+------------+------------+------------+------------+------------+# | factor | 100 hits | 1000 hits | 100K hits | 1M hits | 10M hits |# +--------+------------+------------+------------+------------+------------+# | 0 | 104 | 255 | 255 | 255 | 255 |# +--------+------------+------------+------------+------------+------------+# | 1 | 18 | 49 | 255 | 255 | 255 |# +--------+------------+------------+------------+------------+------------+# | 10 | 10 | 18 | 142 | 255 | 255 |# +--------+------------+------------+------------+------------+------------+# | 100 | 8 | 11 | 49 | 143 | 255 |# +--------+------------+------------+------------+------------+------------+## NOTE: The above table was obtained by running the following commands:## redis-benchmark -n 1000000 incr foo# redis-cli object freq foo## NOTE 2: The counter initial value is 5 in order to give new objects a chance# to accumulate hits.## The counter decay time is the time, in minutes, that must elapse in order# for the key counter to be divided by two (or decremented if it has a value# less &lt;= 10).## The default value for the lfu-decay-time is 1. A Special value of 0 means to# decay the counter every time it happens to be scanned.## lfu-log-factor 10# lfu-decay-time 1########################### ACTIVE DEFRAGMENTATION ######################### 警告: 这个功能是实验性的. 当然此功能已经在包括生产环境在内的环境中通过压力测试. # 并且被多名工程师手工测过一段时间.## What is active defragmentation?# -------------------------------## 活动碎片整理允许 Redis 服务器压缩内存中由于申请和释放数据块导致的碎片, 从而回收内存. # 碎片是每次申请内存 (幸运的是 Jemalloc 出现碎片的几率小很多) 的时候会自然发生的. # 通常来说, 为了降低碎片化程度需要重启服务, 或者至少需要清除所有的数据然后重新创建. # 得益于 Oran Agra 在 Redis 4.0 实现的这个特性, 进程可以在服务运行时以 “热” 方式完成这些目的.## 通常来说当碎片化达到一定程度(查看下面的配置)Redis 会使用 Jemalloc 的特性创建连续的内存空间, # 并在此内存空间对现有的值进行拷贝, 拷贝完成后会释放掉旧的数据. # 这个过程会对所有的导致碎片化的 key 以增量的形式进行.## 需要重点理解的是: # 1. 这个特性默认是关闭的, 并且只有在编译 Redis 时使用我们代码中的 Jemalloc 版本才生效.(这是 Linux 下的默认行为)# 2. 如果没有碎片问题, 你永远不需要启用这项特性 # 3. 如果你需要试验这项特性, 可以通过命令 CONFIG SET activefrag yes 来启用## 相关的配置参数可以很好的调整碎片整理过程. 如果你不知道这些选项的作用最好使用默认值.# 启用碎片整理.# activedefrag yes# 有至少多少碎片时才开始碎片整理.# active-defrag-ignore-bytes 100mb# 有至少多少比例的碎片时才开始碎片整理.# active-defrag-threshold-lower 10# 有多少比例的碎片时才开始以最大努力进行碎片整理.# active-defrag-threshold-upper 100# 进行碎片整理时至少使用多少比例的 CPU 时间.# active-defrag-cycle-min 5# 最大努力进行碎片整理时使用多少 CPU 时间.# active-defrag-cycle-max 75# 进行主字典扫描时处理的 set/hash/zset/list 字段的最大数量# (就是说在进行主字典扫描时 set/hash/zset/list 的长度小于这个值才会处理, 大于这个值的会放在一个列表中延迟处理).# active-defrag-max-scan-fields 1000 参考博文[1]. Redis 配置文件 redis.conf [2]. Redis 命令[3]. 阿里云 Redis 开发规范 Redis 深度探险系列 Redis 深度探险（一）：那些绕不过去的 Redis 知识点 Redis 深度探险（二）：Redis 深入之道 Redis 深度探险（三）：Redis 单机环境搭建以及配置说明 Redis 深度探险（四）：Redis 高可用性解决方案之哨兵与集群]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>安装教程</tag>
        <tag>Redis</tag>
        <tag>配置说明</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git 在团队中的最佳实践（二）：如何正确使用 Git flow 工作流]]></title>
    <url>%2Farchives%2Fc0dca125.html</url>
    <content type="text"><![CDATA[前言当在团队开发中使用版本控制系统时，商定一个统一的工作流程是至关重要的。Git 的确可以在各个方面做很多事情，然而，如果在你的团队中还没有能形成一个特定有效的工作流程，那么混乱就将是不可避免的。基本上你可以定义一个完全适合你自己项目的工作流程，或者使用一个别人定义好的。就像代码需要代码规范一样，代码管理同样需要一个清晰的流程和规范。 Git flow 工作流是经典模型，体现了工作流的经验和精髓。随着项目过程复杂化，会感受到这个工作流中深思熟虑和威力！Git flow 工作流没有用超出功能分支工作流的概念和命令，而是为不同的分支分配一个很明确的角色，并定义分支之间如何和什么时候进行交互。在这章节中我们将一起学习一个当前非常流行的工作流 Git flow。 Git flow 工作介绍版本管理的挑战虽然有这么优秀的版本管理工具，但是我们面对版本管理的时候，依然有非常大得挑战，我们都知道大家工作在同一个仓库上，那么彼此的代码协作必然带来很多问题和挑战，如下： 如何开始一个 Feature 的开发，而不影响别的 Feature？ 由于很容易创建新分支，分支多了如何管理，时间久了，如何知道每个分支是干什么的？ 哪些分支已经合并回了主干？ 如何进行 Release 的管理？开始一个 Release 的时候如何冻结 Feature, 如何在 Prepare Release 的时候，开发人员可以继续开发新的功能？ 线上代码出 Bug 了，如何快速修复？而且修复的代码要包含到开发人员的分支以及下一个 Release? 大部分开发人员现在使用 Git 就只是用三个甚至两个分支，一个是 Master, 一个是 Develop, 还有一个是基于 Develop 打得各种分支。这个在小项目规模的时候还勉强可以支撑，因为很多人做项目就只有一个 Release, 但是人员一多，而且项目周期一长就会出现各种问题。 Git flow 流程荷兰程序员 Vincent Driessen 曾发表了一篇博客 A Successful Git Branching Model，让一个分支策略广为人知。 下面是 Git flow 的流程图： 这一流程最大的亮点是考虑了紧急 Bug 的应对措施，整个流程显得过于复杂，所以在实施该方案前，需要对整个开发流程进行系统的学习。也需要借助 Git flow 等工具的辅助。 Git flow 安装以及初始化工具 Git-flow 是按照 Vincent Driessen 的 branch 模型，实现的一个高层次（级别）的 git 仓库操作扩展集合。 Linux 安装 Git flowUbuntu 中使用 apt-get 安装 Git flow 的方法 1$ sudo apt-get install git-flow CentOS7 中使用 wget 安装 Git flow 的方法 1$ wget --no-check-certificate -q https://raw.githubusercontent.com/petervanderdoes/gitflow-avh/develop/contrib/gitflow-installer.sh &amp;&amp; sudo bash gitflow-installer.sh install develop; rm gitflow-installer.sh Mac 安装 Git flowMac 中使用 brew 安装 Git flow 的方法 1$ brew install git-flow-avh Windows 安装 Git flowWindows 中使用 wget 安装 Git flow 的方法 1$ wget -q -O - --no-check-certificate https://raw.github.com/petervanderdoes/gitflow-avh/develop/contrib/gitflow-installer.sh install stable | bash Git flow 初始化回答几个关于分支的命名约定的问题，建议使用默认值。 1234567891011121314$ git flow initNo branches exist yet. Base branches must be created now.Branch name for production releases: [master] Branch name for "next release" development: [develop] How to name your supporting branch prefixes?Feature branches? [feature/] Bugfix branches? [bugfix/] Release branches? [release/] Hotfix branches? [hotfix/] Support branches? [support/] Version tag prefix? [v] Hooks and filters directory? [/home/ubuntu/study/.git/hooks] Git flow 分支模型的介绍Git flow 工作流仍然用中央仓库作为所有开发者的交互中心。和其它的工作流一样，开发者在本地工作并 push 分支到要中央仓库中。 Master 分支 - 生产分支最为稳定功能比较完整的随时可发布的代码，即代码开发完成，经过测试，没有明显的 bug，才能合并到 master 中。请注意永远不要在 master 分支上直接开发和提交代码，以确保 master 上的代码一直可用。 Develop 分支 - 开发分支用作平时开发的主分支，并一直存在，永远是功能最新最全的分支，包含所有要发布到下一个 release 的代码，主要用于合并其他分支，比如 feature 分支；如果修改代码，新建 feature 分支修改完再合并到 develop 分支。所有的 feature、release 分支都是从 develop 分支上拉的。 Feature 分支 - 功能分支这个分支主要是用来开发新的功能，一旦开发完成，通过测试没问题（这个测试，测试新功能没问题），我们合并回 develop 分支进入下一个 release。 Release 分支 - 发布分支用于发布准备的专门分支。当开发进行到一定程度，或者说快到了既定的发布日，可以发布时，建立一个 release 分支并指定版本号（可以在 finish 的时候添加）。开发人员可以对 release 分支上的代码进行集中测试和修改 bug。（这个测试，测试新功能与已有的功能是否有冲突，兼容性）全部完成经过测试没有问题后，将 release 分支上的代码合并到 master 分支和 develop 分支。 Hotfix 分支 - 热修复分支用于修复线上代码的 bug。从 master 分支上拉，完成 hotfix 后，打上 tag 我们合并回 master 和 develop 分支。 功能分支 - Feature 分支 功能分支：通常为即将发布或者未来发布版开发新的功能，这通常只存在开发者的库中。当新功能开始研发，包含该功能的发布版本在这个还是无法确定发布时间的。功能版本的实质是只要这个功能处于开发状态它就会存在，但是最终会或合并到 develop 分支（确定将新功能添加到不久的发布版中）或取消（譬如一次令人失望的测试）。 分支命名规则：分支名称以 feature/* 开头 使用 Git 命令开发功能分支 - Feature 分支流程： 1234567891011121314151617181920212223242526# 1. 开始一项功能的开发工作时, 基于'develop'创建分支$ git checkout -b feature/some-feature develop# 2. 推送到远程 -&gt; 可选$ git push -u origin feature/some-feature# 3. 在'feature/some-feature'功能分支上开发完成后, 提交至仓库$ git commit -a -m "some feature"# 4. 切换到'develop'分支$ git checkout develop# 5. 从远程仓库拉去最新'develop'$ git pull origin develop# 6. 'develop'分支合并'feature'功能分支 $ git merge --no-ff feature/some-feature# 7. 推送'develop'分支至远程仓库 $ git push origin develop# 8. 删除'feature'功能分支$ git branch -d feature/some-feature# 9. 删除远程'feature'功能分支$ git push origin --delete feature/some-feature –no-ff 标志导致合并操作创建一个新 commit 对象，即使该合并操作可以 fast-forward。这避免了丢失这个功能分支存在的历史信息，将该功能的所有提交组合在一起。 使用 Git-flow 命令开发功能分支 - Feature 分支流程： 123456789101112# 1. 增加新特性 P.S.[创建了一个基于'develop'的功能分支'some-feature', 并切换到这个分支之下]$ git flow feature start some-feature# 2. 完成新特性 P.S.[1. 合并'feature/some-feature'分支到'develop';2. 删除这个新特性分支; 3. 切换回'develop'分支] Summary of actions: - The feature branch 'feature/some-feature' was merged into 'develop' - Feature branch 'feature/some-feature' has been locally deleted - You are now on branch 'develop'$ git flow feature finsh --no-ff some-feature# 3. 推送'develop'分支至远程仓库 $ git push origin develop 12345678# 1. 发布新特性 P.S.[发布新特性分支到远程服务器, 所以, 其它用户也可以使用这分支]$ git flow feature publish some-feature# 2. 取得一个发布的新特性分支 P.S.[取得其它用户发布的新特性分支, 并签出远程的变更]$ git flow feature pull some-feature# 3. 跟踪在 origin 上的特性分支]$ git flow feature track some-feature 热修复分支 - Hotfix 分支 热修复分支：热修复分支与发布分支很相似，他们都为新的生成环境发布做准备，尽管这是未经计划的。他们来自生产环境的处于异常状态压力。当生成环境验证缺陷必须马上修复是，热修复分支可以基于 master 分支上对应与线上版本的 tag 创建。 分支命名规则：分支名称以 hotfix/* 开头 使用 Git 命令开发热修复分支 - Hotfix 分支流程： 1234567891011121314151617181920212223242526272829303132# 1. 基于'master'创建热修复分支$ git checkout -b hotfix/some-hotfix-0.1.1 master# 2. 在'hotfix/some-hotfix-0.1.1'热修复分支上完成紧急修复, 提交至仓库$ git commit -a -m "some hotfix"# 3. 切换到'master'分支$ git checkout master# 4. 'master'分支合并'hotfix/some-hotfix-0.1.1'热修复分支 $ git merge --no-ff hotfix/some-hotfix-0.1.1# 5. 推送'master'分支至远程仓库 $ git push origin master# 6. 切换到'master'分支$ git checkout develop# 7. 'develop'分支合并'hotfix/some-hotfix-0.1.1'功能分支 $ git merge --no-ff hotfix/some-hotfix-0.1.1# 7. 推送'develop'分支至远程仓库 $ git push origin develop# 8. 删除'hotfix/some-hotfix-0.1.1'热修复分支 $ git branch -d hotfix/some-hotfix-0.1.1# 9. 重新打标签 $ git tag -a v0.1.1 master# 10. 提交所有 tag$ git push --tags 使用 Git-flow 命令开发热修复分支 - Hotfix 分支流程： 1234567891011121314151617181920212223# 1. 开始 git flow 紧急修复 P.S.[创建了一个基于'master'的热修复分支, 并切换到这个分支之下]$ git flow hotfix start 0.1.1# 2. 完成新特性 P.S.[1. 合并'hotfix/0.1.1'分支到'master';2. 给'master 打标签'v0.1.1';3. 合并'v0.1.1'分支到'develop';4. 删除本地'hotfix/v0.1.1'分支; 5. 切换回'develop'分支]$ git flow hotfix finish --no-ff 0.1.1 Summary of actions: - Hotfix branch 'hotfix/0.1.1' has been merged into 'master' - The hotfix was tagged 'v0.1.1' - Hotfix tag 'v0.1.1' has been back-merged into 'develop' - Hotfix branch 'hotfix/0.1.1' has been locally deleted - You are now on branch 'develop'# 3. 切换到'master'分支$ git checkout master# 4. 推送'master'分支至远程仓库 $ git push origin master# 5. 切换到'develop'分支$ git checkout develop# 6. 推送'develop'分支至远程仓库 $ git push origin develop 发布分支 - Release 分支发布分支：Release 分支是为新产品的发布做准备的，它允许我们在最后时刻做一些细小的修改，它们允许小 bugs 的修改和准备发布元数据（版本号，开发时间等等）。Release 分支基于 develop 分支创建； 一旦创建了 release 分支，不能在从 develop 分支合并新的改动到 release 分支，可以基于 release 分支进行测试和 bug 修改，测试不用再另外创建用于测试的分支。 分支命名规则：分支名称以 release/* 开头 使用 Git 命令开发发布分支 - Release 分支流程： 1234567891011121314151617181920212223242526272829# 1. 基于'develop'创建发布分支, 在此分支上小 bugs 的修改和准备发布元数据$ git checkout -b release/some-release-0.1.1 develop# 2. 切换到'master'分支$ git checkout master# 3. 'master'分支合并'release/some-release-0.1.1'热修复分支 $ git merge --no-ff release/some-release-0.1.1# 4. 推送'master'发布分支至远程仓库 $ git push origin master# 5. 切换到'master'分支$ git checkout develop# 6. 'develop'开发分支合并'release/some-release-0.1.1'功能分支 $ git merge --no-ff release/some-release-0.1.1# 7. 推送'develop'开发分支至远程仓库 $ git push origin develop# 8. 删除'release/some-release-0.1.1'发布分支 $ git branch -d release/some-release-0.1.1# 9. 重新打标签 $ git tag -a v0.1.1 master# 10. 提交所有 tag$ git push --tags 使用 Git-flow 命令开发发布分支 - Release 分支流程： 1234567891011121314151617181920212223# 1. 开始准备 release 版本 P.S.[创建了一个基于'develop'的热修复分支, 并切换到这个分支之下]$ git flow release start 0.1.1# 2. 完成 release 版本 P.S.[1. 合并'release/0.1.1'分支到'master';2. 给'master 打标签'v0.1.1';3. 合并'v0.1.1'分支到'develop';4. 删除本地'release/v0.1.1'分支; 5. 切换回'develop'分支]$ git flow release finish 0.1.1 Summary of actions: - Release branch 'release/0.1.1' has been merged into 'master' - The release was tagged 'v0.1.1' - Release tag 'v0.1.1' has been back-merged into 'develop' - Release branch 'release/0.1.1' has been locally deleted - You are now on branch 'develop' # 3. 切换到'master'分支$ git checkout master# 4. 推送'master'分支至远程仓库 $ git push origin master# 5. 切换到'develop'分支$ git checkout develop# 6. 推送'develop'分支至远程仓库 $ git push origin develop Git 的分支工作流与 Pull RequestPull request 是 github/bitbucket 给开发人员实现便利合作提供的一个 feature。他们提供一个用户友好的 web 界面在进代码之前来讨论这些变更。 简单说，Pull request 是一种为了开发人员通知 team member 他们已经完成了一个 feature 的机制。一旦他们的 feature branch ready 了，开发人员就通过他们的 github 帐号执行一个 pull request。这将使得每个相干人知晓这个事件，他们需要 review 这个 feature branch 的代码，并且需要决定是否 merge 到 master 分支上去。 但是 pull request 并不仅仅是一种 notification, 他也是一个专门用于讨论这些即将落地代码的细节的论坛。如果有任何问题或意见，同事们可以在 pull request 中提 comments，甚至直接在这个 Pull request 中修改要落地的代码。所有这些活动都由 pull request 来跟踪。 下面通过例子介绍：热修复分支 - Hotfix 分支 Git flow 工作流和 Pull Request 12345678910111213141516171819# 1. 'master'创建'hotfix'分支并发布'hotfix'分支到远程服务器$ git flow hotfix start [hotfix version No.] [release version No.]$ git flow hotfix publish [hotfix version No.] # 2. 'developer'更新代码, 切换到'hotfix'分支$ git pull$ git checkout hotfix/[hotfix version No.]# 3. 'developer'修复代码, 然后提交到远程仓库, 申请 pull request 从 hotfixbug-[hotfix version No.]/[bug name] 到 hotfix/[hotfix version No.]$ git checkout –b hotfixbug-[hotfix version No.]/[bug name]$ // todo fix bug$ git push –u origin hotfixbug-[hotfix version No.]/[bug name]$ // create pull request to hotfix/[hotfix version No.]# 4. 'master'处理 merge 后, 测试人员跟进测试; 无误后, 申请 pull request 从 hotfix/[hotfix version No.] 到 master$ // create pull request to master# 5. 'master'merge to master. 针对项目维护者: git pull 和 git merge 是最常用的 merge Pull Requests 的方式, 在命令行下 merge 之后, PullRequest 也会相应地自动关闭; 或者在网站上点击同意合并$ git flow hotfix finish --no-ff [hotfix version No.] 参考博文[1]. 「译」浅谈 Gitflow[2]. git-flow 备忘清单[3]. 图解 Git Git 在团队中的最佳实践系列 Git 在团队中的最佳实践（一）：Git 备忘清单 Git 在团队中的最佳实践（二）：如何正确使用 Git flow 工作流 Git 在团队中的最佳实践（三）：如何优雅的使用 Git？]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git flow</tag>
        <tag>工作流</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 并发编程之美（五）：揭开 InheritableThreadLocal 的面纱]]></title>
    <url>%2Farchives%2Ff30530aa.html</url>
    <content type="text"><![CDATA[前言博客《Java 并发编程之美（四）：深入剖析 ThreadLocal》提到 ThreadLocal 变量的基本使用方式，ThreadLocal 是一个本地线程副本变量工具类，主要用于将私有线程和该线程存放的副本对象做一个映射，各个线程之间的变量互不干扰，在高并发场景下，可以实现无状态的调用，特别适用于各个线程依赖不同的变量值完成操作的场景。但是在实际的开发中，有这样的一种需求：父线程生成的变量需要传递到子线程中进行使用，那么在使用 ThreadLocal 似乎就解决不了这个问题。由于 ThreadLocal 设计之初就是为了绑定当前线程，如果希望当前线程的 ThreadLocal 能够被子线程使用，实现方式就会相当困难。在此背景下，InheritableThreadLocal 应运而生，使用 InheritableThreadLocal 这个变量就可以轻松的在子线程中依旧使用父线程中的本地变量。 ThreadLocal 与 InheritableThreadLocal 区别ThreadLocal 声明的变量是线程私有的成员变量，每个线程都有该变量的副本，线程对变量的修改对其他线程不可见。 InheritableThreadLocal 声明的变量同样是线程私有的，但是子线程可以使用同样的 InheritableThreadLocal 类型变量从父线程继承 InheritableThreadLocal 声明的变量，父线程无法拿到其子线程的。即使可以继承，但是子线程对变量的修改对父线程也是不可见的。 对 InheritableThreadLocal 的理解InheritableThreadLocal 类是 ThreadLocal 类的子类。ThreadLocal 中每个线程拥有它自己的值，与 ThreadLocal 不同的是，InheritableThreadLocal 允许一个线程以及该线程创建的所有子线程都可以访问它保存的值。 ThreadLocal 是不支持继承性的，所谓继承性也是针对父线程和子线程来说，代码示例： 12345678910111213class Scratch &#123;public static final ThreadLocal&lt;String&gt; localVariable = new ThreadLocal&lt;&gt;(); public static void main(String[] args) &#123; localVariable.set("I'm variable in main"); System.out.println(Thread.currentThread().getName() + ":" + localVariable.get()); new Thread(() -&gt; System.out.println(Thread.currentThread().getName() + ":" + localVariable.get())).start(); &#125;&#125;// Output// main:I'm variable in main// Thread-0:null InheritableThreadLocal 用于子线程能够拿到父线程往 ThreadLocal 里设置的值，代码示例： 123456789101112131415class Scratch &#123; public static final ThreadLocal&lt;String&gt; localVariable = new InheritableThreadLocal&lt;&gt;(); public static void main(String[] args) &#123; localVariable.set("I'm variable in main"); System.out.println(Thread.currentThread().getName() + ":" + localVariable.get()); new Thread(() -&gt; System.out.println(Thread.currentThread().getName() + ":" + localVariable.get())).start(); &#125;&#125;// Output// main:I'm variable in main// Thread-0:I'm variable in main 深入解析 InheritableThreadLocal 类InheritableThreadLocal 类重写了 ThreadLocal 的 3 个函数： 123456789101112131415161718192021222324252627public class InheritableThreadLocal&lt;T&gt; extends ThreadLocal&lt;T&gt; &#123; /** * 该函数在父线程创建子线程，向子线程赋值 InheritableThreadLocal 变量时使用 * 可重写 childValue() 方法实现子线程与父线程之间互不影响 */ protected T childValue(T parentValue) &#123; return parentValue; &#125; /** * 由于重写了 getMap，操作 InheritableThreadLocal 时， * 将只影响 Thread 类中的 inheritableThreadLocals 变量， * 与 threadLocals 变量不再有关系 */ ThreadLocalMap getMap(Thread t) &#123; return t.inheritableThreadLocals; &#125; /** * 类似于 getMap，操作 InheritableThreadLocal 时， * 将只影响 Thread 类中的 inheritableThreadLocals 变量， * 与 threadLocals 变量不再有关系 */ void createMap(Thread t, T firstValue) &#123; t.inheritableThreadLocals = new ThreadLocalMap(this, firstValue); &#125;&#125; InheritableThreadLocal 赋值从源码上看，跟 ThreadLocal 不一样的无非是 ThreadLocalMap 的引用不一样了，从逻辑上来讲，这并不能做到子线程得到父线程里的值。那么秘密在那里呢？通过跟踪 Thread 的构造方法，你能够发现是在构造 Thread 对象的时候对父线程的 InheritableThreadLocal 进行了赋值。下面是 Thread 的部分源码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778public class Thread implements Runnable &#123; /** * 默认人构造方法, 会调用 init 方法进行初使化 */ public Thread() &#123; init(null, null, "Thread-" + nextThreadNum(), 0); &#125; /** * 最终初始化线程的方法 */ private void init(ThreadGroup g, Runnable target, String name, long stackSize, AccessControlContext acc, boolean inheritThreadLocals) &#123; if (name == null) &#123; throw new NullPointerException("name cannot be null"); &#125; this.name = name; // parent 为当前线程, 也就是调用了 new Thread(); 方法的线程 Thread parent = currentThread(); SecurityManager security = System.getSecurityManager(); if (g == null) &#123; /* Determine if it's an applet or not */ /* If there is a security manager, ask the security manager what to do. */ if (security != null) &#123; g = security.getThreadGroup(); &#125; /* If the security doesn't have a strong opinion of the matter use the parent thread group. */ if (g == null) &#123; g = parent.getThreadGroup(); &#125; &#125; /* checkAccess regardless of whether or not threadgroup is explicitly passed in. */ g.checkAccess(); /* * Do we have the required permissions? */ if (security != null) &#123; if (isCCLOverridden(getClass())) &#123; security.checkPermission(SUBCLASS_IMPLEMENTATION_PERMISSION); &#125; &#125; g.addUnstarted(); // 在这里会继承父线程是否为后台线程的属性还有父线程的优先级 this.group = g; this.daemon = parent.isDaemon(); this.priority = parent.getPriority(); if (security == null || isCCLOverridden(parent.getClass())) this.contextClassLoader = parent.getContextClassLoader(); else this.contextClassLoader = parent.contextClassLoader; this.inheritedAccessControlContext = acc != null ? acc : AccessController.getContext(); this.target = target; setPriority(priority); // 这里是重点, 当父线程的 inheritableThreadLocals 不为空的时候, 会调用 ThreadLocal.createInheritedMap 方法, 传入的是父线程的 inheritableThreadLocals。原来复制变量的秘密在这里 if (inheritThreadLocals &amp;&amp; parent.inheritableThreadLocals != null) this.inheritableThreadLocals = ThreadLocal.createInheritedMap(parent.inheritableThreadLocals); /* Stash the specified stack size in case the VM cares */ this.stackSize = stackSize; /* Set thread ID */ tid = nextThreadID(); &#125;&#125; 通过跟踪 Thread 的构造方法，我们发现只要父线程在构造子线程（调用 new Thread()）的时候 inheritableThreadLocals 变量不为空。新生成的子线程会通过 ThreadLocal.createInheritedMap 方法将父线程 inheritableThreadLocals 变量有的对象复制到子线程的 inheritableThreadLocals 变量上。这样就完成了线程间变量的继承与传递。 ThreadLocal.createInheritedMap123456789101112131415161718192021222324252627282930313233343536373839404142public class ThreadLocal&lt;T&gt; &#123; /** * 根据传入的 map, 构造一个新的 ThreadLocalMap */ static ThreadLocalMap createInheritedMap(ThreadLocalMap parentMap) &#123; return new ThreadLocalMap(parentMap); &#125; static class ThreadLocalMap &#123; // 这个 private 的构造方法就是专门给 ThreadLocal 使用的 private ThreadLocalMap(ThreadLocalMap parentMap) &#123; // ThreadLocalMap 还是用 Entry 数组来存储对象的 Entry[] parentTable = parentMap.table; int len = parentTable.length; setThreshold(len); // 创建跟父线程相同大小的 table table = new Entry[len]; // 这里是复制 parentMap 数据的逻辑 for (int j = 0; j &lt; len; j++) &#123; Entry e = parentTable[j]; if (e != null) &#123; // 得到父线程中变量对应的 key, 即 ThreadLocal 对象 ThreadLocal key = e.get(); if (key != null) &#123; // 此处会调用 InheritableThreadLocal 重写的方法, 默认直接返回入参值 Object value = key.childValue(e.value); Entry c = new Entry(key, value); // 通过位与运算找到索引位置 int h = key.threadLocalHashCode &amp; (len - 1); // 如果该索引位置已经被占, 则寻找下一个索引位置 while (table[h] != null) h = nextIndex(h, len); // 将 Entry 放在对应的位置 table[h] = c; size++; &#125; &#125; &#125; &#125; &#125;&#125; InheritableThreadLocal 和线程池搭配使用存在的问题问题展示代码示例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253class Scratch &#123; private static final ThreadLocal&lt;String&gt; localVariable = new InheritableThreadLocal&lt;&gt;(); public static void main(String[] args) throws InterruptedException &#123; ExecutorService executorService = Executors.newFixedThreadPool(2); CountDownLatch doneSignal = new CountDownLatch(2); // 1.main 线程第一次赋值 "I'm variable_1 in main"localVariable.set("I'm variable_1 in main"); System.out.println(Thread.currentThread().getName() + ":" + localVariable.get()); // 2. 线程池执行方法, 查看线程中线程 InheritableThreadLocal 赋值情况 executorService.submit(new Worker(doneSignal)); executorService.submit(new Worker(doneSignal)); // 3.wait for all to finish[等待线程 pool-1-thread-1/pool-1-thread-2 执行完后, 在对主线程的 InheritableThreadLocal 进行赋值, 查看赋值后, 线程池中线程的 InheritableThreadLocal 是否发生变法] doneSignal.await(); // 4.main 线程第二次赋值 "I'm variable_2 in main"localVariable.set("I'm variable_2 in main"); System.out.println(Thread.currentThread().getName() + ":" + localVariable.get()); // 5. 线程池执行方法, 查看线程中线程 InheritableThreadLocal 赋值情况 executorService.submit(new Worker(doneSignal)); executorService.submit(new Worker(doneSignal)); // 6. 关闭线程池 executorService.shutdown(); &#125; static class Worker implements Runnable &#123; private final CountDownLatch doneSignal; Worker(CountDownLatch doneSignal) &#123; this.doneSignal = doneSignal; &#125; @Override public void run() &#123; doneSignal.countDown(); System.out.println(Thread.currentThread().getName() + ":" + localVariable.get()); &#125; &#125;&#125;// Output// main:I'm variable_1 in main// pool-1-thread-2:I'm variable_1 in main// pool-1-thread-1:I'm variable_1 in main// main:I'm variable_2 in main// pool-1-thread-2:I'm variable_1 in main// pool-1-thread-1:I'm variable_1 in main 前后两次调用获取的值是一开始赋值的值，因为线程池中是缓存使用过的线程，当线程被重复调用的时候并没有再重新初始化 init() 线程，而是直接使用已经创建过的线程，所以这里的值并不会被再次操作。因为实际的项目中线程池的使用频率非常高，每一次从线程池中取出线程不能够直接使用之前缓存的变量，所以要解决这一个问题，网上大部分是推荐使用 alibaba 的开源项目 transmittable-thread-local。 transmittable-thread-localJDK 的 InheritableThreadLocal 类可以完成父线程到子线程的值传递。但对于使用线程池等会池化复用线程的组件的情况，线程由线程池创建好，并且线程是池化起来反复使用的；这时父子线程关系的 ThreadLocal 值传递已经没有意义，应用需要的实际上是把任务提交给线程池时的 ThreadLocal 值传递到任务执行时。 在 ThreadLocal 的需求场景即是 TTL（装饰器模式）的潜在需求场景，如果你的业务需要『在使用线程池等会池化复用线程的组件情况下传递 ThreadLocal』则是 TTL 目标场景。下面是几个典型场景例子：1、分布式跟踪系统；2、日志收集记录系统上下文；3 应用容器或上层框架跨应用代码给下层 SDK 传递信息。 12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;transmittable-thread-local&lt;/artifactId&gt; &lt;version&gt;2.10.2&lt;/version&gt;&lt;/dependency&gt; 代码示例： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455class Scratch &#123; private static final ThreadLocal&lt;String&gt; localVariable = new TransmittableThreadLocal&lt;&gt;(); public static void main(String[] args) throws InterruptedException &#123; ExecutorService executorService = Executors.newFixedThreadPool(2); // 额外的处理，生成修饰了的对象 executorService executorService = TtlExecutors.getTtlExecutorService(executorService); CountDownLatch doneSignal = new CountDownLatch(2); // 1.main 线程第一次赋值 "I'm variable_1 in main"localVariable.set("I'm variable_1 in main"); System.out.println(Thread.currentThread().getName() + ":" + localVariable.get()); // 2. 线程池执行方法, 查看线程中线程 InheritableThreadLocal 赋值情况 executorService.submit(new Worker(doneSignal)); executorService.submit(new Worker(doneSignal)); // 3.wait for all to finish[等待线程 pool-1-thread-1/pool-1-thread-2 执行完后, 在对主线程的 InheritableThreadLocal 进行赋值, 查看赋值后, 线程池中线程的 InheritableThreadLocal 是否发生变法] doneSignal.await(); // 4.main 线程第二次赋值 "I'm variable_2 in main"localVariable.set("I'm variable_2 in main"); System.out.println(Thread.currentThread().getName() + ":" + localVariable.get()); // 5. 线程池执行方法, 查看线程中线程 InheritableThreadLocal 赋值情况 executorService.submit(new Worker(doneSignal)); executorService.submit(new Worker(doneSignal)); // 6. 关闭线程池 executorService.shutdown(); &#125; static class Worker implements Runnable &#123; private final CountDownLatch doneSignal; Worker(CountDownLatch doneSignal) &#123; this.doneSignal = doneSignal; &#125; @Override public void run() &#123; doneSignal.countDown(); System.out.println(Thread.currentThread().getName() + ":" + localVariable.get()); &#125; &#125;&#125;// Output// main:I'm variable_1 in main// pool-1-thread-2:I'm variable_1 in main// pool-1-thread-1:I'm variable_1 in main// main:I'm variable_2 in main// pool-1-thread-1:I'm variable_2 in main// pool-1-thread-2:I'm variable_2 in main 整个过程的完整时序图： 总结 ThreadLocal 和 InheritableThreadLocal 本质上只是为了方便编码给的工具类，具体存数据是 ThreadLocalMap 对象。 ThreadLocalMap 存的 key 对象是 ThreadLocal，value 就是真正需要存的业务对象。 Thread 里通过两个变量持用 ThreadLocalMap 对象，分别为：threadLocals 和 inheritableThreadLocals。 InheritableThreadLocal 之所以能够完成线程间变量的传递，是在 newThread() 的时候对 inheritableThreadLocals 对象里的值进行了复制。 子线程通过继承得到的 InheritableThreadLocal 里的值与父线程里的 InheritableThreadLocal 的值具有相同的引用，如果父子线程想实现不影响各自的对象，可以重写 InheritableThreadLocal 的 childValue 方法。 参考博文[1]. ThreadLocal 和 InheritableThreadLocal 深入分析[2]. transmittable-thread-local[3]. InheritableThreadLocal 详解 Java 并发编程之美系列 Java 并发编程之美（一）：并发队列 Queue 原理剖析 Java 并发编程之美（二）：线程池 ThreadPoolExecutor 原理探究 Java 并发编程之美（三）：异步执行框架 Eexecutor Java 并发编程之美（四）：深入剖析 ThreadLocal Java 并发编程之美（五）：揭开 InheritableThreadLocal 的面纱 Java 并发编程之美（六）：J.U.C 之线程同步辅助工具类]]></content>
      <categories>
        <category>并发</category>
      </categories>
      <tags>
        <tag>ThreadLocal</tag>
        <tag>InheritableThreadLocal</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大话数据结构（一）：那些年面试常见的 Java 排序算法]]></title>
    <url>%2Farchives%2F9c51d4e.html</url>
    <content type="text"><![CDATA[前言排序就是将一组对象按照某种逻辑顺序重新排列的过程。排序在数据处理和现代科学计算中有很重要的地位，应用于很多领域。排序问题一直是程序员工作与面试的重点，今天特意整理研究下与大家共勉。本文将介绍一下常见的排序算法以及 Java 代码实现，如有问题，欢迎指正！ 各算法原理及代码实现 冒泡排序（Bubble Sort）冒泡排序（Bubble Sort）是一种简单的排序算法。它重复地走访要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢 “浮” 到数列的顶端。 原理 比较相邻的元素。如果第一个比第二个大，就交换他们两个。 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对。在这一点，最后的元素应该会是最大的数。 针对所有的元素重复以上的步骤，除了最后一个。 持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较。 代码实现123456789101112public static void bubbleSort(int[] numbers) &#123; for (int i = 0; i &lt; numbers.length - 1; i++) &#123; for (int j = 0; j &lt; numbers.length - i - 1; j++) &#123; if (numbers[j] &lt; numbers[j + 1]) &#123; // 交换两数位置 int temp = numbers[j]; numbers[j] = numbers[j + 1]; numbers[j + 1] = temp; &#125; &#125; &#125;&#125; 快速排序（Quick Sort）选择一个基准元素，通常选择第一个元素或者最后一个元素，通过一趟扫描，将待排序列分成两部分，一部分比基准元素小，一部分大于等于基准元素，此时基准元素在其排好序后的正确位置，然后再用同样的方法递归地排序划分的两部分。 快速排序123456789101112131415161718192021222324252627public static void quickSort(int[] numbers, int low, int high) &#123; // 找到递归算法的出口 if (low &gt;= high) &#123; return; &#125; int temp = numbers[low], begin = low, end = high; while (low &lt; high) &#123; while (low &lt; high &amp;&amp; numbers[high] &gt;= temp) &#123; high--; &#125; // 比中轴小的记录移到低端 numbers[low] = numbers[high]; while (low &lt; high &amp;&amp; numbers[low] &lt;= temp) &#123; low++; &#125; // 比中轴大的记录移到高端 numbers[high] = numbers[low]; &#125; // 中轴记录到尾 numbers[low] = temp; // 对低字段表进行递归排序 quickSort(numbers, begin, low - 1); // 对高字段表进行递归排序 quickSort(numbers, high + 1, end);&#125; 三向切分快速排序1234567891011121314151617181920212223242526public static void quickSort3Way(int[] numbers, int low, int high) &#123; if (low &gt;= high) &#123; return; &#125; int temp = numbers[low], begin = low, i = low + 1, end = high; while (i &lt;= high) &#123; if (numbers[i] &lt; temp) &#123; int t = numbers[i]; numbers[i] = numbers[low]; numbers[low] = t; i++; low++; &#125; else if (numbers[i] &gt; temp) &#123; int t = numbers[i]; numbers[i] = numbers[high]; numbers[high] = t; high--; &#125; else &#123; i++; &#125; &#125; quickSort3Way(numbers, begin, low - 1); quickSort3Way(numbers, high + 1, end);&#125; 选择排序（Selection Sort）直接选择排序是一种简单直观的排序算法。它的工作原理如下：首先在未排序序列中找到最小元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小元素，然后放到排序序列末尾(目前已被排序的序列)。以此类推，直到所有元素均排序完毕。 原理假设数据放在一个数组 a 中，且数组的长度是 N，则直接选择排序的流程为： 从 a[0]-a[N-1]中选出最小的数据，然后与 a[0]交换位置 从 a[1]-a[N-1]中选出最小的数据，然后与 a[1]交换位置（第 1 步结束后 a[0]就是 N 个数的最小值） 从 a[2]-a[N-1]中选出最小的数据，然后与 a[2]交换位置（第 2 步结束后 a[1]就是 N-1 个数的最小值） 以此类推，N-1 次排序后，待排数据就已经按照从小到大的顺序排列了。 代码实现12345678910111213141516171819public static void selectionSort(int[] numbers) &#123; for (int i = 0; i &lt; numbers.length - 1; i++) &#123; // 待确定的位置 int k = i; // 选择出应该在第 i 个位置的数 for (int j = i; j &lt; numbers.length; j++) &#123; if (numbers[j] &lt; numbers[k]) &#123; k = j; &#125; &#125; // 在内层循环结束, 也就是找到本轮循环的最小的数以后, 再进行交换 if (k != i) &#123; int temp = numbers[i]; numbers[i] = numbers[k]; numbers[k] = temp; &#125; &#125;&#125; 堆排序（Heap Sort）堆排序算法和直接选择排序算法最大的不同在于，堆排序算法充分利用大顶堆和完全二叉树的性质，保留每次排序后的结构，同时由于每次比较只是比较根节点和它的子节点，因此大大降低了比较的次数和交换的次数，从而提高效率。 原理假设数据放在一个数组 a 中，且数组的长度是 N： 以数组 a 为数据，建立一个大顶堆（这样对于二叉树的每个节点，根节点总是比子节点大，其实没必要要求二叉树的每个子树也是大顶堆） 交换大顶堆的根节点和数组 a 中的最后一个节点（最后一个节点不在参与后边的工作） 重复上边的工作，经过 N-1 次后，数组 a 已经排好序。 代码实现123456789101112131415161718192021222324252627282930313233343536373839public static void heapSort(int[] numbers) &#123; // 将待排序的序列构建成一个大顶堆 for (int i = numbers.length / 2 - 1; i &gt;= 0; i--) &#123; // 从第一个非叶子结点从下至上, 从右至左调整结构 adjustHeap(numbers, i, numbers.length); &#125; // 调整堆结构 + 交换堆顶元素与末尾元素 for (int j = numbers.length - 1; j &gt; 0; j--) &#123; // 将堆顶记录和当前未经排序子序列的最后一个记录交换 int temp = numbers[0]; numbers[0] = numbers[j]; numbers[j] = temp; // 重新对堆进行调整 adjustHeap(numbers, 0, j); &#125;&#125;public static void adjustHeap(int[] numbers, int low, int high) &#123; // 先取出当前元素 i int temp = numbers[low]; // 从 low 结点的左子结点开始, 也就是 2low+1 处开始 for (int k = low * 2 + 1; k &lt; high; k = k * 2 + 1) &#123; // 如果左子结点小于右子结点, k 指向右子结点 if (k + 1 &lt; high &amp;&amp; numbers[k] &lt; numbers[k + 1]) &#123; k++; &#125; // 如果子节点大于父节点, 将子节点值赋给父节点(不用进行交换) if (numbers[k] &gt; temp) &#123; numbers[low] = numbers[k]; low = k; &#125; else &#123; break; &#125; &#125; // 将 temp 值放到最终的位置 numbers[low] = temp;&#125; 插入排序（Insertion Sort）插入排序是一种通过不断地把新元素插入到已排好序的数据中的排序算法，常用的插入排序算法包括直接插入排序和希尔（Shell）排序，直接插入排序实现比较简单，但是直接插入没有充分的利用已插入的数据已经排序这个事实，因此有很多针对直接插入排序改进的算法。 原理在要排序的一组数中，假设前面 (n-1)[n&gt;=2] 个数已经是排好顺序的，现在要把第 n 个数插到前面的有序数中，使得这 n 个数也是排好顺序的。如此反复循环，直到全部排好顺序。 也就是说，先从无序区拿第一个记录出来，它是有序的，然后把无序区中的记录一个一个插入到其中，那么插入之后是有序的，所以直到最后都是有序的。 代码实现123456789101112public static void insertionSort(int[] numbers) &#123; for (int i = 1; i &lt; numbers.length; i++) &#123; // 保存每次需要插入的那个数 int temp = numbers[i]; int j; // 假如 temp 比前面的值小, 则将前面的值后移 for (j = i; j &gt;= 1 &amp;&amp; temp &lt; numbers[j - 1]; j--) &#123; numbers[j] = numbers[j - 1]; &#125; numbers[j] = temp; &#125;&#125; 希尔排序（Shell Sort）希尔排序是插入排序的一种。是针对直接插入排序算法的改进。该方法又称缩小增量排序，因 DL．Shell 于 1959 年提出而得名。 原理先取定一个小于 n 的整数 d1 作为第 1 个增量，把文件的全部记录分成 d1 个组，所有距离为 d1 的倍数的记录放在同一个组中，在各组内进行直接插入排序；然后，取第 2 个增量 d2&lt;d1 重复上述的分组和排序，直至所取的增量 d1=1(dt&lt;dt-1&lt;…&lt;d2&lt;d1)，即所有记录放在同一组中进行直接插入排序为止。 代码实现12345678910111213141516public static void shellSort(int[] numbers) &#123; // 每次将步长缩短为原来的一半 for (int h = numbers.length / 2; h &gt; 0; h /= 2) &#123; // 进行插入排序 for (int i = h; i &lt; numbers.length; i++) &#123; int j; // 保存每次需要插入的那个数 int temp = numbers[i]; for (j = i; j &gt;= h &amp;&amp; temp &lt; numbers[j - h]; j -= h) &#123; // 如想从小到大排只需修改这里 numbers[j] = numbers[j - h]; &#125; numbers[j] = temp; &#125; &#125;&#125; 归并排序（Merge Sort）归并排序是建立在归并操作上的一种有效的排序算法, 该算法是采用分治法（Divide and Conquer）的一个非常典型的应用。 原理归并（Merge）排序法是将两个（或两个以上）有序表合并成一个新的有序表，即把待排序序列分为若干个子序列，每个子序列是有序的。然后再把有序子序列合并为整体有序序列。 代码实现1234567891011121314151617181920212223242526272829303132333435363738394041424344public static void mergeSort(int[] numbers, int low, int high) &#123; if (low &lt; high) &#123; // 找出中间索引 int mid = (low + high) / 2; // 对左边数组进行递归 mergeSort(numbers, low, mid); // 对右边数组进行递归 mergeSort(numbers, mid + 1, high); // 合并 merge(numbers, low, mid, high); &#125;&#125;public static void merge(int[] numbers, int low, int mid, int high) &#123; // 申请一个新空间来保存排序后数组 int[] mergeArr = new int[high - low + 1]; // i、j 是检测指针, k 是存放指针 int i = low; int j = mid + 1; int k = 0; while (i &lt;= mid &amp;&amp; j &lt;= high) &#123; if (numbers[i] &lt; numbers[j]) &#123; mergeArr[k++] = numbers[i++]; &#125; else &#123; mergeArr[k++] = numbers[j++]; &#125; &#125; // 把左边剩余的元素导入 while (i &lt;= mid) &#123; mergeArr[k++] = numbers[i++]; &#125; // 把右边剩余的元素导入 while (j &lt;= high) &#123; mergeArr[k++] = numbers[j++]; &#125; // 将新排好序的数组放入元素相应的位置中 for (int m = 0; m &lt; mergeArr.length; m++) &#123; numbers[low + m] = mergeArr[m]; &#125;&#125; 大话数据结构系列 大话数据结构（一）：那些年面试常见的 Java 排序算法 大话数据结构（二）：大白话布隆过滤器 Bloom Filter]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式设计之美（一）：主流分布式锁实现方案]]></title>
    <url>%2Farchives%2Fe8097e40.html</url>
    <content type="text"><![CDATA[前言目前很多大型网站及应用都是分布式部署的，分布式场景中的数据一致性问题一直是一个比较重要的话题。分布式系统的 CAP 理论告诉我们“任何一个分布式系统都无法同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance），最多只能同时满足两项”。所以，很多系统在设计之初就要对这三者做出取舍。在互联网领域的绝大多数的场景中，都需要牺牲强一致性来换取系统的高可用性，系统往往只需要保证“最终一致性”，只要这个最终时间是在用户可以接受的范围内即可。在很多场景中，我们为了保证数据的最终一致性，需要很多的技术方案来支持，比如分布式事务、分布式锁等。 针对分布式锁的实现，目前比较常用的有以下几种方案：1、基于数据库实现分布式锁；2、基于缓存 Redis 实现分布式锁；3、基于 Zookeeper 实现分布式锁。下面我将谈谈它们各种的实现方案。 基于 Redis 实现分布式锁基于单机版 Redis 分布式锁 SETNX使用 SETNX（set if not exist）指令插入一个键值对，如果 Key 已经存在，那么会返回 False，否则插入成功并返回 True。SETNX 指令和数据库的唯一索引类似，保证了只存在一个 Key 的键值对，那么可以用一个 Key 的键值对是否存在来判断是否存于锁定状态。EXPIRE 指令可以为一个键值对设置一个过期时间，从而避免了数据库唯一索引实现方式中释放锁失败的问题。 命令 SET resource-name anystring NX EX max-lock-time 是一种在 Redis 中实现锁的简单方法。客户端执行以上的命令：如果服务器返回 OK，那么这个客户端获得锁；如果服务器返回 NIL，那么客户端获取锁失败，可以在稍后再重试。设置的过期时间到达之后，锁将自动释放。 可以通过以下修改，让这个锁实现更健壮：1、不使用固定的字符串作为键的值，而是设置一个不可猜测（non-guessable）的长随机字符串，作为口令串（token）。2、不使用 DEL 命令来释放锁，而是发送一个 Lua 脚本，这个脚本只在客户端传入的值和键的口令串相匹配时，才对键进行删除。这两个改动可以防止持有过期锁的客户端误删现有锁的情况出现。 123456789// 锁的获取：SET resource_name my_random_value NX PX 30000// 锁的释放：if redis.call("get",KEYS[1]) == ARGV[1] then return redis.call("del",KEYS[1])else return 0end 实现方式一1、组件依赖：通过 Maven 引入 Jedis 开源组件，在 pom.xml 文件加入下面的代码： 12345&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;3.0.1&lt;/version&gt;&lt;/dependency&gt; 2、代码实现：通过 Java 代码实现分布式锁 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class RedisDistributedLock &#123; private static final String LOCK_SUCCESS = "OK"; private static final String SET_IF_NOT_EXIST = "NX"; private static final String SET_WITH_EXPIRE_TIME = "PX"; private static final Long RELEASE_SUCCESS = 1L; private static final String RELEASE_LOCK_SCRIPT = "if redis.call('get', KEYS[1]) == ARGV[1] then return redis.call('del', KEYS[1]) else return 0 end"; /** * 尝试获取分布式锁 * * @param jedis Redis 客户端 * @param lockKey 加锁键 * @param clientId 加锁客户端唯一标识(采用 UUID) * @param expireTime 锁过期时间 * @return 是否获取成功 */ public static Boolean acquireLock(Jedis jedis, String lockKey, String clientId, int expireTime) &#123; // String nxxx,NX|XX; String expx,EX|PX, EX = seconds, PX = milliseconds; String result = jedis.set(lockKey, clientId, SET_IF_NOT_EXIST, SET_WITH_EXPIRE_TIME, expireTime); // 如果 setNX 成功, 返回 "OK"; 如果 setNX 失败, 返回 null; if (LOCK_SUCCESS.equals(result)) &#123; return true; &#125; return false; &#125; /** * 释放 Redis 全局锁 * P.S. * 1. 锁的释放必须使用 lua 脚本, 保证操作的原子性; 使用 lua 脚本使 get 与 del 方法执行成为原子性 * 2. 为了保证锁被锁的持有者释放, 使用 lua 脚本删除 redis 中匹配 value 的 key, 可以避免由于方法执行时间过长而 redis 锁自动过期失效的时候误删其他线程的锁 * * @param jedis Redis 客户端 * @param lockKey 加锁键 * @param clientId 加锁客户端唯一标识(采用 UUID) * @return Boolean */ public Boolean releaseLock(Jedis jedis, String lockKey, String clientId) &#123; // 释放锁的时候, 有可能因为持锁之后方法执行时间大于锁的有效期, 此时有可能已经被另外一个线程持有锁, 所以不能直接删除 Object result = jedis.eval(RELEASE_LOCK_SCRIPT, Collections.singletonList(lockKey), Collections.singletonList(clientId)); return RELEASE_SUCCESS.equals(result); &#125;&#125; 3、try-with-resources 实现：大家在写程序的时候是不是总忘记释放锁呢？就像以前对流操作时，忘记了关闭流。从 Java 7 开始，加入了 try-with-resources 的方式，它可以 自动的执行 close() 方法，释放资源，再也不用写 finally 块了。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class RedisDistributedLock implements Closeable &#123; private static final String LOCK_SUCCESS = "OK"; private static final String SET_IF_NOT_EXIST = "NX"; private static final String SET_WITH_EXPIRE_TIME = "PX"; private static final String RELEASE_LOCK_SCRIPT = "if redis.call('get', KEYS[1]) == ARGV[1] then return redis.call('del', KEYS[1]) else return 0 end"; private Jedis jedis; private String lockKey; private String clientId; private int expireTime; /** * 构造全局分布式锁 * * @param jedis Redis 客户端 * @param lockKey 加锁键 * @param clientId 加锁客户端唯一标识(采用 UUID) * @param expireTime 锁过期时间 */ public RedisDistributedLock(Jedis jedis, String lockKey, String clientId, int expireTime) &#123; this.jedis = jedis; this.lockKey = lockKey; this.clientId = clientId; this.expireTime = expireTime; &#125; /** * 尝试获取分布式锁 * * @return 是否获取成功 */ public Boolean acquireLock() &#123; // String nxxx,NX|XX; String expx,EX|PX, EX = seconds, PX = milliseconds; String result = jedis.set(lockKey, clientId, SET_IF_NOT_EXIST, SET_WITH_EXPIRE_TIME, expireTime); // 如果 setNX 成功, 返回 "OK"; 如果 setNX 失败, 返回 null; return LOCK_SUCCESS.equals(result); &#125; /** * 释放 Redis 全局锁 * P.S. * 1. 锁的释放必须使用 lua 脚本, 保证操作的原子性; 使用 lua 脚本使 get 与 del 方法执行成为原子性 * 2. 为了保证锁被锁的持有者释放, 使用 lua 脚本删除 redis 中匹配 value 的 key, 可以避免由于方法执行时间过长而 redis 锁自动过期失效的时候误删其他线程的锁 */ @Override public void close() throws IOException &#123; // 释放锁的时候, 有可能因为持锁之后方法执行时间大于锁的有效期, 此时有可能已经被另外一个线程持有锁, 所以不能直接删除 jedis.eval(RELEASE_LOCK_SCRIPT, Collections.singletonList(lockKey), Collections.singletonList(clientId)); &#125;&#125; 实现方式二1、组件依赖：通过 Maven 引入 spring-boot-starter-data-redis 开源组件，在 pom.xml 文件加入下面的代码： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; 2、代码实现：通过 Java 代码实现分布式锁 1234567891011121314151617181920212223242526272829303132333435363738394041@Component@Slf4jpublic class RedisDistributedLock &#123; @Autowired private StringRedisTemplate stringRedisTemplate; /** * 该加锁方法仅针对单实例 Redis 可实现分布式加锁; * P.S. * 1. 单机版分布式锁 SETNX, 所谓 SETNX, 是「SET if Not eXists」的缩写; * 2. SET resource_name my_random_value NX PX 30000 因此当多个客户端去争抢执行上锁或解锁代码时, 最终只会有一个客户端执行成功. 同时 set 命令还可以指定 key 的有效期, 这样即使当前客户端奔溃, 过一段时间锁也会被 redis 自动释放, 这就给了其它客户端获取锁的机会. * * @param lockKey 加锁键 * @param clientId 加锁客户端唯一标识(采用 UUID) * @param timeout 锁过期时间 * @param timeUnit 锁过期单位 * @return Boolean */ public Boolean acquireLock(String lockKey, String clientId, Long timeout, TimeUnit timeUnit) &#123; return stringRedisTemplate.opsForValue().setIfAbsent(lockKey, clientId, timeout, timeUnit); &#125; /** * 释放 Redis 全局锁 * P.S. * 1. 锁的释放必须使用 lua 脚本, 保证操作的原子性; 使用 lua 脚本使 get 与 del 方法执行成为原子性 * * @param lockKey 加锁键 * @param clientId 加锁客户端唯一标识(采用 UUID) * @return Boolean */ public Boolean releaseLock(String lockKey, String clientId) &#123; // 释放锁的时候, 有可能因为持锁之后方法执行时间大于锁的有效期, 此时有可能已经被另外一个线程持有锁, 所以不能直接删除 return (Boolean) stringRedisTemplate.execute((RedisCallback&lt;Object&gt;) connection -&gt; &#123; // 为了保证锁被锁的持有者释放, 使用 lua 脚本删除 redis 中匹配 value 的 key, 可以避免由于方法执行时间过长而 redis 锁自动过期失效的时候误删其他线程的锁 String script = "if redis.call('get', KEYS[1]) == ARGV[1] then return redis.call('del', KEYS[1]) else return 0 end"; return connection.&lt;Boolean&gt;eval(script.getBytes(), ReturnType.BOOLEAN, 1, lockKey.getBytes(), clientId.getBytes()); &#125;); &#125;&#125; 注意上述代码实现，仅对 redis 单实例架构有效，当面对 redis 集群时就无效了。但是一般情况下，我们的 redis 架构多数会做成 “主备” 模式，然后再通过 redis 哨兵实现主从切换，这种模式下我们的应用服务器直接面向主机，也可看成是单实例，因此上述代码实现也有效。但是当在主机宕机，从机被升级为主机的一瞬间的时候，如果恰好在这一刻，由于 redis 主从复制的异步性，导致从机中数据没有即时同步，那么上述代码依然会无效，导致同一资源有可能会产生两把锁，违背了分布式锁的原则。 基于 Redis 的 RedLock 算法使用了多个 Redis 实例来实现分布式锁，这是为了保证在发生单点故障时仍然可用。Redis 的作者提出了 RedLock 的解决方案。方案非常的巧妙和简洁。RedLock 的核心思想就是，同时使用多个 Redis Master 来冗余，且这些节点都是完全的独立的，也不需要对这些节点之间的数据进行同步。 假设我们有 N 个 Redis 节点，N 应该是一个大于 2 的奇数。RedLock 的实现步骤： 获取当前 Unix 时间，以毫秒为单位。 使用上文提到的方法依次获取 N 个节点的 Redis 锁。 如果获取到的锁的数量大于（N/2+1）个，且获取的时间小于锁的有效时间（lock validity time）就认为获取到了一个有效的锁。 如果获取锁的数量小于（N/2+1），或者在锁的有效时间（lock validity time）内没有获取到足够的锁，就认为获取锁失败。这个时候需要向所有节点发送释放锁的消息。 实现方式一Redisson 在基于 NIO 的 Netty 框架上，充分的利用了 Redis 键值数据库提供的一系列优势，在 Java 实用工具包中常用接口的基础上，为使用者提供了一系列具有分布式特性的常用工具类。使得原本作为协调单机多线程并发程序的工具包获得了协调分布式多机多线程并发系统的能力，大大降低了设计和研发大规模分布式系统的难度。同时结合各富特色的分布式服务，更进一步简化了分布式环境中程序相互之间的协作。 基于 Redis 的 Redisson 红锁 RedissonRedLock 对象实现了 Redlock 介绍的加锁算法。该对象也可以用来将多个 RLock 对象关联为一个红锁，每个 RLock 对象实例可以来自于不同的 Redisson 实例。 1、组件依赖：通过 Maven 引入 redisson 开源组件，在 pom.xml 文件加入下面的代码： 12345&lt;dependency&gt; &lt;groupId&gt;org.redisson&lt;/groupId&gt; &lt;artifactId&gt;redisson&lt;/artifactId&gt; &lt;version&gt;3.9.1&lt;/version&gt;&lt;/dependency&gt; 2、代码实现：通过 Java 代码实现分布式锁 1234567891011121314151617181920212223242526272829303132333435363738394041424344public static void main() &#123; Config config1 = new Config(); config1.useSingleServer().setAddress("redis://xxxx1:xxx1") .setPassword("xxxx1") .setDatabase(0); RedissonClient redissonClient1 = Redisson.create(config1); Config config2 = new Config(); config2.useSingleServer() .setAddress("redis://xxxx2:xxx2") .setPassword("xxxx2") .setDatabase(0); RedissonClient redissonClient2 = Redisson.create(config2); Config config3 = new Config(); config3.useSingleServer(). setAddress("redis://xxxx3:xxx3") .setPassword("xxxx3") .setDatabase(0); RedissonClient redissonClient3 = Redisson.create(config3); String lockName = "redlock-test"; RLock lock1 = redissonClient1.getLock(lockName); RLock lock2 = redissonClient2.getLock(lockName); RLock lock3 = redissonClient3.getLock(lockName); RedissonRedLock redLock = new RedissonRedLock(lock1, lock2, lock3); boolean isLock; try &#123; // 为加锁等待 0.5 秒时间，并在加锁成功 30 秒钟后自动解开 isLock = redLock.tryLock(500, 30000, TimeUnit.MILLISECONDS); System.out.println("isLock =" + isLock); if (isLock) &#123; // lock success, do something; Thread.sleep(30000); &#125; &#125; catch (Exception e) &#123; &#125; finally &#123; // 无论如何, 最后都要解锁 redLock.unlock(); System.out.println("unlock success"); &#125;&#125; 注意失败时重试：当客户端无法获取到锁时，应该随机延时后进行重试，防止多个客户端在同一时间抢夺同一资源的锁（会导致脑裂，最终都不能获取到锁）。客户端获得超过半数节点的锁花费的时间越短，那么脑裂的概率就越低。所以，理想的情况下，客户端最好能够同时（并发）向所有 redis 发出 set 命令。当客户端从多数节点获取锁失败时，应该尽快释放已经成功获取的锁，这样其他客户端不需要等待锁过期后再获取。（如果存在网络分区，客户端已经无法和 redis 进行通信，那么此时只能等待锁过期后自动释放）。 PUBSUB：订阅者模式，当释放锁的时候，其他客户端能够知道锁已经被释放的消息，并让队列中的第一个消费者获取锁。使用 PUB/SUB 消息机制的优点：减少申请锁时的等待时间、安全、 锁带有超时时间、锁的标识唯一，防止死锁 锁设计为可重入，避免死锁。 基于 Zookeeper 实现分布式锁Zookeeper 节点性质 有序节点：假如当前有一个父节点为 / lock，我们可以在这个父节点下面创建子节点；zookeeper 提供了一个可选的有序特性，例如我们可以创建子节点 “/lock/node-” 并且指明有序，那么 zookeeper 在生成子节点时会根据当前的子节点数量自动添加整数序号，也就是说如果是第一个创建的子节点，那么生成的子节点为 / lock/node-0000000000，下一个节点则为 / lock/node-0000000001，依次类推。 临时节点：客户端可以建立一个临时节点，在会话结束或者会话超时后，zookeeper 会自动删除该节点。 事件监听：在读取数据时，我们可以同时对节点设置事件监听，当节点数据或结构变化时，zookeeper 会通知客户端。当前 zookeeper 有如下四种事件：1）节点创建；2）节点删除；3）节点数据修改；4）子节点变更。 基于创建临时 Znode某个节点尝试创建临时 znode，此时创建成功了就获取了这个锁；这个时候别的客户端来创建锁会失败，只能注册个监听器监听这个锁。释放锁就是删除这个 znode，一旦释放掉就会通知客户端，然后有一个等待着的客户端就可以再次重新加锁。 这种方案的正确性和可靠性是 ZooKeeper 机制保证的，实现简单。缺点是会产生 “惊群” 效应，假如许多客户端在等待一把锁，当锁释放时候所有客户端都被唤醒，仅仅有一个客户端得到锁。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119public class ZooKeeperSession &#123; private static CountDownLatch connectedSemaphore = new CountDownLatch(1); private ZooKeeper zookeeper; private CountDownLatch latch; public ZooKeeperSession() &#123; try &#123; this.zookeeper = new ZooKeeper("192.168.31.187:2181,192.168.31.19:2181,192.168.31.227:2181", 50000, new ZooKeeperWatcher()); try &#123; connectedSemaphore.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("ZooKeeper session established......"); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; /** * 获取分布式锁 * * @param productId */ public Boolean acquireDistributedLock(Long productId) &#123; String path = "/product-lock-" + productId; try &#123; zookeeper.create(path, "".getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL); return true; &#125; catch (Exception e) &#123; while (true) &#123; try &#123; // 相当于是给 node 注册一个监听器，去看看这个监听器是否存在 Stat stat = zk.exists(path, true); if (stat != null) &#123; this.latch = new CountDownLatch(1); this.latch.await(waitTime, TimeUnit.MILLISECONDS); this.latch = null; &#125; zookeeper.create(path, "".getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL); return true; &#125; catch (Exception ee) &#123; continue; &#125; &#125; &#125; return true; &#125; /** * 释放掉一个分布式锁 * * @param productId */ public void releaseDistributedLock(Long productId) &#123; String path = "/product-lock-" + productId; try &#123; zookeeper.delete(path, -1); System.out.println("release the lock for product[id=" + productId + "]......"); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; /** * 建立 zk session 的 watcher */ private class ZooKeeperWatcher implements Watcher &#123; public void process(WatchedEvent event) &#123; System.out.println("Receive watched event:" + event.getState()); if (KeeperState.SyncConnected == event.getState()) &#123; connectedSemaphore.countDown(); &#125; if (this.latch != null) &#123; this.latch.countDown(); &#125; &#125; &#125; /** * 封装单例的静态内部类 */ private static class Singleton &#123; private static ZooKeeperSession instance; static &#123; instance = new ZooKeeperSession(); &#125; public static ZooKeeperSession getInstance() &#123; return instance; &#125; &#125; /** * 获取单例 * * @return */ public static ZooKeeperSession getInstance() &#123; return Singleton.getInstance(); &#125; /** * 初始化单例的便捷方法 */ public static void init() &#123; getInstance(); &#125;&#125; 基于创建临时顺序节点对于加锁操作，可以让所有客户端都去 / lock 目录下创建临时顺序节点，如果客户端发现自身创建节点序列号是 / lock / 目录下最小的节点，则获得锁。否则，监视比自己创建节点的序列号小的节点（比自己创建的节点小的最大节点），进入等待。对于解锁操作，只需要将自身创建的节点删除即可，然后唤醒自己的后一个节点。 特点：利用临时顺序节点来实现分布式锁机制其实就是一种按照创建顺序排队的实现。这种方案效率高，避免了 “惊群” 效应，多个客户端共同等待锁，当锁释放时只有一个客户端会被唤醒。 实现步骤 客户端连接 zookeeper，并在 / lock 下创建临时的且有序的子节点，第一个客户端对应的子节点为 / lock/lock-0000000000，第二个为 / lock/lock-0000000001，以此类推。 客户端获取 / lock 下的子节点列表，判断自己创建的子节点是否为当前子节点列表中序号最小的子节点，如果是则认为获得锁，否则监听刚好在自己之前一位的子节点删除消息，获得子节点变更通知后重复此步骤直至获得锁； 执行业务代码； 完成业务流程后，删除对应的子节点释放锁。 实现方式一123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101public class ZooKeeperDistributedLock implements Watcher &#123; private ZooKeeper zk; private String locksRoot = "/locks"; private String productId; private String waitNode; private String lockNode; private CountDownLatch latch; private CountDownLatch connectedLatch = new CountDownLatch(1); private int sessionTimeout = 30000; public ZooKeeperDistributedLock(String productId) &#123; this.productId = productId; try &#123; String address = "192.168.31.187:2181,192.168.31.19:2181,192.168.31.227:2181"; zk = new ZooKeeper(address, sessionTimeout, this); connectedLatch.await(); &#125; catch (IOException | InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; public void process(WatchedEvent event) &#123; if (event.getState() == KeeperState.SyncConnected) &#123; connectedLatch.countDown(); return; &#125; if (this.latch != null) &#123; this.latch.countDown(); &#125; &#125; public void acquireDistributedLock() &#123; try &#123; if (this.tryLock()) &#123; return; &#125; else &#123; waitForLock(waitNode, sessionTimeout); &#125; &#125; catch (KeeperException | InterruptedException e) &#123; throw new RuntimeException(e); &#125; &#125; public boolean tryLock() &#123; try &#123; // 传入进去的 locksRoot + "/" + productId // 假设 productId 代表了一个商品 id, 比如说 1 // locksRoot = locks // /locks/10000000000,/locks/10000000001,/locks/10000000002 lockNode = zk.create(locksRoot + "/" + productId, new byte[0], ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL); // 看看刚创建的节点是不是最小的节点 // locks：10000000000,10000000001,10000000002 List&lt;String&gt; locks = zk.getChildren(locksRoot, false); Collections.sort(locks); if (lockNode.equals(locksRoot + "/" + locks.get(0))) &#123; // 如果是最小的节点, 则表示取得锁 return true; &#125; // 如果不是最小的节点, 找到比自己小 1 的节点 int previousLockIndex = -1; for (int i = 0; i &lt; locks.size(); i++) &#123; if (lockNode.equals(locksRoot + "/" + locks.get(i))) &#123; previousLockIndex = i - 1; break; &#125; &#125; this.waitNode = locks.get(previousLockIndex); &#125; catch (KeeperException | InterruptedException e) &#123; e.printStackTrace(); &#125; return false; &#125; private boolean waitForLock(String waitNode, long waitTime) throws InterruptedException, KeeperException &#123; Stat stat = zk.exists(locksRoot + "/" + waitNode, true); if (stat != null) &#123; this.latch = new CountDownLatch(1); this.latch.await(waitTime, TimeUnit.MILLISECONDS); this.latch = null; &#125; return true; &#125; public void unlock() &#123; try &#123; // 删除 / locks/10000000001 节点 System.out.println("unlock" + lockNode); zk.delete(lockNode, -1); lockNode = null; zk.close(); &#125; catch (InterruptedException | KeeperException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 实现方式二虽然 Zookeeper 原生客户端暴露的 API 已经非常简洁了，但是实现一个分布式锁还是比较麻烦的。我们可以直接使用 curator 这个开源项目提供的 zookeeper 分布式锁实现。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public class DistributedLock &#123; private static Logger log = LoggerFactory.getLogger(DistributedLock.class); private InterProcessMutex interProcessMutex; // 可重入排它锁 private String lockName; // 竞争资源标志 private String root = "/distributed/lock/";// 根节点 private static CuratorFramework curatorFramework; private static String ZK_URL = "zookeeper1.tq.master.cn:2181,zookeeper3.tq.master.cn:2181,zookeeper2.tq.master.cn:2181,zookeeper4.tq.master.cn:2181,zookeeper5.tq.master.cn:2181"; static &#123; curatorFramework = CuratorFrameworkFactory.newClient(ZK_URL, new ExponentialBackoffRetry(1000, 3)); curatorFramework.start(); &#125; /** * 实例化 * * @param lockName */ public DistributedLock(String lockName) &#123; try &#123; this.lockName = lockName; interProcessMutex = new InterProcessMutex(curatorFramework, root + lockName); &#125; catch (Exception e) &#123; log.error("initial InterProcessMutex exception=" + e); &#125; &#125; /** * 获取锁 */ public void acquireLock() &#123; int flag = 0; try &#123; // 重试 2 次，每次最大等待 2s，也就是最大等待 4s while (!interProcessMutex.acquire(2, TimeUnit.SECONDS)) &#123; flag++; if (flag &gt; 1) &#123; // 重试两次 break; &#125; &#125; &#125; catch (Exception e) &#123; log.error("distributed lock acquire exception=" + e); &#125; if (flag &gt; 1) &#123; log.info("Thread:" + Thread.currentThread().getId() + "acquire distributed lock busy"); &#125; else &#123; log.info("Thread:" + Thread.currentThread().getId() + "acquire distributed lock success"); &#125; &#125; /** * 释放锁 */ public void releaseLock() &#123; try &#123; if (interProcessMutex != null &amp;&amp; interProcessMutex.isAcquiredInThisProcess()) &#123; interProcessMutex.release(); curatorFramework.delete().inBackground().forPath(root + lockName); log.info("Thread:" + Thread.currentThread().getId() + "release distributed lock success"); &#125; &#125; catch (Exception e) &#123; log.info("Thread:" + Thread.currentThread().getId() + "release distributed lock exception=" + e); &#125; &#125;&#125; 基于 MySQL 实现分布式锁基于数据库表要实现分布式锁，最简单的方式可能就是直接创建一张锁表，然后通过操作该表中的数据来实现了。当我们要锁住某个方法或资源的时候，我们就在该表中增加一条记录，想要释放锁的时候就删除这条记录。获得锁时向表中插入一条记录，释放锁时删除这条记录。唯一索引可以保证该记录只被插入一次，那么就可以用这个记录是否存在来判断是否存于锁定状态。 创建这样一张数据库表： 12345678CREATE TABLE `method_lock` ( `id` INT (11) NOT NULL AUTO_INCREMENT COMMENT '主键', `method_name` VARCHAR (64) NOT NULL DEFAULT ''COMMENT'锁定的方法名', `desc` VARCHAR (1024) NOT NULL DEFAULT '备注信息', `update_time` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '保存数据时间, 自动生成', PRIMARY KEY (`id`), UNIQUE KEY `uidx_method_name` (`method_name `) USING BTREE) ENGINE = INNODB DEFAULT CHARSET = utf8 COMMENT = '锁定中的方法'; a. 当我们要锁住某个方法时，执行以下 SQL： 1INSERT INTO method_lock (method_name, DESC) VALUES (‘method_name’, ‘desc’); 因为我们对 method_name 做了唯一性约束，这里如果有多个请求同时提交到数据库的话，数据库会保证只有一个操作可以成功，那么我们可以认为操作成功的那个线程获得了该方法的锁，可以执行具体内容。 b. 当方法执行完毕之后，想要释放锁的话，需要执行以下 sql： 1DELETE FROM method_lock WHERE method_name = 'method_name'; 上面这种简单的实现有以下几个问题：1、这把锁依赖数据库的可用性，数据库是一个单点，一旦数据库挂掉，会导致业务系统不可用。2、这把锁没有失效时间，一旦解决操作失败，就会导致记录一直在数据库中，其他线程无法在获得锁。3、这把锁只能是非阻塞的，因为数据的 insert 操作，一旦插入失败就会直接报错。没有获得锁的线程并不会进入排队队列，要想再次获得锁就要再次触发获得锁的操作。4、这把锁是非重入的，同一个线程在没有释放锁之前无法再次获得该锁。因为数据库表中数据已经存在了。 基于数据库表做乐观锁大多数是基于数据版本（version）的记录机制实现的。何谓数据版本号？即为数据增加一个版本标识，在基于数据库表的版本解决方案中，一般是通过为数据库表添加一个 “version” 字段来实现读取出数据时，将此版本号一同读出，之后更新时，对此版本号加 1。在更新过程中，会对版本号进行比较，如果是一致的，没有发生改变，则会成功执行本次操作；如果版本号不一致，则会更新失败。 假设我们有一张资源表，状态(1：未分配；2：已分配)、资源创建时间、资源更新时间、资源数据版本号。 那么如果使用乐观锁如何解决问题呢? a. 先执行 select 操作查询当前数据的数据版本号，比如当前数据版本号是 26： 1SELECT id, resource, state, version FROM t_resource WHERE state = 1 AND id = 5780; b. 执行更新操作： 1UPDATE t_resoure SET state = 2, version = 27, update_time = now() WHERE resource = xxxxxx AND state = 1 AND version = 26; c. 如果上述 update 语句真正更新影响到了一行数据，那就说明占位成功。如果没有更新影响到一行数据，则说明这个资源已经被别人占位了。 基于数据库表做乐观锁的一些缺点：1、这种操作方式，使原本一次的 update 操作，必须变为 2 次操作：select 版本号一次；update 一次。增加了数据库操作的次数。2、如果业务场景中的一次业务流程中，多个资源都需要用保证数据一致性，那么如果全部使用基于数据库资源表的乐观锁，就要让每个资源都有一张资源表，这个在实际使用场景中肯定是无法满足的。而且这些都基于数据库操作，在高并发的要求下，对数据库连接的开销一定是无法忍受的。3、乐观锁机制往往基于系统中的数据存储逻辑，因此可能会造成脏数据被更新到数据库中。在系统设计阶段，我们应该充分考虑到这些情况出现的可能性，并进行相应调整，如将乐观锁策略在数据库存储过程中实现，对外只开放基于此存储过程的数据更新途径，而不是将数据库表直接对外公开。 基于数据库表做悲观锁利用 for update 加显式的行锁，这样就能利用这个行级的排他锁来实现分布式锁了，同时 unlock 的时候只要释放 commit 这个事务，就能达到释放锁的目的。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * 超时获取锁 * * @param lockID * @param timeOuts * @return boolean * @throws InterruptedException */public boolean acquireByUpdate(String lockID, long timeOuts) throws InterruptedException, SQLException &#123; String sql = "SELECT id from test_lock where id = ? for UPDATE"; long futureTime = System.currentTimeMillis() + timeOuts; long ranmain = timeOuts; long timerange = 500; connection.setAutoCommit(false); while (true) &#123; CountDownLatch latch = new CountDownLatch(1); try &#123; PreparedStatement statement = connection.prepareStatement(sql); statement.setString(1, lockID); statement.setInt(2, 1); statement.setLong(1, System.currentTimeMillis()); boolean ifsucess = statement.execute();// 如果成功，那么就是获取到了锁 if (ifsucess) return true; &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; latch.await(timerange, TimeUnit.MILLISECONDS); ranmain = futureTime - System.currentTimeMillis(); if (ranmain &lt;= 0) break; if (ranmain &lt; timerange) &#123; timerange = ranmain; &#125; continue; &#125; return false;&#125;/** * 释放锁 * * @param lockID * @throws SQLException */public void unlockforUpdtate(String lockID) throws SQLException &#123; connection.commit();&#125; 优点：实现简单缺点：连接池爆满和事务超时的问题单点的问题，单点问题，行锁升级为表锁的问题，并发量大的时候请求量太大、没有线程唤醒机制。适用场景：并发量略高于上面使用乐观锁的情况下，可以采用这种方法。 总结：不论如何，使用 Mysql 来实现分布式锁都不推荐，其性能，可靠性，以及实现上跟其它两种方式对比均没啥优势，因此，学习 Mysql 实现分布式锁可以仅仅作为一种了解和思想升华。 参考博文[1]. Redlock：Redis 分布式锁最牛逼的实现[2]. 一般实现分布式锁都有哪些方式？使用 redis 如何设计分布式锁？使用 zk 来设计分布式锁可以吗？这两种分布式锁的实现方式哪种效率比较高？ 分布式设计之美系列 分布式设计之美（一）：主流分布式锁实现方案 分布式设计之美（二）：微服务架构下分布式事务解决方案]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>Redis</tag>
        <tag>分布式锁</tag>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git 在团队中的最佳实践（一）：Git 备忘清单]]></title>
    <url>%2Farchives%2F1cad84b0.html</url>
    <content type="text"><![CDATA[前言Git 是一个免费并且开源的分布式版本控制系统，旨在快速高效地处理从小到大所有项目的版本管理。Git 是目前最流行的版本管理工具，目前绝大部分公司都是使用 Git 作为项目的版本管理工具。目前最火的开源社区 Github，就是基于 Git 版本控制系统，所以掌握 Git 技能很重要。由于 Git 开发效率高、团队协作方便，现在很多 IDE 都集成了 Git，并且提供一些相关的图形化操作。也有很多很优秀，专门用来简化 Git 操作的 Git GUI 工具，例如 Sourcetree，Tortoise 等。我刚接触 Git 的时候，就是从 GUI 入手的，使用 Sourcetree 可视化版本控制工具进行操作。Sourcetree 底层也是对常用的 Git 命令进行封装实现的，傻瓜式操作，使用非常方便，但是我用完什么都不懂，Git 的内部原理也不易理解，决定在回头仔细学习一下 Git 命令行。学习完 Git 命令行后发现，命令行很好学，非常灵活，而且使用起来非常帅气。因此，直接 Git 命令，才是最灵活的操作。 本篇是我学习 Git 系列的开篇，主要讲述 Git 的基本概念和工作原理，然后介绍一下 Git 安装以及环境配置，最后探讨一下 Git 常用命令以及使用场景。 Git 基本概念以及工作原理你所不了解的 Git 诞生史同生活中的许多伟大事件一样，Git 诞生于一个极富纷争大举创新的年代。众所周知，Linux 内核开源项目有着为数众广的参与者，但是绝大多数的 Linux 内核维护工作都花在了提交补丁和保存归档的繁琐事务上（1991－2002 年间），即 Linus 本人通过手工方式合并世界各地志愿者通过 diff 的方式传过来的代码。 到 2002 年后，由于代码库太大，Linus 很难继续通过手工方式进行管理，于是 Linux 整个项目组开始启用分布式版本控制系统 BitKeeper 来管理和维护代码，BitKeeper 的东家 BitMover 公司出于人道主义精神，授权 Linux 社区免费使用这个版本控制系统。 到了 2005 年，一位 Linux 开发成员 Andrew（Samba 协议之父）写了一个可以连接 BitKeeper 仓库的外挂，因此 BitMover 公司（BitKeeper 持有者）认为他反编译了 BitKeeper；于是 BitMover 决定中止 Linux 免费使用 BitKeeper 的授权。最终 Linux 团队与 BitMover 磋商无果，开发 BitKeeper 的商业公司同 Linux 内核开源社区的合作关系结束，他们收回了免费使用 BitKeeper 的权力。这就迫使 Linux 开源社区（特别是 Linux 的缔造者 Linus Torvalds）不得不吸取教训，只有开发一套属于自己的版本控制系统才不至于重蹈覆辙。 于是，Linus 花了两周时间用 C 语言写了一个分布式版本控制系统，这就是 Git！一个月之内，Linux 系统的源码已经由 Git 进行管理了！ Git 工作原理Git 是一套内容寻址文件系统，Git 从核心上来看不过是简单地存储键值对（key-value）。它允许插入任意类型的内容，并会返回一个键值，通过该键值可以在任何时候再取出该内容。可以通过底层命令 hash-object 来示范这点，传一些数据给该命令，它会将数据保存在 .git 目录并返回表示这些数据的键值。 文件目录 Git 工作区有个隐藏目录. git，核心文件包括：config 文件、objects 文件夹、HEAD 文件、index 文件以及 refs 文件夹。下面依次对其进行说明： config 文件：该文件主要记录针对该项目的一些配置信息，例如是否以 bare 方式初始化、remote 的信息等，通过 git remote add 命令增加的远程分支的信息就保存在这里； objects 文件夹：该文件夹主要包含 git 对象。Git 中的文件和一些操作都会以 git 对象来保存，git 对象分为 BLOB、tree 和 commit 三种类型，例如 git commit 便是 git 中的 commit 对象，而各个版本之间是通过版本树来组织的，比如当前的 HEAD 会指向某个 commit 对象，而该 commit 对象又会指向几个 BLOB 对象或者 tree 对象。objects 文件夹中会包含很多的子文件夹，其中 Git 对象保存在以其 sha-1 值的前两位为子文件夹、后 38 位位文件名的文件中；除此以外，Git 为了节省存储对象所占用的磁盘空间，会定期对 Git 对象进行压缩和打包，其中 pack 文件夹用于存储打包压缩的对象，而 info 文件夹用于从打包的文件中查找 git 对象； HEAD 文件：该文件指明了 git branch（即当前分支）的结果，比如当前分支是 master，则该文件就会指向 master，但是并不是存储一个 master 字符串，而是分支在 refs 中的表示，例如 ref: refs/heads/master。 index 文件：该文件保存了暂存区域的信息。该文件某种程度就是缓冲区（staging area），内容包括它指向的文件的时间戳、文件名、sha1 值等； Refs 文件夹：该文件夹存储指向数据（分支）的提交对象的指针。其中 heads 文件夹存储本地每一个分支最近一次 commit 的 sha-1 值（也就是 commit 对象的 sha-1 值），每个分支一个文件；remotes 文件夹则记录你最后一次和每一个远程仓库的通信，Git 会把你最后一次推送到这个 remote 的每个分支的值都记录在这个文件夹中；tag 文件夹则是分支的别名，这里不需要对其有过多的了解； 工作区域Git 本地有三个工作区域：工作目录（Workspace）、暂存区 (Stage/Index)、资源库(Repository)。如果在加上远程的 git 仓库(Remote Directory) 就可以分为四个工作区域。文件在这四个区域之间的转换关系如下： Workspace：工作区，就是你平时存放项目代码的地方。 Index / Stage：暂存区，用于临时存放你的改动，事实上它只是一个文件，保存即将提交到文件列表信息。 Repository：仓库区（或本地仓库），就是安全存放数据的位置，这里面有你提交到所有版本的数据。其中 HEAD 指向最新放入仓库的版本。 Remote：远程仓库，托管代码的服务器，可以简单的认为是你项目组中的一台电脑用于远程数据交换。 工作流程Git 的工作流程一般是这样的： １、在工作目录中添加、修改文件； ２、将需要进行版本管理的文件放入暂存区域； ３、将暂存区域的文件提交到 Git 仓库。 Git 文件 4 种状态 Untracked: 未跟踪, 此文件在文件夹中, 但并没有加入到 git 库, 不参与版本控制. 通过 git add 状态变为 Staged. Unmodify: 文件已经入库, 未修改, 即版本库中的文件快照内容与文件夹中完全一致. 这种类型的文件有两种去处, 如果它被修改, 而变为 Modified. 如果使用 git rm 移出版本库, 则成为 Untracked 文件 Modified: 文件已修改, 仅仅是修改, 并没有进行其他的操作. 这个文件也有两个去处, 通过 git add 可进入暂存 staged 状态, 使用 git checkout 则丢弃修改过, 返回到 unmodify 状态, 这个 git checkout 即从库中取出文件, 覆盖当前修改 Staged: 暂存状态. 执行 git commit 则将修改同步到库中, 这时库中的文件和本地文件又变为一致, 文件为 Unmodify 状态. 执行 git reset HEAD filename 取消暂存, 文件状态为 Modified Git 安装以及环境配置本文统一使用软件包管理器的方式安装 Git，减少环境变量的配置，更加方便快捷。 Linux 安装 GitCentOS7 中使用 yum 安装 Git 的方法 1234567891011# 1. 查看 git 是否安装$ git --version# 2. 显示所有已经安装和可以安装的 git 程序包$ sudo yum list git# 3. 使用 yum 安装$ sudo yum install git -y# 4. 查看安装是否成功$ git --version Mac 安装 GitMac 中使用 brew 安装 Git 的方法 12345678# 1. 查看 git 是否安装$ git --version# 2. 使用 brew 安装$ brew install git# 3. 查看安装是否成功$ git --version Windows 安装 GitWindows 中使用 choco 安装 Git 的方法 1234567891011# 1. 查看 git 是否安装$ git --version# 2. 搜索 git 安装包$ choco search git# 3. 使用 choco 安装$ choco install git# 4. 查看安装是否成功$ git --version 全局配置Git 用户的配置文件位于 ~/.gitconfigGit 单个仓库的配置文件位于 ~/$PROJECT_PATH/.git/config 123456789# 1. 显示当前的 Git 配置$ git config --list# 2. 编辑 Git 配置文件 p.s.[--global: 表示全局配置, 如果不加, 则表示当前 git 仓库的配置]$ git config -e [--global]# 3. 设置提交代码时的用户信息$ git config [--global] user.name "[name]"$ git config [--global] user.email "[email address]" 如果用了 –global 选项，那么更改的配置文件就是位于你用户主目录下的那个，以后你所有的项目都会默认使用这里配置的用户信息。如果要在某个特定的项目中使用其他名字或者电邮，只要去掉 –global 选项重新配置即可，新的设定保存在当前项目的 .git/config 文件里。 服务器上的 Git - 生成 SSH 公钥大多数 Git 服务器都会选择使用 SSH 公钥来进行授权。系统中的每个用户都必须提供一个公钥用于授权，没有的话就要生成一个。SSH 公钥默认储存在账户的主目录下的 ~/.ssh 目录。 1234567891011# 1. 进入主目录下的~/.ssh 目录$ cd ~/.ssh# 2. 创建一个 SSH key p.s.[-t 指定密钥类型，默认是 rsa，可以省略；-C 设置注释文字，比如邮箱；-f 指定密钥文件存储文件名]$ ssh-keygen -t rsa -C "your_email@example.com"# 3. 添加你的 SSH key 到 github 上面去$ cat ./id_rsa.pub# 4. 测试一下该 SSH key$ ssh -T git@github.com 关于在多个操作系统上设立相同 SSH 公钥的教程，可以查阅 GitHub 上有关 SSH 公钥的向导。 Git 常用命令以及使用场景 仓库12345678# 1. 在当前目录新建一个 Git 代码库$ git init# 2. 新建一个目录, 将其初始化为 Git 代码库$ git init [project-name]# [3]. 下载一个项目和它的整个代码历史$ git clone [url] 增加 / 删除文件1234567891011121314151617181920# 1. 添加指定文件到暂存区$ git add [file1] [file2] ...# 2. 添加指定目录到暂存区, 包括子目录$ git add [dir]# [3]. 添加当前目录的所有文件到暂存区$ git add .# 4. 添加每个变化前, 都会要求确认[对于同一个文件的多处变化, 可以实现分次提交]$ git add -p# 5. 删除工作区文件, 并且将这次删除放入暂存区 p.s.[git rm -f [file1]强制删除暂存区某个文件]$ git rm [file1] [file2] ...# [6]. 停止追踪指定文件, 但该文件会保留在工作区$ git rm --cached [file]# 7. 改名文件, 并且将这个改名放入暂存区$ git mv [file-original] [file-renamed] 代码提交1234567891011121314151617# 1. 提交暂存区到仓库区$ git commit -m [message]# 2. 提交暂存区的指定文件到仓库区$ git commit [file1] [file2] ... -m [message]# [3]. 提交工作区自上次 commit 之后的变化, 直接到仓库区$ git commit -a# [4]. 提交时显示所有 diff 信息$ git commit -v# [5]. 使用一次新的 commit, 替代上一次提交[如果代码没有任何新变化, 则用来改写上一次 commit 的提交信息]$ git commit --amend -m [message]# 6. 重做上一次 commit, 并包括指定文件的新变化$ git commit --amend [file1] [file2] ... 分支12345678910111213141516171819202122232425262728293031323334353637383940414243# 1. 列出所有本地分支$ git branch# 2. 列出所有远程分支$ git branch -r# [3]. 列出所有本地分支和远程分支$ git branch -a# 4. 新建一个分支, 但依然停留在当前分支$ git branch [branch-name]# [5]. 新建一个分支, 并切换到该分支 p.s.[git checkout -b [branch] [remote/branch]根据远程分支，创建本地分支]$ git checkout -b [branch]$ git checkout -b [branch] [remote/branch]# 6. 新建一个分支, 指向指定 commit$ git branch [branch] [commit]# 7. 新建一个分支, 与指定的远程分支建立追踪关系$ git branch --track [branch] [remote-branch]# 8. 切换到指定分支, 并更新工作区$ git checkout [branch-name]# [9]. 切换到上一个分支$ git checkout -# 10. 建立追踪关系, 在现有分支与指定的远程分支之间$ git branch --set-upstream [branch] [remote-branch]# [11]. 合并指定分支到当前分支 p.s.[git merge --no-ff -m "message" [branch]: 合并某分支到当前分支, 可以保存你之前的分支历史]$ git merge [branch]# [12]. 选择一个 commit，合并进当前分支 p.s.[选择某一个分支中的一个或几个 commit(s)来进行操作（操作的对象是 commit）]$ git cherry-pick [commit1] [commit2] ... # [13]. 删除分支$ git branch -d [branch-name]# [14]. 删除远程分支$ git push origin --delete [branch-name]$ git branch -dr [remote/branch] 标签1234567891011121314151617181920212223242526# [1]. 列出所有 tag$ git tag# 2. 新建一个 tag 在当前 commit p.s.[git tag -a [tag] -m [message]]$ git tag [tag]# 3. 新建一个 tag 在指定 commit$ git tag [tag] [commit]# 4. 删除本地 tag$ git tag -d [tag]# [5]. 删除远程 tag$ git push origin :refs/tags/[tagName]# [6]. 查看 tag 信息$ git show [tag]# [7]. 提交指定 tag$ git push [remote] [tag]# 8. 提交所有 tag$ git push [remote] --tags# [9]. 新建一个分支, 指向某个 tag$ git checkout -b [branch] [tag] 查看信息123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# [1]. 显示有变更的文件$ git status# 2. 显示当前分支的版本历史$ git log# [3]. 显示 commit 历史, 以及每次 commit 发生变更的文件$ git log --stat# 4. 搜索提交历史, 根据关键词$ git log -S [keyword]# 5. 显示某个 commit 之后的所有变动, 每个 commit 占据一行$ git log [tag] HEAD --pretty=format:%s# 6. 显示某个 commit 之后的所有变动, 其 "提交说明" 必须符合搜索条件$ git log [tag] HEAD --grep feature# [7]. 显示某个文件的版本历史, 包括文件改名$ git log --follow [file]$ git whatchanged [file]# [8]. 显示指定文件相关的每一次 diff$ git log -p [file]# [9]. 显示过去 5 次提交$ git log -5 --pretty --oneline# [10]. 显示所有提交过的用户, 按提交次数排序$ git shortlog -sn# [11]. 显示指定文件是什么人在什么时间修改过$ git blame [file]# [12]. 显示暂存区和工作区的差异$ git diff# [13]. 显示暂存区和上一个 commit 的差异$ git diff --cached [file]# 14. 显示工作区与当前分支最新 commit 之间的差异$ git diff HEAD# [15]. 显示两次提交之间的差异$ git diff [first-branch]...[second-branch]# 16. 显示今天你写了多少行代码$ git diff --shortstat "@&#123;0 day ago&#125;"# [17]. 显示某次提交的元数据和内容变化$ git show [commit]# 18. 显示某次提交发生变化的文件$ git show --name-only [commit]# [19]. 显示某次提交时，某个文件的内容$ git show [commit]:[filename]# [20]. 显示当前分支的最近几次提交$ git reflog 远程同步123456789101112131415161718192021222324252627282930313233343536# [1]. 下载远程仓库的所有变动 p.s.[从远程 refs/heads / 命名空间复制所有分支, 并将它们存储到本地的 refs/remotes/origin / 命名空间]$ git fetch [remote] # 1.1.1 将远程仓库的 master 分支下载到本地当前 branch 中 $ git fetch orgin master # 1.1.2 比较本地的 master 分支和 origin/master 分支的差别 $ git log -p master ..origin/master # 1.1.3 最后进行合并 $ git merge origin/master # 1.2.1 从远程仓库 master 分支获取最新, 在本地建立 tmp 分支 $ git fetch origin master:tmp # 1.2.2 将当前分支和 tmp 进行对比 $ git diff tmp # 1.2.3 合并 tmp 分支到当前分支 $ git merge tmp# 2. 显示所有远程仓库$ git remote -v# 3. 显示某个远程仓库的信息$ git remote show [remote]# 4. 增加一个新的远程仓库, 并命名$ git remote add [shortname] [url]# [5]. 取回远程仓库的变化，并与本地分支合并$ git pull [remote] [branch]# [6]. 上传本地指定分支到远程仓库 $ git push [remote] [branch]# [7]. 强行推送当前分支到远程仓库, 即使有冲突 p.s.[git push -f [remote] [branch]: 强行用本地仓库覆盖远端仓库, 强制推送是非常不好的行为, 建议禁止使用这个方式]$ git push [remote] --force# 8. 推送所有分支到远程仓库$ git push [remote] --all 撤销1234567891011121314151617181920212223242526272829303132333435363738# 1. 恢复暂存区的指定文件到工作区 p.s.[撤销对工作区修改: 这个命令是以最新的存储时间节点 (add 和 commit) 为参照, 覆盖工作区对应文件 file; 这个命令改变的是工作区]$ git checkout [file]# 2. 恢复某个 commit 的指定文件到暂存区和工作区$ git checkout [commit] [file]# [3]. 恢复暂存区的所有文件到工作区, 即放弃工作区所有改动$ git checkout .# [4]. 重置暂存区的指定文件, 与上一次 commit 保持一致, 但工作区不变 p.s.[git reset HEAD &lt;filename&gt;：取消暂存某个文件]$ git reset [file]# 5. 重置暂存区与工作区, 与上一次 commit 保持一致$ git reset --hard# [6]. 重置当前分支的指针为指定 commit, 同时重置暂存区, 但工作区不变 p.s.[git reset HEAD：重置暂存区]$ git reset [commit]# [7]. 重置当前分支的 HEAD 为指定 commit, 同时重置暂存区和工作区, 与指定 commit 一致 p.s.[git reset --hard HEAD：强制恢复 git 管理的文件夹的內容及状态]$ git reset --hard [commit]# 8. 重置当前 HEAD 为指定 commit, 但保持暂存区和工作区不变$ git reset --keep [commit]# [9]. 抵消式撤销, 新建一个 commit, 用来撤销指定 commit, 后者的所有变化都将被前者抵消, 并且应用到当前分支 p.s.[git revert [commit]..HEAD: 撤销指定 commit 到当前 HEAD 之间所有的变化]$ git revert [commit]# [10]. 暂时将未提交的变化移除, 稍后再移入$ git stash # 10.1 将当前 stash 中的内容弹出, 并应用到当前分支对应的工作目录上 $ git stash pop # 10.2 查看当前 stash 中的内容 $ git stash list # 10.3 查看堆栈中最新保存的 stash 和当前目录的差异 $ git stash slow # [11]. 重置当前 HEAD 为指定 commit[hard: 强行合并 - 重置 stage 区和工作目录; soft: 软合并 - 保留工作目录, 并把重置 HEAD 所带来的新的差异放进暂存区; mixed: 混合合并 - 保留工作目录, 并清空暂存区]$ git reset [--hard|soft|mixed|merge|keep] [commit|HEAD] 参考博文[1]. 深入浅出 Git 教程[2]. 常用 Git 命令清单[3]. 图解 Git Git 在团队中的最佳实践系列 Git 在团队中的最佳实践（一）：Git 备忘清单 Git 在团队中的最佳实践（二）：如何正确使用 Git flow 工作流 Git 在团队中的最佳实践（三）：如何优雅的使用 Git？]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>安装教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis 深度探险（二）：Redis 深入之道]]></title>
    <url>%2Farchives%2Fe993c76c.html</url>
    <content type="text"><![CDATA[前言在 Redis 系列的开篇文章中，我们对 Redis 概述以及 Redis 数据结构与对象进行了详细的讨论以及了解。经过上一篇文章的阅读，相信读者已经对 Redis 的内部结构有了大致了解，接下来我们继续深入了解 Redis 内部结构。 对于 Redis，相信大家对 “Redis 的持久化有哪几种方式？”、“Redis 的数据淘汰机制？” 、“Redis 的过期键淘汰策略？” 、“Redis 订阅与发布机制？” 等面试题目都不陌生，那么本文就从常见的 Redis 面试题目出发，带领大家深入了解 Redis。 Redis 数据库结构？服务器中的数据库Redis 服务器将所有数据库都保存在服务器状态 redis.h/redisServer 结构的 db 数组中，db 数据的每一项都是一个 redis.h/redisDb 结构，每个 redisDb 结构代表一个数据库： 12345678struct redisServer &#123; // ... // 服务器的数据库数量 int dbnum; // 一个数组，保存着服务器中的所有数据库 redisDb *db; // ...&#125;; 在初始化服务器时，程序会根据服务器状态的 dbnum 属性来决定应该创建多少个数据库，dbnum 属性的值由服务器配置的 database 选项决定，默认情况下，该选项的值为 16。默认情况下，Redis 客户端的目标数据库为 0 号数据库。 在服务器内部，客户端状态 redisClient 结构的 db 属性记录了客户端当前的目标数据库，这个属性是一个指向 redisDb 结构的指针： 123456struct redisClient &#123; // ... // 记录客户端当前正在使用的数据库 redisDb *db; // ...&#125; redisClient; 客户端通过修改目标数据库指针，让它指向 redisServer.db 数组中的不同元素来切换不同的数据库。 数据库键空间Redis 是一个键值对（key-value pair）数据库服务器，服务器中的每个数据库都由一个 redis.h/redisDb 结构表示，数据库主要由 dict 和 expires 两个字典构成，其中，redisDb 结构的 dict 字典保存了数据库中的所有键值对，我们将这个字典称为键空间（key space）；而 expires 字典保存了数据库中的键的过期时间： 12345678typedef struct reddisDb &#123; // ... // 数据库键空间，保存着数据库中的所有键值对 dict *dict; // 过期时间，保存着键的过期时间 dict *expires; // ...&#125; redisDb; 因为数据库的键空间是一个字典，所以所有针对数据库的操作，比如添加一个键值对到数据库，或者从数据库中删除一个键值对，又或者在数据库中获取某个键值对等，实际上都是通过键空间字段进行操作来实现的。 当使用 Redis 命令对数据库进行读写时，服务器不仅会对键空间执行指定的读写操作，还会执行一些额外的维护操作，其中包括： 在读取一个键之后（读操作和写操作都要对键进行读取），服务器会根据键是否存在来更新服务器的键空间命中（hit）次数或者键空间不命中（miss）次数，这两个值可以在 INFO stats 命令的 keyspace_hits 属性和 keyspace_misses 属性中查看。 在读取一个键之后，服务器会更新键的 LRU（最后一次使用）时间，这个值可以用于计算键的闲置时间，使用 OBJECT idletime &lt; key &gt; 命令可以查看键 key 的闲置时间。 如果服务器在读取一个键时发现该键已经过期，那么服务器会先删除这个过期键，然后才执行余下的其它操作。 如果有客户端使用 WATCH 命令监视了某个键，那么服务器在对被监视的键进行修改之后，会将这个键标记为脏（dirty），从而让事物程序注意到这个键已经被修改过。 服务器每次修改一个键之后，都会对脏（dirty）键计数器的值增 1，这个计数器会触发服务器的持久化以及复制操作。 如果服务器开启了数据库通知功能，那么在对键进行修改之后，服务器将按配置发送相应的数据库通知。 Redis 的过期键淘汰策略？过期时间设置键的生存时间或者过期时间通过 EXPIRE 命令或者 PEXPIRE 命令，客户端可以以秒或者毫秒精度为数据库中的某个键设置生存时间（Time To Live，TTL），在经过指定的秒数或者毫秒数之后，服务器就会自动删除生存时间为 0 的键。 与 EXPIRE 命令和 PEXPIRE 命令类似，客户端可以通过 EXPIREAT 命令或者 PEXPIREAT 命令，以秒或者毫秒精度给数据库中的某个键设置过期时间（expire time）。 虽然有多种不同单位和不同形式的设置命令，但实际上 EXPIRE、PEXPIRE、EXPIREAT 三个命令都是使用 PEXPIREAT 命令来实现的：无论客户端执行的是以上四个命令中的哪一个，经过转换之后，最终的执行效果都和执行 PEXPIREAT 命令一样。 保存过期时间redisDb 结构的 expires 字典保存了数据中所有键的过期时间，我们称这个字典为过期字典： 过期字典的键是一个指针，这个指针指向键空间中的某个键对象（也即是某个数据库键）。 过期字典的值是一个 long 类型的整数，这个整数保存了键所指向的数据库键的过期时间——一个毫秒精度的 UNIX 时间戳。 12345678typedef struct redisDb &#123; // ... // 数据库键空间，保存着数据库中的所有键值对 dict *dict; // 过期时间，保存着键的过期时间 dict *expires; // ...&#125; redisDb; 过期键删除策略定时删除在设置键的过期时间的同时，创建一个定时器（timer），让定时器在键的过期时间来临时，立即执行对键的删除操作。（主动删除） 优点：对内存是最友好的，通过使用定时器，定时删除策略可以保证过期键会尽可能快地被删除，并释放过期键所占用的内存。 缺点：对 CPU 时间是最不友好的，在过期键比较多的情况下，删除过期键这一行为可能会占用相当一部分的 CPU 时间，在内存不紧张但是 CPU 时间非常紧张的情况下，将 CPU 时间用在删除和当前任务无关的过期键上，无疑会对服务器的相应时间和吞吐量造成影响。 惰性删除放任键过期不管，但是每次从键空间中获取键时，都检查取得的键是否过期，如果过期的话，就删除该键；如果没有过期，就返回该键。（被动删除） 优点：对 CPU 时间来说是最友好的，程序只会在取出键时才对键进行过期检查，这可以保证删除过期键的操作只会在非做不可的情况下进行，并且删除的目标仅限于当前处理的键，这个策略不会在删除其他无关的过期键上花费任何 CPU 时间。 缺点：对内存不友好，如果一个键已经过期，而这个键又仍然保留在数据库中，那么只要这个过期不被删除，它所占用的内存就不会释放。 定期删除每隔一段时间，程序就对数据库进行一次检查，删除里面的过期键。至于要删除多少过期键，以及要检查多少个数据库，则由算法决定。（主动删除） 优点：定期删除策略是定时删除和惰性删除两种策略的一种整合和折中，通过限制删除操作执行的时长和频率来减少删除操作对 CPU 时间的影响；通过定期删除过期键，有效减少了因为过期键而带来的内存浪费。 缺点：定期删除策略的难点是确定删除操作执行的时长和频率，如果删除操作执行得太频繁或者执行的时间太长，定期删除策略就会退化成定时删除策略，以至于将 CPU 时间过多地消耗在删除过期键上面；如果删除操作执行的得太少或者执行得时间太短，定期删除策略又会和惰性删除策略一样，出现浪费内存的情况。 Redis 的过期键删除策略Redis 服务器实际使用的是惰性删除和定期删除两种策略（定期删除是集中处理，惰性删除是零散处理）：通过配合使用这两种删除策略，服务器可以很好地合理使用 CPU 时间和避免浪费内存空间之间取得平衡 惰性删除策略的实现：过期键的惰性删除策略由 db.c/expireIfNeeded 函数实现，所有读写数据库的 Redis 命令在执行之前都会调用 expireIfNeeded 函数对输入键进行检查。expireIfNeeded 函数就像一个过滤器，它可以在命令真正执行之前，过滤掉过期的输入键，从而避免命令接触到过期键。 定期删除策略的实现：过期键的定期删除策略由 redis.c/activeExpireCycle 函数实现，每当 Redis 的服务器周期性操作 redis.c/serverCron 函数执行时，activeExpireCycle 函数就会被调用，它在规定的时间内，分多次遍历服务器中的各个数据库（默认每次检查的数据库数量为 16），从数据库的 expire 字典中随机检查一部分键（默认每个数据库检查的键数量为 20）的过期时间，并删除其中的过期键。 AOF、RDB 和复制功能对过期键的处理 执行 SAVE 命令或者 BGSAVE 命令所产生的新 RDB 文件不会包含已经过期的键。 执行 BGREWRITEAOF 命令所产生的重写 AOF 文件不会包含已经过期的键。 当一个过期键被删除之后，服务器会追加一条 DEL 命令到现有 AOF 文件的末尾，显式地删除过期键。 当主服务器删除一个过期键之后，它会向所有从服务器发送一条 DEL 命令，显式地删除过期键。 从服务器即使发现过期键也不会自作主张地删除它，而是等待主节点发来 DEL 命令，这种统一、中心化的过期键删除策略可以保证主从服务器数据的一致性。 当 Redis 命令对数据库进行修改之后，服务器会根据配置向客户端发送数据库通知。 Redis 的数据淘汰机制？Redis 配置文件中可以使用 maxmemory&lt; bytes &gt; 将内存使用限制设置为指定的字节数。当达到内存限制时，Redis 会根据选择的淘汰策略来删除键。这样可以减少内存紧张的情况，由此获取更为稳健的服务。Redis 内存数据集大小上升到一定大小的时候，就会施行数据淘汰策略。 Redis 中当内存超过限制时，按照配置的策略，淘汰掉相应的 kv，使得内存可以继续留有足够的空间保存新的数据。Redis 确定驱逐某个键值对后，会删除这个数据，并将这个数据变更消息发布到本地（AOF 持久化）和从机（主从连接）。 缓存淘汰算法FIFO（First In First Out，先进先出算法） 一种比较容易实现的算法。它的思想是先进先出（FIFO，队列），这是最简单、最公平的一种思想，即如果一个数据是最先进入的，那么可以认为在将来它被访问的可能性很小。空间满的时候，最先进入的数据最先被置换（淘汰）。 LRU（Least Recently Used， 最近最少使用算法 ）是一种常见的缓存算法，在很多分布式缓存系统（如 Redis、Memcached）中都有广泛使用。LRU 算法的思想是：如果一个数据在最近一段时间没有被访问到，那么可以认为在将来它被访问的可能性也很小。因此，当空间满时，最久没有访问的数据最先被置换（淘汰）。 LFU（Least Frequently Used ， 最不经常使用算法）也是一种常见的缓存算法。LFU 算法的思想是：如果一个数据在最近一段时间很少被访问到，那么可以认为在将来它被访问的可能性也很小。因此，当空间满时，最小频率访问的数据最先被置换（淘汰）。 Redis 提供 6 种数据淘汰策略我们在该系列的上一篇文章中了解到，redisobject 中除了 type、encoding、ptr 和 refcount 属性外，还有一个 lru 属性用来计算空转时长。OBJECT IDLETIME 命令可以打印出给定键的空转时长，是用当前时间减去键的 lru 时间计算得出的。OBJECT IDLETIME 命令是特殊的，这个命令在访问键的对象时，不会修改值对象的 lru 属性。 键的空转时长还有一个作用，如果服务器打开了 maxmemory 选项，并且服务器用于回收内存的算法是 volatile-lru 或者 allkeys-lru，那么当服务器占用的内存数超过了 maxmemory 选项所设置的上限值时，空转时长较高的那部分键会优先被服务器释放，从而回收内存。 volatile-lru : 从已设置过期时间的数据集 (server.db[i].expires) 中挑选最近最少使用的数据淘汰。（推荐） volatile-ttl : 从已设置过期时间的数据集 (server.db[i].expires) 中挑选将要过期的数据淘汰。 volatile-random : 从已设置过期时间的数据集 (server.db[i].expires) 中任意选择数据淘汰。 allkeys-lru : 从数据集 (server.db[i].dict) 中挑选最近最少使用的数据淘汰。（一般推荐） allkeys-random : 从数据集 (server.db[i].dict) 中任意选择数据淘汰。 no-enviction：不会继续服务写请求 (DEL 请求可以继续服务)，读请求可以继续进行。这样可以保证不会丢失数据，但是会让线上的业务不能持续进行。（默认） Redis 4.0 版本后增加以下两种： volatile-lfu：从已设置过期时间的数据集 (server.db[i].expires) 中挑选最不经常使用的数据淘汰。 allkeys-lfu：从数据集 (server.db[i].dict) 中挑选最不经常使用的数据淘汰。 在 Redis 中 LRU 算法是一个近似算法，默认情况下，Redis 随机挑选 5 个键，并且从中选取一个最近最久未使用的 key 进行淘汰，在配置文件中可以通过 maxmemory-samples 的值来设置 redis 需要检查 key 的个数，但是检查的越多，耗费的时间也就越久，结构越精确 (也就是 Redis 从内存中淘汰的对象未使用的时间也就越久)，设置多少，综合权衡。 对于具体的数据淘汰机制以及数据淘汰策略，大家可以阅读 Redis 配置文件 redis.conf 中有相关注释。 Redis 的持久化有哪几种方式？因为 Redis 是内存数据库，它将自己的数据库状态储存在内存里面，所以如果不想办法将储存在内存的数据库状态保存至磁盘里面，那么一旦服务器进程退出，服务器中的数据库状态也会消失不见。 为了解决这个问题，Redis 提供了 RDB（Redis DataBase） 持久化功能，这个功能可以将 Redis 在内存中的数据库状态保存到磁盘里面，避免数据意外丢失。 除了 RDB 持久化功能之外，Redis 还提供了 AOF（Append Only File）持久化功能。与 RDB 持久化通过保存数据库中的键值对来记录数据库状态不同，AOF 持久化是通过保存 Redis 服务器所执行的写命令来记录数据库状态的。 RDB（Redis DataBase）RDB（Redis DataBase） 是 Redis 默认的持久化方案。在指定的时间间隔内，执行指定次数的写操作，则会将内存中的数据写入到磁盘中。即在指定目录下生成一个 dump.rdb 文件，Redis 重启会通过加载 dump.rdb 文件恢复数据。 RDB 文件是一个经过压缩的二进制文件，由多个部分组成，用于保存和还原 Redis 服务器所有数据库中的所有键值对数据。对于不同类型的键值对，RDB 文件会使用不同的方式来保存它们。 RDB 文件的创建和载入有两个 Redis 命令可以用于生成 RDB 文件，一个是 SAVE，另一个是 BGSAVE。 SAVE 命令有服务器进程直接执行保存操作，因此 SAVE 命令会阻塞 Redis 服务器进程，直到 RDB 文件创建完毕为止，在服务器进程阻塞期间，服务器不能处理任何命令请求。 BGSAVE 命令由子进程执行保存操作，BGSAVE 命令会派生（fork）出一个子进程，然后由子进程负责创建 RDB 文件，服务器进程（父进程）继续处理命令请求，所以该命令不会阻塞服务器。 服务器在载入 RDB 文件期间，会一直处于阻塞状态，直到载入工作完成为止。 自动间隔性保存当 Redis 服务器启动时，用户可以通过指定配置文件或者传入启动参数的方式设置 save 选项，如果用户没有主动设置 save 选项，那么服务器就会为 save 选项设置默认条件： 123456// 服务器在 900 秒（15 分钟）之内，对数据库进行了至少 1 次修改。save 900 1// 服务器在 300 秒（5 分钟）之内，对数据库进行了至少 10 次修改。save 300 10// 服务器在 60 秒（1 分钟）之内，对数据库进行了至少 10000 次修改。save 60 10000 以上三个条件中的任意一个满足，BGSAVE 命令就会被执行。Redis 的服务器周期性操作函数 serverCron 默认每隔 100 毫秒就会执行一次，该函数用于对正在运行的服务器进行维护，它的其中一项工作就是检查 save 选项所设置的保存条件是否已经满足，如果满足的话，就执行 BGSAVE 命令。 AOF（Append Only File）AOF（Append Only File）在 Redis 中默认不开启（appendonly no）， 默认是每秒将写操作日志追加到 AOF 文件中，它的出现是为了弥补 RDB 的不足（数据的不一致性），所以它采用日志的形式来记录每个写操作，并追加到文件中。Redis 重启的时候会根据日志文件 appendonly.aof 的内容将写指令从前到后执行一次以完成数据的恢复工作。 AOF 文件中的所有命令都以 Redis 命令请求协议的格式保存，请求命令会先保存到 AOF 缓冲区里面，之后再定期写入并同步到 AOF 文件。 如果服务器开启了 AOF 持久化功能，那么服务器会优先使用 AOF 文件来还原数据库状态。只有在 AOF 持久化功能处于关闭状态时，服务器才会使用 RDB 文件来还原数据库状态。 AOF 文件的载入与数据还原因为 AOF 文件里面包含了重建数据库状态所需的所有写命令，所以服务器只要读入并重新执行一遍 AOF 文件里面保存的写命令，就可以还原服务器关闭之前的数据库状态。 AOF 重写因为 AOF 持久化是通过保存被执行的写命令来记录数据库状态的，所以随着服务器运行时间的流逝，AOF 文件的体积也会越来越大，如果不加以控制的话，体积过大的 AOF 文件很可能对 Redis 服务器、甚至整个宿主计算机造成影响，并且 AOF 文件的体积越大，使用 AOF 文件来进行数据还原所需的时间就越多。 为了解决 AOF 文件体积膨胀的问题，Redis 提供了 AOF 文件重写（rewrite）功能。AOF 重写是一个有歧义的名字，该功能是通过读取数据库中的键值对来实现的，程序无须对现有的 AOF 文件进行任何读取、分析或者写入操作。通过该功能，Redis 服务器可以创建一个新的 AOF 文件来替代现有的 AOF 文件，新旧两个 AOF 文件所保存的数据库状态相同，但新的 AOF 文件不会包含任何浪费空间的冗余命令，所以新的 AOF 文件的体积通常会比旧的 AOF 文件体积要小得多。 在执行 BGREWRITEAOF 命令时，Redis 服务器会维护一个 AOF 重写缓冲区，该缓冲区会在子进程创建新 AOF 文件期间，记录服务器执行的所有写命令。当子进程完成创建新 AOF 文件的工作之后，服务器会将重写缓冲区中的所有内容追加到新的 AOF 文件的末尾，使得新旧两个 AOF 文件所保存的数据库状态一致。最后，服务器用新的 AOF 文件替换旧的 AOF 文件，以此来完成 AOF 文件重写操作。 Redis 4.0 对于持久化机制的优化Redis 4.0 开始支持 RDB 和 AOF 的混合持久化（默认关闭，可以通过配置项 aof-use-rdb-preamble 开启）。如果把混合持久化打开，AOF 重写的时候就直接把 RDB 的内容写到 AOF 文件开头。这样做的好处是可以结合 RDB 和 AOF 的优点，快速加载同时避免丢失过多的数据。当然缺点也是有的， AOF 里面的 RDB 部分是压缩格式不再是 AOF 格式，可读性较差。 Redis 订阅（subscribe）与发布（publish）机制？Redis 的发布和订阅功能由 PUBLIST、SUBSCRIBE、PSUBSCRIBE 等命令组成。通过执行 SUBSCRIBE 命令，客户端可以订阅一个或多个频道，从而你成为这些频道的订阅者（subscriber）：每当有其它客户端向被订阅的频道发送消息（message）时，频道的所有订阅者都会收到这条消息。除了订阅频道之外，客户端还可以通过执行 PSUBSCRIBE 命令订阅一个或多个模式，从而成为这些模式的订阅者：每当有其它客户端向某个频道发送消息时，消息不仅会被发送给这个频道的所有订阅者，它还会被发送给所有与这个频道相匹配的模式的订阅者。 服务器状态在 pubsub_channels 字典保存了所有频道的订阅关系， 字典的键为被订阅的频道，字典的值为订阅频道的所有客户端：SUBSCRIBE 命令负责将客户端和被订阅的频道关联到这个字典里面，而 UNSUBSCRIBE 命令则负责解除客户端和被退订频道之间的关联。 当有新消息发送到频道时，程序遍历频道（键）所对应的（值）所有客户端，然后将消息发送到所有订阅频道的客户端上。 服务器状态在 pubsub_patterns 链表保存了所有模式的订阅关系，链表的每个节点都保存着一个 pubsubPattern 结构，结构中保存着被订阅的模式，以及订阅该模式的客户端：PSBUSCRIBE 命令负责将客户端和被订阅的模式记录到这个链表中，而 PUNSUBSCRIBE 命令则负责移除客户端和被退订模式在链表中的记录。 程序通过遍历链表来查找某个频道是否和某个模式匹配。 PUBLISH 命令通过访问 pubsub_channels 字典在向频道的所有订阅者发送消息，通过访问 pubsub_patterns 链表来向所有匹配频道的模式的订阅者发送消息。 PUBSUB 命令的三个子命令都是通过读取 pubsub_channels 字典和 pubsub_patterns 链表中的信息来实现的。 12345678struct redisServer &#123; // ... // 保存所有频道订阅关系 dict *pubsub_channels; // 保存所有模式订阅关系 list *pubsub_patterns; // ...&#125;; 参考博文[1]. 《Redis 设计与实现》，第二部分 单机数据库的实现[2]. 分布式之数据库和缓存双写一致性方案解析[3]. Redis 配置文件 redis.conf Redis 深度探险系列 Redis 深度探险（一）：那些绕不过去的 Redis 知识点 Redis 深度探险（二）：Redis 深入之道 Redis 深度探险（三）：Redis 单机环境搭建以及配置说明 Redis 深度探险（四）：Redis 高可用性解决方案之哨兵与集群]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>发布与订阅</tag>
        <tag>持久化</tag>
        <tag>数据淘汰</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 并发编程之美（四）：深入剖析 ThreadLocal]]></title>
    <url>%2Farchives%2Fb9c60c0e.html</url>
    <content type="text"><![CDATA[前言想必很多朋友对 ThreadLocal 并不陌生，而本人也在项目中应用到了 ThreadLocal，今天我们就来一起探讨下 ThreadLocal 的使用方法和实现原理。首先，本文先谈一下对 ThreadLocal 的理解，然后根据 ThreadLocal 类的源码分析了其实现原理和使用需要注意的地方，最后给出了 ThreadLocal 应用场景。 对 ThreadLocal 的理解ThreadLocal 为解决多线程的并发问题提供了一种新的思路。ThreadLocal，顾名思义是线程的一个本地化对象，当工作于多线程中的对象使用 ThreadLocal 维护变量时，ThreadLocal 为每个使用该变量的线程分配一个独立的变量副本，所以每一个线程都可以独立的改变自己的副本，而不影响其他线程所对应的副本。从线程的角度看，这个变量就像是线程的本地变量。 ThreadLocal 是一个本地线程副本变量工具类。主要用于将私有线程和该线程存放的副本对象做一个映射，各个线程之间的变量互不干扰，在高并发场景下，可以实现无状态的调用，特别适用于各个线程依赖不同的变量值完成操作的场景。 从上面的结构图，我们已经窥见 ThreadLocal 的核心机制： 每个 Thread 线程内部都有一个 Map。 Map 里面存储线程本地对象（key）和线程的变量副本（value） 但是，Thread 内部的 Map 是由 ThreadLocal 维护的，由 ThreadLocal 负责向 Map 获取和设置线程的变量值。 代码示例： 12345678910111213141516171819202122class Scratch &#123; // 创建一个 String 型的线程本地变量, 设置初始值 "Hello World!" public static final ThreadLocal&lt;String&gt; localVariable = ThreadLocal.withInitial(() -&gt; "Hello World!"); public static void main(String[] args) &#123; Thread threadOne = new Thread(() -&gt; &#123; localVariable.set("I'm variable in threadOne"); System.out.println(Thread.currentThread().getName() + ":" + localVariable.get()); &#125;); Thread threadTwo = new Thread(() -&gt; &#123; localVariable.set("I'm variable in threadTwo"); System.out.println(Thread.currentThread().getName() + ":" + localVariable.get()); &#125;); threadOne.start(); threadTwo.start(); &#125;&#125;// Thread-0:I'm variable in threadOne// Thread-1:I'm variable in threadTwo 虽然程序里面是操作的同一个变量 localVariable，但是不同线程都有自己的一份拷贝。 深入解析 ThreadLocal 类先了解一下 ThreadLocal 类提供的几个方法： 1234567891011// get() 方法用于获取当前线程的副本变量值public T get()// set() 方法用于保存当前线程的副本变量值public void set(T value)// remove() 方法移除当前前程的副本变量值public void remove()// initialValue() 为当前线程初始副本变量值 [一个 protected 方法，一般是用来在使用时进行重写的]protected T initialValue() public T get()123456789101112131415161718192021222324252627282930313233public T get() &#123; Thread t = Thread.currentThread(); // 1. 获取当前线程的 ThreadLocalMap 对象 threadLocals ThreadLocalMap map = getMap(t); if (map != null) &#123; // 2. 从 map 中获取线程存储的 K-V Entry 节点 ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) // 3. 从 Entry 节点获取存储的 Value 副本值返回. return (T) e.value; &#125; // 4.map 为空的话返回初始值 null, 即线程变量副本为 null, 在使用时需要注意判断 NullPointerException. return setInitialValue();&#125;ThreadLocalMap getMap(Thread t) &#123; return t.threadLocals;&#125;private T setInitialValue() &#123; T value = initialValue(); Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); return value;&#125;protected T initialValue() &#123; return null;&#125; public void set(T value)12345678910111213141516171819public void set(T value) &#123; Thread t = Thread.currentThread(); // 1. 获取当前线程的 ThreadLocalMap 对象 threadLocals ThreadLocalMap map = getMap(t); if (map != null) // 2.map 非空, 则重新将 ThreadLocal 和新的 value 副本放入到 map 中. map.set(this, value); else // 3.map 空, 则对线程的成员变量 ThreadLocalMap 进行初始化创建, 并将 ThreadLocal 和 value 副本放入 map 中. createMap(t, value);&#125;ThreadLocalMap getMap(Thread t) &#123; return t.threadLocals;&#125;void createMap(Thread t, T firstValue) &#123; t.threadLocals = new ThreadLocalMap(this, firstValue);&#125; public void remove()1234567891011public void remove() &#123; // 1. 获取当前线程的 ThreadLocalMap 对象 threadLocals ThreadLocalMap m = getMap(Thread.currentThread()); if (m != null) // 2. 从 map 中删除该 K-V Entry 节点 m.remove(this);&#125;ThreadLocalMap getMap(Thread t) &#123; return t.threadLocals;&#125; Hash 冲突怎么解决？和 HashMap 的最大的不同在于，ThreadLocalMap 结构非常简单，没有 next 引用，也就是说 ThreadLocalMap 中解决 Hash 冲突的方式并非链表的方式，而是采用线性探测（开放寻址法）的方式，所谓线性探测，就是根据初始 Key 的 Hashcode 值确定元素在 table 数组中的位置，如果发现这个位置上已经有其他 Key 值的元素被占用，则利用固定的算法寻找一定步长的下个位置，依次判断，直至找到能够存放的位置。 ThreadLocalMap 解决 Hash 冲突的方式就是简单的步长加 1 或减 1，寻找下一个相邻的位置。 12345678910111213/** * Increment i modulo len. */private static int nextIndex(int i, int len) &#123; return ((i + 1 &lt; len) ? i + 1 : 0);&#125;/** * Decrement i modulo len. */private static int prevIndex(int i, int len) &#123; return ((i - 1 &gt;= 0) ? i - 1 : len - 1);&#125; 显然 ThreadLocalMap 采用线性探测的方式解决 Hash 冲突的效率很低，如果有大量不同的 ThreadLocal 对象放入 Map 中时发生冲突，或者发生二次冲突，则效率很低。 所以这里引出的良好建议是：每个线程只存一个变量，这样的话所有的线程存放到 Map 中的 Key 都是相同的 ThreadLocal，如果一个线程要保存多个变量，就需要创建多个 ThreadLocal，多个 ThreadLocal 放入 Map 中时会极大的增加 Hash 冲突的可能。 如何避免泄漏ThreadLocalMap 原理 ThreadLocal 的原理：每个 Thread 内部维护着一个 ThreadLocalMap（初始容量 16，负载因子 2/3，解决冲突的方法是再 hash 法），它是一个 Map。这个映射表的 Key 是一个弱引用，其实就是 ThreadLocal 本身，Value 是真正存的线程变量 Object。也就是说 ThreadLocal 本身并不真正存储线程的变量值，它只是一个工具，用来维护 Thread 内部的 Map，帮助存和取。注意上图的虚线，它代表一个弱引用类型，而弱引用的生命周期只能存活到下次 GC 前。 ThreadLocal 为什么会内存泄漏？ThreadLocal 在 ThreadLocalMap 中是以一个弱引用身份被 Entry 中的 Key 引用的，因此如果 ThreadLocal 没有外部强引用来引用它，那么 ThreadLocal 会在下次 JVM 垃圾收集时被回收。这个时候就会出现 Entry 中 Key 已经被回收，出现一个 null Key 的情况，外部读取 ThreadLocalMap 中的元素是无法通过 null Key 来找到 Value 的。因此如果当前线程的生命周期很长，一直存在，那么其内部的 ThreadLocalMap 对象也一直生存下来，这些 null Key 就存在一条强引用链的关系一直存在：Thread–&gt;ThreadLocalMap–&gt;Entry–&gt;Value，这条强引用链会导致 Entry 不会回收，Value 也不会回收，但 Entry 中的 Key 却已经被回收的情况，造成内存泄漏。 但是 JVM 团队已经考虑到这样的情况，并做了一些措施来保证 ThreadLocal 尽量不会内存泄漏：在 ThreadLocal 的 get()、set()、remove() 方法调用的时候会清除掉线程 ThreadLocalMap 中所有 Entry 中 Key 为 null 的 Value，并将整个 Entry 设置为 null，利于下次内存回收。 但这样也并不能保证 ThreadLocal 不会发生内存泄漏，例如：使用 static 的 ThreadLocal，延长了 ThreadLocal 的生命周期，可能导致的内存泄漏。分配使用了 ThreadLocal 又不再调用 get()、set()、remove() 方法，那么就会导致内存泄漏。 ThreadLocal 如何避免泄漏？既然 Key 是弱引用，那么我们要做的事，就是在调用 ThreadLocal 的 get()、set() 方法时完成后再调用 remove() 方法，将 Entry 节点和 Map 的引用关系移除，这样整个 Entry 对象在 GCRoots 分析后就变成不可达了，下次 GC 的时候就可以被回收。如果使用 ThreadLocal 的 set() 方法之后，没有显示的调用 remove() 方法，就有可能发生内存泄露，所以养成良好的编程习惯十分重要，使用完 ThreadLocal 之后，记得调用 remove() 方法。 在使用线程池的情况下，没有及时清理 ThreadLocal，不仅是内存泄漏的问题，更严重的是可能导致业务逻辑出现问题。所以，使用 ThreadLocal 就跟加锁完要解锁一样，用完就清理。 应用场景ThreadLocal 适用于如下两种场景： 每个线程需要有自己单独的实例 实例需要在多个方法中共享，但不希望被多线程共享 对于第一点，每个线程拥有自己实例，实现它的方式很多。例如可以在线程内部构建一个单独的实例。ThreadLocal 可以以非常方便的形式满足该需求。 对于第二点，可以在满足第一点（每个线程有自己的实例）的条件下，通过方法间引用传递的形式实现。ThreadLocal 使得代码耦合度更低，且实现更优雅。 ThreaLocal 的 JDK 文档中说明：ThreadLocal instances are typically private static fields in classes that wish to associate state with a thread。如果我们希望通过某个类将状态（例如用户 ID、事务 ID）与线程关联起来，那么通常在这个类中定义 private static 类型的 ThreadLocal 实例。 1234567891011121314151617181920212223242526272829303132public class ThreadContext &#123; private String userId; private Long transactionId; private static ThreadLocal&lt;ThreadContext&gt; threadLocal = ThreadLocal.withInitial(ThreadContext::new); public static ThreadContext getThreadContext() &#123; return threadLocal.get(); &#125; public static void removeThreadContext() &#123; threadLocal.remove(); &#125; public String getUserId() &#123; return userId; &#125; public void setUserId(String userId) &#123; this.userId = userId; &#125; public Long getTransactionId() &#123; return transactionId; &#125; public void setTransactionId(Long transactionId) &#123; this.transactionId = transactionId; &#125;&#125; ThreadLocal 的作用是提供线程内的局部变量，这种变量在线程的生命周期内起作用。作用：提供一个线程内公共变量（比如本次请求的用户信息），减少同一个线程内多个函数或者组件之间一些公共变量的传递的复杂度，或者为线程提供一个私有的变量副本，这样每一个线程都可以随意修改自己的变量副本，而不会对其他线程产生影响。 还记得 Hibernate 的 session 获取场景吗？ 123456789101112131415161718192021private static final ThreadLocal&lt;Session&gt; threadLocal = new ThreadLocal&lt;Session&gt;();// 获取 Sessionpublic static Session getCurrentSession() &#123; Session session = threadLocal.get(); // 判断 Session 是否为空, 如果为空, 将创建一个 session, 并设置到本地线程变量中 try &#123; if (session == null &amp;&amp; !session.isOpen()) &#123; if (sessionFactory == null) &#123; rbuildSessionFactory();// 创建 Hibernate 的 SessionFactory &#125; else &#123; session = sessionFactory.openSession(); &#125; &#125; threadLocal.set(session); &#125; catch (Exception e) &#123; // TODO: handle exception &#125; return session;&#125; 为什么？每个线程访问数据库都应当是一个独立的 Session 会话，如果多个线程共享同一个 Session 会话，有可能其他线程关闭连接了，当前线程再执行提交时就会出现会话已关闭的异常，导致系统异常。此方式能避免线程争抢 Session，提高并发下的安全性。使用 ThreadLocal 的典型场景正如上面的数据库连接管理，线程会话管理等场景，只适用于独立变量副本的情况，如果变量为全局共享的，则不适用在高并发下使用。 总结 ThreadLocal 并不解决线程间共享数据的问题 ThreadLocal 通过隐式的在不同线程内创建独立实例副本避免了实例线程安全的问题 每个线程持有一个 Map 并维护了 ThreadLocal 对象与具体实例的映射，该 Map 由于只被持有它的线程访问，故不存在线程安全以及锁的问题 ThreadLocalMap 的 Entry 对 ThreadLocal 的引用为弱引用，避免了 ThreadLocal 对象无法被回收的问题 ThreadLocalMap 的 set 方法通过调用 replaceStaleEntry 方法回收键为 null 的 Entry 对象的值（即为具体实例）以及 Entry 对象本身从而防止内存泄漏 ThreadLocal 适用于变量在线程间隔离且在方法间共享的场景 参考博文[1]. ThreadLocal - 面试必问深度解析[2]. 谈谈 Java 中的 ThreadLocal[3]. Java 进阶（七）正确理解 Thread Local 的原理与适用场景 Java 并发编程之美系列 Java 并发编程之美（一）：并发队列 Queue 原理剖析 Java 并发编程之美（二）：线程池 ThreadPoolExecutor 原理探究 Java 并发编程之美（三）：异步执行框架 Eexecutor Java 并发编程之美（四）：深入剖析 ThreadLocal Java 并发编程之美（五）：揭开 InheritableThreadLocal 的面纱 Java 并发编程之美（六）：J.U.C 之线程同步辅助工具类]]></content>
      <categories>
        <category>并发</category>
      </categories>
      <tags>
        <tag>ThreadLocal</tag>
        <tag>内存泄漏</tag>
        <tag>哈希冲突</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入浅出集合框架（一）：HashMap 的底层实现原理]]></title>
    <url>%2Farchives%2Fb0f424b6.html</url>
    <content type="text"><![CDATA[前言哈希表（hash table）也叫散列表，是一种非常重要的数据结构，应用场景及其丰富，许多缓存技术（比如 memcached）的核心其实就是在内存中维护一张大的哈希表，而 HashMap 的实现原理也常常出现在各类的面试题中，重要性可见一斑。随着 JDK（Java Developmet Kit）版本的更新，JDK 1.8 对 HashMap 底层的实现进行了优化，例如引入红黑树的数据结构和扩容的优化等。本文结合 JDK 1.7 和 JDK 1.8 的区别，深入探讨 HashMap 的结构实现和功能原理。 什么是哈希表？散列表（Hash table，也叫哈希表），是根据关键码值（Key value）而直接进行访问的数据结构。也就是说，它通过把关键码值映射到表中一个位置来访问记录，以加快查找的速度。这个映射函数叫做散列函数，存放记录的数组叫做散列表。 给定表 M，存在函数 f(key)，对任意给定的关键字值 key，代入函数后若能得到包含该关键字的记录在表中的地址，则称表 M 为哈希 (Hash）表，函数 f(key) 为哈希 (Hash) 函数。 数据结构在讨论哈希表之前，我们先大概了解下其他数据结构在新增，查找等基础操作执行性能。 数组：采用一段连续的存储单元来存储数据。对于指定下标的查找，时间复杂度为 O(1)；通过给定值进行查找，需要遍历数组，逐一比对给定关键字和数组元素，时间复杂度为 O(n)，当然，对于有序数组，则可采用二分查找，插值查找，斐波那契查找等方式，可将查找复杂度提高为 O(logn)；对于一般的插入删除操作，涉及到数组元素的移动，其平均复杂度也为 O(n)。 线性链表：对于链表的新增，删除等操作（在找到指定操作位置后），仅需处理结点间的引用即可，时间复杂度为 O(1)，而查找操作需要遍历链表逐一进行比对，复杂度为 O(n)。 二叉树：对一棵相对平衡的有序二叉树，对其进行插入，查找，删除等操作，平均复杂度均为 O(logn)。 哈希表：哈希表中进行添加，删除，查找等操作，性能十分之高，不考虑哈希冲突的情况下，仅需一次定位即可完成，时间复杂度为 O(1)，接下来我们就来看看哈希表是如何实现达到惊艳的常数阶 O(1)的。 物理存储结构我们知道，数据结构的物理存储结构只有两种：顺序存储结构（数组存储）和链式存储结构（链表存储）。 数组存储区间是连续的，占用内存严重，故空间复杂的很大。但数组的二分查找时间复杂度小，为 O(1)；数组的特点是：寻址容易，插入和删除困难； 链表存储区间离散，占用内存比较宽松，故空间复杂度很小，但时间复杂度很大，达 O(n)。链表的特点是：寻址困难，插入和删除容易。 综合这两者的优点，摒弃缺点，哈希表就诞生了，既满足了数据查找方面的特点，占用的空间也不大。比如我们要新增或查找某个元素，我们通过把当前元素的关键字通过某个函数映射到数组中的某个位置，通过数组下标一次定位就可完成操作。 哈希冲突然而万事无完美，如果两个不同的元素，通过哈希函数得出的实际存储地址相同怎么办？也就是说，当我们对某个元素进行哈希运算，得到一个存储地址，然后要进行插入的时候，发现已经被其他元素占用了，其实这就是所谓的哈希冲突，也叫哈希碰撞。 数组是一块连续的固定长度的内存空间，再好的哈希函数也不能保证得到的存储地址绝对不发生冲突。那么哈希冲突如何解决呢？哈希冲突的解决方案有多种： 开放定址法（当发生地址冲突的时候，按照某种方法继续探测哈希表中的其他存储单元，直到找到空位置为止） 再散列函数法（同时构造多个不同的哈希函数，当发生冲突时，使用第二个、第三个…… 哈希函数计算地址，直到无冲突时） 链地址法（将所有关键字为同义词的记录存储在同一线性链表中） 建立公共溢出区（将哈希表分为基本表和溢出表两部分，凡是和基本表发生冲突的元素，一律填入溢出表） HashMap 即是采用了链地址法，也就是数组 + 链表的方式。 HashMap 实现原理Java 为数据结构中的映射定义了一个接口 java.util.Map，此接口主要有四个常用的实现类，分别是 HashMap、Hashtable、LinkedHashMap 和 TreeMap，类继承关系如下图所示： 下面针对各个实现类的特点做一些说明： (1) HashMap：它根据键的 hashCode 值存储数据，大多数情况下可以直接定位到它的值，因而具有很快的访问速度，但遍历顺序却是不确定的。 HashMap 最多只允许一条记录的键为 null，允许多条记录的值为 null。HashMap 非线程安全，即任一时刻可以有多个线程同时写 HashMap，可能会导致数据的不一致。如果需要满足线程安全，可以用 Collections 的 synchronizedMap 方法使 HashMap 具有线程安全的能力，或者使用 ConcurrentHashMap。 (2) Hashtable：Hashtable 是遗留类，很多映射的常用功能与 HashMap 类似，不同的是它承自 Dictionary 类，并且是线程安全的，任一时间只有一个线程能写 Hashtable，并发性不如 ConcurrentHashMap，因为 ConcurrentHashMap 引入了分段锁。Hashtable 不建议在新代码中使用，不需要线程安全的场合可以用 HashMap 替换，需要线程安全的场合可以用 ConcurrentHashMap 替换。 (3) LinkedHashMap：LinkedHashMap 是 HashMap 的一个子类，保存了记录的插入顺序，在用 Iterator 遍历 LinkedHashMap 时，先得到的记录肯定是先插入的，也可以在构造时带参数，按照访问次序排序。 (4) TreeMap：TreeMap 实现 SortedMap 接口，能够把它保存的记录根据键排序，默认是按键值的升序排序，也可以指定排序的比较器，当用 Iterator 遍历 TreeMap 时，得到的记录是排过序的。如果使用排序的映射，建议使用 TreeMap。在使用 TreeMap 时，key 必须实现 Comparable 接口或者在构造 TreeMap 传入自定义的 Comparator，否则会在运行时抛出 java.lang.ClassCastException 类型的异常。 对于上述四种 Map 类型的类，要求映射中的 key 是不可变对象。不可变对象是该对象在创建后它的哈希值不会被改变。如果对象的哈希值发生变化，Map 对象很可能就定位不到映射的位置了。 通过上面的比较，我们知道了 HashMap 是 Java 的 Map 家族中一个普通成员，鉴于它可以满足大多数场景的使用条件，所以是使用频度最高的一个。下文我们主要结合源码，从存储结构、常用方法分析、扩容以及安全性等方面深入讲解 HashMap 的工作原理。 HashMap 存储结构在 JDK1.6、JDK 1.7 中，HashMap 采用数组 + 链表实现，即使用链表处理冲突，同一 hash 值的元素都存储在一个链表里。但是当位于一个桶中的元素较多，即 hash 值相等的元素较多时，通过 key 值依次查找的效率较低。而 JDK 1.8 中，HashMap 采用数组 + 链表 + 红黑树实现，当链表长度超过阈值（8）时，将链表转换为红黑树，这样大大减少了查找时间；当链表长度小于链表还原阈值（6）时，将红黑树转换为链表。 Node：Node&lt;K, V&gt; 类用来实现数组及链表的数据结构。Node 是 HashMap 的一个内部类，实现了 Map.Entry 接口，本质是就是一个映射(键值对)。 123456789101112131415161718192021222324static class Node&lt;K, V&gt; implements Map.Entry&lt;K, V&gt; &#123; // 保存节点的 hash 值 final int hash; // 保存节点的 key 值 final K key; // 保存节点的 value 值 V value; // 指向链表结构下的当前节点的 next 节点, 红黑树 TreeNode 节点中也有用到 Node&lt;K, V&gt; next; Node(int hash, K key, V value, Node&lt;K, V&gt; next) &#123; ...&#125; public final K getKey() &#123; ...&#125; public final V getValue() &#123; ...&#125; public final String toString() &#123; ...&#125; public final int hashCode() &#123; ...&#125; public final V setValue(V newValue) &#123; ...&#125; public final boolean equals(Object o) &#123; ...&#125;&#125; TreeNode：TreeNode&lt;K, V&gt; 继承 LinkedHashMap.Entry&lt;K, V&gt;，用来实现红黑树相关的存储结构。 12345678910111213141516static final class TreeNode&lt;K, V&gt; extends LinkedHashMap.Entry&lt;K, V&gt; &#123; // 存储当前节点的父节点 HashMap.TreeNode&lt;K, V&gt; parent; // red-black tree links // 存储当前节点的左孩子 HashMap.TreeNode&lt;K, V&gt; left; // 存储当前节点的右孩子 HashMap.TreeNode&lt;K, V&gt; right; // 存储当前节点的前一个节点 HashMap.TreeNode&lt;K, V&gt; prev; // needed to unlink next upon deletion // 存储当前节点的颜色（红、黑） boolean red; TreeNode(int hash, K key, V val, Node&lt;K, V&gt; next) &#123; super(hash, key, val, next); &#125;&#125; HashMap 各常量、成员变量作用 HashMap 各常量 1234567891011121314151617// 创建 HashMap 时未指定初始容量情况下的默认容量, 即默认的数组长度 16static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4;// HashMap 的最大容量static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;// HashMap 默认的装载因子, 当 HashMap 中元素数量超过 容量 * 装载因子 时, 进行 resize() 操作static final float DEFAULT_LOAD_FACTOR = 0.75f;// 用来确定何时将解决 hash 冲突的链表转变为红黑树static final int TREEIFY_THRESHOLD = 8;// 用来确定何时将解决 hash 冲突的红黑树转变为链表static final int UNTREEIFY_THRESHOLD = 6;// 当需要将解决 hash 冲突的链表转变为红黑树时, 需要判断下此时数组容量, 若是由于数组容量太小（小于 MIN_TREEIFY_CAPACITY）导致的 hash 冲突太多, 则不进行链表转变为红黑树操作, 转为利用 resize() 函数对 hashMap 扩容[为了避免进行扩容、树形化选择的冲突]static final int MIN_TREEIFY_CAPACITY = 64; HashMap 各成员变量 1234567891011121314151617// 保存 Node&lt;K,V &gt; 节点的数组transient Node&lt;K, V&gt;[] table;// 由 hashMap 中 Node&lt;K,V&gt; 节点构成的 settransient Set&lt;Map.Entry&lt;K, V&gt;&gt; entrySet;// 记录 hashMap 当前存储的元素的数量transient int size;// 记录 hashMap 发生结构性变化的次数（注意 value 的覆盖不属于结构性变化）transient int modCount;// threshold 的值应等于 table.length * loadFactor, size 超过这个值时进行 resize()扩容int threshold;// 记录 hashMap 装载因子final float loadFactor; HashMap 构造方法构造方法：指定初始容量及装载因子 123456789101112131415161718192021222324252627// 构造方法: 指定初始容量及装载因子public HashMap(int initialCapacity, float loadFactor) &#123; // 指定的初始容量非负 if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal initial capacity:" + initialCapacity); // 如果指定的初始容量大于最大容量, 置为最大容量 if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; // 填充比为正 if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException("Illegal load factor:" + loadFactor); this.loadFactor = loadFactor; /* tableSizeFor(initialCapacity) 方法返回的值是最接近 initialCapacity 的 2 的幂, 若指定初始容量为９, 则实际 hashMap 容量为 16 */ // 注意此种方法创建的 hashMap 初始容量的值存在 threshold 中 this.threshold = tableSizeFor(initialCapacity);&#125;// tableSizeFor(initialCapacity) 方法返回的值是最接近 initialCapacity 的 2 的幂static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; // &gt;&gt;&gt; 代表无符号右移 n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; HashMap 源码分析根据 key 获取哈希桶数组索引位置不管增加、删除、查找键值对，定位到哈希桶数组的位置都是很关键的第一步。前面说过 HashMap 的数据结构是数组和链表的结合，所以我们当然希望这个 HashMap 里面的元素位置尽量分布均匀些，尽量使得每个位置上的元素数量只有一个，那么当我们用 hash 算法求得这个位置的时候，马上就可以知道对应位置的元素就是我们要的，不用遍历链表，大大优化了查询的效率。HashMap 定位数组索引位置，直接决定了 hash 方法的离散性能。 Hash 算法本质上就是三步：取 key 的 hashCode 值、高位运算、取模运算。 12345678910111213141516171819202122/** * 计算 hash 值[JDK 1.8 &amp; JDK 1.7] * 在 JDK 1.8 的实现中, 优化了高位运算的算法, 通过 hashCode()的高 16 位异或低 16 位实现的:(h = k.hashCode()) ^ (h &gt;&gt;&gt; 16), * 主要是从速度、功效、质量来考虑的, 这么做可以在数组 table 的 length 比较小的时候, 也能保证考虑到高低 Bit 都参与到 Hash 的计算中, 同时不会有太大的开销。 */static final int hash(Object key) &#123; int h; // h = key.hashCode() 为第一步 取 hashCode 值 // h ^ (h &gt;&gt;&gt; 16) 为第二步 高位参与运算 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125;/** * 取模运算[JDK 1.7 的源码, JDK 1.8 没有这个方法, 但是实现原理一样的] * 它通过 h &amp; (table.length -1)来得到该对象的保存位, * 而 HashMap 底层数组的长度总是 2 的 n 次方, 这是 HashMap 在速度上的优化。 * 当 length 总是 2 的 n 次方时, h &amp; (length - 1)运算等价于对 length 取模, 也就是 h%length, 但是 &amp; 比 % 具有更高的效率 */static int indexFor(int h, int length) &#123; // 第三步 取模运算 return h &amp; (length - 1);&#125; HashMap put()及其相关方法HashMap 的 put 方法执行过程可以通过下图来理解。 1、判断键值对数组 table[i]是否为空或为 null，否则执行 resize()进行扩容； 2、根据键值 key 计算 hash 值得到插入的数组索引 i，如果 table[i]==null，直接新建节点添加，转向步骤六，如果 table[i]不为空，转向步骤三； 3、判断 table[i]的首个元素是否和 key 一样，如果相同直接覆盖 value，否则转向步骤四，这里的相同指的是 hashCode 以及 equals； 4、判断 table[i]是否为 treeNode，即 table[i]是否是红黑树，如果是红黑树，则直接在树中插入键值对，否则转向步骤五； 5、遍历 table[i]，判断链表长度是否大于 8，大于 8 的话把链表转换为红黑树，在红黑树中执行插入操作，否则进行链表的插入操作；遍历过程中若发现 key 已经存在直接覆盖 value 即可； 6、插入成功后，判断实际存在的键值对数量 size 是否超多了最大容量 threshold，如果超过，进行扩容。 JDK 1.8HashMap 的 put 方法源码如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public V put(K key, V value) &#123; // 对 key 的 hashCode()做 hash return putVal(hash(key), key, value, false, true);&#125;final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K, V&gt;[] tab; Node&lt;K, V&gt; p; int n, i; // 步骤一: tab 为空则创建[如果 table 还未被初始化, 那么初始化它] if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 步骤二: 计算 index, 并对 null 做处理[根据键的 hash 值找到该键对应到数组中存储的索引; 如果为 null, 那么说明此索引位置并没有被占用] if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); // 不为 null, 说明此处已经被占用, 只需要将构建一个节点插入到这个链表的尾部即可 else &#123; Node&lt;K, V&gt; e; K k; // 步骤三: 节点 key 存在, 直接覆盖 value[当前结点和将要插入的结点的 hash 和 key 相同, 说明这是一次修改操作] if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 步骤四: 判断该链为红黑树[如果 p 这个头结点是红黑树结点的话, 以红黑树的插入形式进行插入] else if (p instanceof TreeNode) e = ((TreeNode&lt;K, V&gt;) p).putTreeVal(this, tab, hash, key, value); // 步骤五: 该链为链表[遍历此条链表, 将构建一个节点插入到该链表的尾部] else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); // 链表长度大于 8 转换为红黑树进行处理 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; // key 已经存在直接覆盖 value[遍历的过程中, 如果发现与某个结点的 hash 和 key, 这依然是一次修改操作 ] if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; // e 不是 null, 说明当前的 put 操作是一次修改操作并且 e 指向的就是需要被修改的结点 if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; // 步骤六: 超过最大容量就扩容[如果添加后, 数组容量达到阈值, 进行扩容] if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; HashMap 扩容方法 resize()及其相关方法扩容 (resize) 就是重新计算容量，向 HashMap 对象里不停的添加元素，而 HashMap 对象内部的数组无法装载更多的元素时，对象就需要扩大数组的长度，以便能装入更多的元素。当然 Java 里的数组是无法自动扩容的，方法是使用一个新的数组代替已有的容量小的数组，就像我们用一个小桶装水，如果想装更多的水，就得换大水桶。 我们分析下 resize()的源码，鉴于 JDK 1.8 融入了红黑树，较复杂，为了便于理解我们仍然使用 JDK 1.7 的代码，好理解一些，本质上区别不大，具体区别后文再说。 123456789101112131415161718192021// 传入新的容量void resize(int newCapacity) &#123; // 引用扩容前的 Entry 数组 Entry[] oldTable = table; int oldCapacity = oldTable.length; // 扩容前的数组大小如果已经达到最大 (2^30) 了 if (oldCapacity == MAXIMUM_CAPACITY) &#123; // 修改阈值为 int 的最大值(2^31-1)，这样以后就不会扩容了 threshold = Integer.MAX_VALUE; return; &#125; // 初始化一个新的 Entry 数组 Entry[] newTable = new Entry[newCapacity]; // 将数据转移到新的 Entry 数组里 transfer(newTable); // HashMap 的 table 属性引用新的 Entry 数组 table = newTable; // 修改阈值 threshold = (int)(newCapacity * loadFactor);&#125; 这里就是使用一个容量更大的数组来代替已有的容量小的数组，transfer()方法将原有 Entry 数组的元素拷贝到新的 Entry 数组里。 12345678910111213141516171819202122232425void transfer(Entry[] newTable) &#123; // src 引用了旧的 Entry 数组 Entry[] src = table; int newCapacity = newTable.length; // 遍历旧的 Entry 数组 for (int j = 0; j &lt; src.length; j++) &#123; // 取得旧 Entry 数组的每个元素 Entry&lt;K,V&gt; e = src[j]; if (e != null) &#123; // 释放旧 Entry 数组的对象引用（for 循环后，旧的 Entry 数组不再引用任何对象） src[j] = null; do &#123; Entry&lt;K,V&gt; next = e.next; // 重新计算每个元素在数组中的位置 int i = indexFor(e.hash, newCapacity); // 标记[1] e.next = newTable[i]; // 将元素放在数组上 newTable[i] = e; // 访问下一个 Entry 链上的元素 e = next; &#125; while (e != null); &#125; &#125;&#125; newTable[i]的引用赋给了 e.next，也就是使用了单链表的头插入方式，同一位置上新元素总会被放在链表的头部位置；这样先放在一个索引上的元素终会被放到 Entry 链的尾部(如果发生了 hash 冲突的话），这一点和 JDK 1.8 有区别，下文详解。在旧数组中同一条 Entry 链上的元素，通过重新计算索引位置后，有可能被放到了新数组的不同位置上。 下面举个例子说明下扩容过程。假设了我们的 hash 算法就是简单的用 key mod 一下表的大小（也就是数组的长度）。其中的哈希桶数组 table 的 size=2， 所以 key = 3、7、5，put 顺序依次为 5、7、3。在 mod 2 以后都冲突在 table[1]这里了。这里假设负载因子 loadFactor=1，即当键值对的实际大小 size 大于 table 的实际大小时进行扩容。接下来的三个步骤是哈希桶数组 resize 成 4，然后所有的 Node 重新 rehash 的过程。 下面我们讲解下 JDK 1.8 做了哪些优化。经过观测可以发现，我们使用的是 2 次幂的扩展(指长度扩为原来 2 倍)，所以，元素的位置要么是在原位置，要么是在原位置再移动 2 次幂的位置。看下图可以明白这句话的意思，n 为 table 的长度，图（a）表示扩容前的 key1 和 key2 两种 key 确定索引位置的示例，图（b）表示扩容后 key1 和 key2 两种 key 确定索引位置的示例，其中 hash1 是 key1 对应的哈希与高位运算结果。 元素在重新计算 hash 之后，因为 n 变为 2 倍，那么 n-1 的 mask 范围在高位多 1bit(红色)，因此新的 index 就会发生这样的变化： 因此，我们在扩充 HashMap 的时候，不需要像 JDK 1.7 的实现那样重新计算 hash，只需要看看原来的 hash 值新增的那个 bit 是 1 还是 0 就好了，是 0 的话索引没变，是 1 的话索引变成“原索引 + oldCap”，可以看看下图为 16 扩充为 32 的 resize 示意图： 这个设计确实非常的巧妙，既省去了重新计算 hash 值的时间，而且同时，由于新增的 1bit 是 0 还是 1 可以认为是随机的，因此 resize 的过程，均匀的把之前的冲突的节点分散到新的 bucket 了。这一块就是 JDK 1.8 新增的优化点。有一点注意区别，JDK 1.7 中 rehash 的时候，旧链表迁移新链表的时候，如果在新表的数组索引位置相同，则链表元素会倒置，但是从上图可以看出，JDK 1.8 不会倒置。有兴趣的同学可以研究下 JDK 1.8 的 resize 源码，写的很赞，如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081final Node&lt;K, V&gt;[] resize() &#123; Node&lt;K, V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123; // 超过最大值就不再扩充了, 就只好随你碰撞去吧 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; // 没超过最大值, 就扩充为原来的 2 倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int) (DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; // 计算新的 resize 上限 if (newThr == 0) &#123; float ft = (float) newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float) MAXIMUM_CAPACITY ? (int) ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;"rawtypes","unchecked"&#125;) Node&lt;K, V&gt;[] newTab = (Node&lt;K, V&gt;[]) new Node[newCap]; table = newTab; if (oldTab != null) &#123; // 把每个 bucket 都移动到新的 buckets 中 for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K, V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K, V&gt;) e).split(this, newTab, j, oldCap); else &#123; // 链表优化重 hash 的代码块 Node&lt;K, V&gt; loHead = null, loTail = null; Node&lt;K, V&gt; hiHead = null, hiTail = null; Node&lt;K, V&gt; next; do &#123; next = e.next; // 原索引 if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; // 原索引 + oldCap else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); // 原索引放到 bucket 里 if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; // 原索引 + oldCap 放到 bucket 里 if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; HashMap 线程安全性在多线程使用场景中，应该尽量避免使用线程不安全的 HashMap，而使用线程安全的 ConcurrentHashMap。那么为什么说 HashMap 是线程不安全的，下面举例子说明在并发的多线程使用场景中使用 HashMap 可能造成死循环。代码例子如下(便于理解，仍然使用 JDK 1.7 的环境)： 12345678910111213141516171819202122public class HashMapInfiniteLoop &#123; private static HashMap&lt;Integer, String&gt; map = new HashMap&lt;&gt;(2, 0.75f); public static void main(String[] args) &#123; map.put(5, "C"); new Thread("Thread1") &#123; public void run() &#123; map.put(7, "B"); System.out.println(map); &#125; &#125;.start(); new Thread("Thread2") &#123; public void run() &#123; map.put(3, "A"); System.out.println(map); &#125; &#125;.start(); &#125;&#125; 其中，map 初始化为一个长度为 2 的数组，loadFactor=0.75，threshold=2*0.75=1，也就是说当 put 第二个 key 的时候，map 就需要进行 resize。 通过设置断点让线程 1 和线程 2 同时 debug 到 transfer 方法 (3.3 小节代码块) 的首行。注意此时两个线程已经成功添加数据。放开 thread1 的断点至 transfer 方法的“Entry next = e.next;” 这一行；然后放开线程 2 的的断点，让线程 2 进行 resize。结果如下图。 注意，Thread1 的 e 指向了 key(3)，而 next 指向了 key(7)，其在线程二 rehash 后，指向了线程二重组后的链表。 线程一被调度回来执行，先是执行 newTalbe[i] = e， 然后是 e = next，导致了 e 指向了 key(7)，而下一次循环的 next = e.next 导致了 next 指向了 key(3)。 e.next = newTable[i] 导致 key(3).next 指向了 key(7)。注意：此时的 key(7).next 已经指向了 key(3)， 环形链表就这样出现了。 于是，当我们用线程一调用 map.get(11)时，悲剧就出现了—— Infinite Loop。 JDK 1.7 中 HashMap 和 JDK 1.8 中 HashMap 的实现区別？JDK 1.8 针对 HashMap 主要优化是减少了 Hash 冲突，提高哈希表的存、取效率。 底层数据结构不一样：JDK 1.7 中，HashMap 采用数组 + 链表形式实现，即使用链表处理冲突，同一 hash 值的键值对会被放在同一个链表里，当链表中元素较多时，通过 key 值查找的效率较低。而 JDK 1.8 中，HashMap 采用数组 + 链表 + 红黑树实现，当链表长度超过阈值（8）时，将链表转换为红黑树，来把时间复杂度从 O（n）变成 O（logN），这样大大减少了查找时间。 初始化方式不一样：JDK 1.7 中 HashMap 的 resize()方法负责扩容，inflateTable()负责创建表；而 JDK 1.8 中 HashMap 的 resize()方法在表为空时，创建表；在表不为空时，则负责扩容。 插入数据方式不一样：JDK 1.7 中 HashMap 新增节点采用头插法，先将原位置的数据移到后 1 位，再插入数据到该位置；而 JDK 1.8 中 HashMap 新增节点采用尾插法。因为 JDK 1.7 中 HashMap 是用单链表进行的纵向延伸，当采用头插法时会容易出现逆序且环形链表死循环问题。但是在 JDK 1.8 中 HashMap 是因为加入了红黑树使用尾插法，能够避免出现逆序且链表死循环的问题。 扩容后存储位置的计算方式不一样：在 JDK 1.7 中 HashMap 扩容时全部按照原来方式进行计算，直接用 hash 值和需要扩容的二进制数进行 &amp;（hash 值 &amp; length-1），这里就是为什么扩容的时候为啥一定必须是 2 的多少次幂的原因所在，因为如果只有 2 的 n 次幂的情况时最后一位二进制数才一定是 1，这样能最大程度减少 hash 碰撞；而在 JDK 1.8 中 HashMap 扩容时按照扩容后的规则计算，即扩容后的位置 = 扩容前的原始位置 or 扩容前的原始位置 + 旧容量的大小值。 扩容时插入数据的插入时机不一样：JDK 1.7 中 HashMap 在扩容后插入数据，而 JDK 1.8 中 HashMap 在扩容前插入数据。 hash 值计算方式不一样：在计算 hash 值的时候，JDK 1.7 中 HashMap 用了 9 次扰动处理 = 4 次位运算 + 5 次异或，而 JDK 1.8 中 HashMap 只用了 2 次扰动处理 = 1 次位运算 + 1 次异或。 扩容流程对比图： 总结(1) 扩容是一个特别耗性能的操作，所以当程序员在使用 HashMap 的时候，估算 map 的大小，初始化的时候给一个大致的数值，避免 map 进行频繁的扩容。 (2) 负载因子是可以修改的，也可以大于 1，但是建议不要轻易修改，除非情况非常特殊。 (3) HashMap 是线程不安全的，不要在并发的环境中同时操作 HashMap，建议使用 ConcurrentHashMap。 (4) JDK 1.8 引入红黑树大程度优化了 HashMap 的性能。 简单来说，HashMap 由数组 + 链表组成的，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的，如果定位到的数组位置不含链表（当前 entry 的 next 指向 null），那么对于查找，添加等操作很快，仅需一次寻址即可；如果定位到的数组包含链表，对于添加操作，其时间复杂度为 O(n)，首先遍历链表，存在即覆盖，否则新增；对于查找操作来讲，仍需遍历链表，然后通过 key 对象的 equals()方法逐一比对查找。所以，性能考虑，HashMap 中的链表出现越少，性能才会越好。 特别感谢特别感谢 “美团技术团队” 的知识奉献，为我解惑。 参考博文[1]. 深入理解 HashMap 底层原理剖析(JDK 1.8)[2]. Java 8 系列之重新认识 HashMap[3]. HashMap 实现原理 深入浅出集合框架系列 深入浅出集合框架（一）：HashMap 的底层实现原理 深入浅出集合框架（二）：为并发而生的 ConcurrentHashMap]]></content>
      <categories>
        <category>集合</category>
      </categories>
      <tags>
        <tag>Java8</tag>
        <tag>HashMap</tag>
        <tag>集合</tag>
        <tag>Java7</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 进阶讲解（一）：MySQL 索引原理]]></title>
    <url>%2Farchives%2Fc2d5e679.html</url>
    <content type="text"><![CDATA[前言索引对查询的速度有着至关重要的影响，理解索引也是进行数据库性能调优的起点。索引在 MySQL 中也叫做“键”，存储引擎用于快速查找记录的一种数据结构，通过合理的使用数据库索引可以大大提高系统的访问性能。关于 MySQL 索引的好处，如果正确合理设计并且使用索引的 MySQL 是一辆兰博基尼的话，那么没有设计和使用索引的 MySQL 就是一个人力三轮车。对于没有索引的表，单表查询可能几十万数据就是瓶颈，而通常大型网站单日就可能会产生几十万甚至几百万的数据，没有索引查询会变的非常缓慢。接下来主要介绍在 MySQL 数据库中索引类型，以及如何创建出更加合理且高效的索引技巧。 MySQL 索引的概念索引是一种特殊的文件(InnoDB 数据表上的索引是表空间的一个组成部分)，它们包含着对数据表里所有记录的引用指针。更通俗的说，数据库索引好比是一本书前面的目录，能加快数据库的查询速度。 索引的优缺点 优势：可以快速检索，减少 I/O 次数，加快检索速度；根据索引分组和排序，可以加快分组和排序； 劣势：索引本身也是表，因此会占用存储空间，一般来说，索引表占用的空间的数据表的 1.5 倍；索引表的维护和创建需要时间成本，这个成本随着数据量增大而增大；构建索引会降低数据表的修改操作（删除，添加，修改）的效率，因为在修改数据表的同时还需要修改索引表； MySQL 索引的类型常见的索引类型有：主键索引、唯一索引、普通索引、全文索引、组合索引。 阿里巴巴的 Java 开发手册中提到： 【强制】主键索引名为 pk_字段名；唯一索引名为 uk_字段名；普通索引名则为 idx_字段名。 说明：pk_ 即 primary key；uk_ 即 unique key；idx_ 即 index 的简称。 12345-- 显示索引信息SHOW INDEX FROM table_name;-- 删除索引DROP INDEX index_name ON table_name; PRIMARY - 主键索引即主索引，根据主键 pk_clolum（length）建立索引，不允许重复，不允许空值； UNIQUE - 唯一索引与普通索引类似，不同的就是：索引列的值必须唯一，但允许有空值（注意和主键不同）。如果是组合索引，则列值的组合必须唯一，创建方法和普通索引类似。 123456789101112131415-- 创建唯一索引[唯一索引名为 uk_字段名]CREATE UNIQUE INDEX uk_column_name ON table_name (column_name(length));-- 修改表结构ALTER TABLE table_name ADD UNIQUE uk_column_name ON (column_name(length));-- 创建表的时候直接指定CREATE TABLE `table` ( `id` INT (11) NOT NULL AUTO_INCREMENT, `title` CHAR (255) NOT NULL, `content` text NULL, `time` INT (10) NULL DEFAULT NULL, PRIMARY KEY (`id`), UNIQUE uk_title (title(length))); INDEX - 普通索引这是最基本的索引，它没有任何限制，MyIASM 中默认的 BTREE 类型的索引，也是我们大多数情况下用到的索引。 123456789101112131415-- 直接创建索引[如果是 CHAR,VARCHAR 类型, length 可以小于字段实际长度；如果是 BLOB 和 TEXT 类型, 必须指定 length.]CREATE INDEX idx_column_name ON table_name (column_name(length));-- 修改表结构的方式添加索引ALTER TABLE table_name ADD INDEX idx_column_name ON (column_name(length));-- 创建表的时候同时创建索引CREATE TABLE `table` ( `id` INT (11) NOT NULL AUTO_INCREMENT, `title` CHAR (255) NOT NULL, `content` text NULL, `time` INT (10) NULL DEFAULT NULL, PRIMARY KEY (`id`), INDEX idx_title (title(length))); FULLTEXT - 全文索引全文索引（也称全文检索）是目前搜索引擎使用的一种关键技术。它能够利用分词技术等多种算法智能分析出文本文字中关键字词的频率及重要性，然后按照一定的算法规则智能地筛选出我们想要的搜索结果。全文索引是一种特殊类型的索引，它查找的是文本中的关键词，而不是直接索引中的值。全文搜索和其他几类索引的匹配方式完全不一样。它有很多需要注意的，如停用词、词干、复数、布尔搜索等。 123456789101112131415-- 直接创建索引CREATE FULLTEXT INDEX ft_column_name ON table_name (content);-- 修改表结构添加全文索引ALTER TABLE table_name ADD FULLTEXT ft_column_name ON (column_name);-- 创建表的适合添加全文索引CREATE TABLE `table` ( `id` INT (11) NOT NULL AUTO_INCREMENT, `title` CHAR (255) NOT NULL, `content` text NULL, `time` INT (10) NULL DEFAULT NULL, PRIMARY KEY (`id`), FULLTEXT (content)); 组合索引一个表中含有多个单列索引不代表是组合索引，通俗一点讲组合索引是：包含多个字段但是只有索引名称。 在表中的多个字段组合上创建的索引，只有在查询条件中使用了这些字段的左边字段时，索引才会被使用，使用组合索引时遵循最左前缀集合。 123456789101112131415-- 修改表结构的方式添加索引ALTER TABLE table_name ADD INDEX idx_column_name_1_column_name_2 (column_name_1, column_name_2);-- 直接创建索引CREATE INDEX idx_column_name_1_column_name_2 ON table_name (column_name_1, column_name_2);-- 创建表的时候同时创建索引CREATE TABLE `table` ( `id` INT (11) NOT NULL AUTO_INCREMENT, `title` CHAR (255) NOT NULL, `content` text NULL, `time` INT (10) NULL DEFAULT NULL, PRIMARY KEY (`id`), INDEX idx_title_time (`title`, `time`)); MySQL 索引的方法不同的存储引擎对于索引有不同的支持：Innodb 和 MyISAM 默认的索引是 Btree 索引；而 MEMORY 默认的索引是 Hash 索引。 Hash 索引哈希索引就是采用一定的哈希算法，把键值换算成新的哈希值，Hash 索引结构的特殊性，其检索效率非常高，索引的检索可以一次定位，不像 Btree 索引需要从根节点到枝节点，最后才能访问到页节点这样多次的 IO 访问，所以 Hash 索引的查询效率要远高于 Btree 索引。 可能很多人又有疑问了，既然 Hash 索引的效率要比 Btree 高很多，为什么大家不都用 Hash 索引而还要使用 Btree 索引呢？任何事物都是有两面性的，Hash 索引也一样，虽然 Hash 索引效率高，但是 Hash 索引本身由于其特殊性也带来了很多限制和弊端，主要有以下这些： （1）Hash 索引仅仅能满足 “=”,”IN” 和 “&lt;=&gt;” 查询，不能使用范围查询。 如果是等值查询，那么哈希索引明显有绝对优势，因为只需要经过一次算法即可找到相应的键值；当然了，这个前提是，键值都是唯一的。如果键值不是唯一的，就需要先找到该键所在位置，然后再根据链表往后扫描，直到找到相应的数据；由于 Hash 索引比较的是进行 Hash 运算之后的 Hash 值，所以它只能用于等值的过滤，不能用于基于范围的过滤，因为经过相应的 Hash 算法处理之后的 Hash 值的大小关系，并不能保证和 Hash 运算前完全一样。 （2）Hash 索引无法被用来避免数据的排序操作。 由于 Hash 索引中存放的是经过 Hash 计算之后的 Hash 值，而且 Hash 值的大小关系并不一定和 Hash 运算前的键值完全一样，所以数据库无法利用索引的数据来避免任何排序运算。 （3）Hash 索引不能利用部分索引键查询。 对于组合索引，Hash 索引在计算 Hash 值的时候是组合索引键合并后再一起计算 Hash 值，而不是单独计算 Hash 值，所以通过组合索引的前面一个或几个索引键进行查询的时候，Hash 索引也无法被利用。 （4）Hash 索引在任何时候都不能避免表扫描。 前面已经知道，Hash 索引是将索引键通过 Hash 运算之后，将 Hash 运算结果的 Hash 值和所对应的行指针信息存放于一个 Hash 表中，由于不同索引键存在相同 Hash 值，所以即使取满足某个 Hash 键值的数据的记录条数，也无法从 Hash 索引中直接完成查询，还是要通过访问表中的实际数据进行相应的比较，并得到相应的结果。 （5）Hash 索引遇到大量 Hash 值相等的情况后性能并不一定就会比 Btree 索引高。 对于选择性比较低的索引键，如果创建 Hash 索引，那么将会存在大量记录指针信息存于同一个 Hash 值相关联。这样要定位某一条记录时就会非常麻烦，会浪费多次表数据的访问，而造成整体性能低下。 Btree 索引Btree 索引是 MySQL 数据库中使用最为频繁的索引类型，除了 ARCHIVE 存储引擎之外的其他所有的存储引擎都支持 Btree 索引。不仅仅在 MySQL 中是如此，实际上在其他的很多数据库管理系统中 Btree 索引也同样是作为最主要的索引类型，这主要是因为 Btree 索引的存储结构在数据库的数据检索中有非常优异的表现。 一般来说，MySQL 中的 Btree 索引的物理文件大多都是以 Balance Tree 的结构来存储的，也就是所有实际需要的数据都存放于 Tree 的 Leaf Node，而且到任何一个 Leaf Node 的最短路径的长度都是完全相同的，所以我们大家都称之为 Btree 索引。当然，可能各种数据库（或 MySQL 的各种存储引擎）在存放自己的 Btree 索引的时候会对存储结构稍作改造。如 Innodb 存储引擎的 Btree 索引实际使用的存储结构实际上是 B+Tree，也就是在 Btree 数据结构的基础上做了很小的改造，在每一个 Leaf Node 上面除了存放索引键的相关信息之外，还存储了指向与该 Leaf Node 相邻的后一个 Leaf Node 的指针信息，这主要是为了加快检索多个相邻 Leaf Node 的效率考虑。 MYSQL 使用 B+Tree 做索引： （1）所有关键字都出现在叶子结点的链表中（稠密索引），且链表中的关键字恰好是有序的。（只有根节点存储关键字最后树的末梢才有值) （2）非叶子结点相当于是叶子结点的索引（稀疏索引），叶子结点相当于是存储（关键字）数据的数据层。（非根节点，存储的其实是指向根节点的索引） （3） 不可能在非叶子结点存数据。 （4）根节点横向也有链指针。(方便快速顺藤摸瓜嘛，没这个指针，就算下一个取的值是挨着的邻居，也得跑个圈才能拿到) 综合起来，B+Tree 比 B - 树优势有三个：1、IO 次数更少；2、查询性能稳定；3、范围查询简便。 聚簇索引与非聚簇索引当数据库一条记录里包含多个字段时，一棵 B+Tree 就只能存储主键，如果检索的是非主键字段，则主键索引失去作用，又变成顺序查找了。这时应该在第二个要检索的列上建立第二套索引。这个索引由独立的 B+Tree 来组织。有两种常见的方法可以解决多个 B+Tree 访问同一套表数据的问题，一种叫做聚簇索引（clustered index），一种叫做非聚簇索引（secondary index）。这两个名字虽然都叫做索引，但这并不是一种单独的索引类型，而是一种数据存储方式。对于聚簇索引存储来说，行数据和主键 B+Tree 存储在一起，辅助键 B+Tree 只存储辅助键和主键，主键和非主键 B+Tree 几乎是两种类型的树。对于非聚簇索引存储来说，主键 B+Tree 在叶子节点存储指向真正数据行的指针，而非主键。 非聚簇索引 非聚簇索引的主索引和辅助索引几乎是一样的，只是主索引不允许重复，不允许空值，他们的叶子结点的 key 都存储指向键值对应的数据的物理地址。 非聚簇索引的数据表和索引表是分开存储的。 非聚簇索引中的数据是根据数据的插入顺序保存。因此非聚簇索引更适合单个数据的查询。插入顺序不受键值影响。 只有在 MyISAM 中才能使用 FULLTEXT 索引。 聚簇索引 聚簇索引的主索引的叶子结点存储的是键值对应的数据本身，辅助索引的叶子结点存储的是键值对应的数据的主键键值。因此主键的值长度越小越好，类型越简单越好。 聚簇索引的数据和主键索引存储在一起。 聚簇索引的数据是根据主键的顺序保存。因此适合按主键索引的区间查找，可以有更少的磁盘 I/O，加快查询速度。但是也是因为这个原因，聚簇索引的插入顺序最好按照主键单调的顺序插入，否则会频繁的引起页分裂，严重影响性能。 在 InnoDB 中，如果只需要查找索引的列，就尽量不要加入其它的列，这样会提高查询效率。 MyISAM Btree 索引实现MyISAM 使用的是非聚簇索引，非聚簇索引的两棵 B+Tree 看上去没什么不同，节点的结构完全一致只是存储的内容不同而已，主键索引 B+Tree 的节点存储了主键，辅助键索引 B+Tree 存储了辅助键。表数据存储在独立的地方，这两颗 B+Tree 的叶子节点都使用一个地址指向真正的表数据，对于表数据来说，这两个键没有任何差别。由于索引树是独立的，通过辅助键检索无需访问主键的索引树。 MyISAM 索引文件和数据文件是分离的，索引文件仅保存数据记录的地址。MyISAM 中索引检索的算法为首先按照 B+Tree 搜索算法搜索索引，如果指定的 Key 存在，则取出其 data 域的值，然后以 data 域的值为地址，读取相应数据记录。 InnoDB Btree 索引实现InnoDB 使用的是聚簇索引，将主键组织到一棵 B+Tree 中，而行数据就储存在叶子节点上，若使用 “where id = 14” 这样的条件查找主键，则按照 B+Tree 的检索算法即可查找到对应的叶节点，之后获得行数据。若对 Name 列进行条件搜索，则需要两个步骤：第一步在辅助索引 B+Tree 中检索 Name，到达其叶子节点获取对应的主键。第二步使用主键在主索引 B+Tree 种再执行一次 B+Tree 检索操作，最终到达叶子节点即可获取整行数据。 InnoDB 中，表数据文件本身就是按 B+Tree 组织的一个索引结构，这棵树的叶结点 data 域保存了完整的数据记录。因为 InnoDB 的数据文件本身要按主键聚集，所以 InnoDB 要求表必须有主键（MyISAM 可以没有），如果没有显式指定，则 MySQL 系统会自动选择一个可以唯一标识数据记录的列作为主键，如果不存在这种列，则 MySQL 自动为 InnoDB 表生成一个隐含字段作为主键，这个字段长度为 6 个字节，类型为长整形。 为了更形象说明这两种索引的区别，我们假想一个表如下图存储了 4 行数据。其中 Id 作为主索引，Name 作为辅助索引。图示清晰的显示了聚簇索引和非聚簇索引的差异。 我们重点关注聚簇索引，看上去聚簇索引的效率明显要低于非聚簇索引，因为每次使用辅助索引检索都要经过两次 B+Tree 查找，这不是多此一举吗？聚簇索引的优势在哪？ （1）由于行数据和叶子节点存储在一起，这样主键和行数据是一起被载入内存的，找到叶子节点就可以立刻将行数据返回了，如果按照主键 Id 来组织数据，获得数据更快。 （2）辅助索引使用主键作为“指针”，而不是使用地址值作为指针的好处是，减少了当出现行移动或者数据页分裂时辅助索引的维护工作，使用主键值当作指针会让辅助索引占用更多的空间，换来的好处是 InnoDB 在移动行时无须更新辅助索引中的这个“指针”。也就是说行的位置会随着数据库里数据的修改而发生变化，使用聚簇索引就可以保证不管这个主键 B+Tree 的节点如何变化，辅助索引树都不受影响。 建立索引的规则1、选择唯一性索引：唯一性索引的值是唯一的，可以更快速的通过该索引来确定某条记录。例如，学生表中学号是具有唯一性的字段。为该字段建立唯一性索引可以很快的确定某个学生的信息。如果使用姓名的话，可能存在同名现象，从而降低查询速度。 2、为经常需要排序、分组和联合操作的字段建立索引：经常需要 ORDER BY、GROUP BY、DISTINCT 和 UNION 等操作的字段，排序操作会浪费很多时间。如果为其建立索引，可以有效地避免排序操作。 3、为常作为查询条件的字段建立索引：如果某个字段经常用来做查询条件，那么该字段的查询速度会影响整个表的查询速度。因此，为这样的字段建立索引，可以提高整个表的查询速度。 4、限制索引的数目：索引的数目不是越多越好。每个索引都需要占用磁盘空间，索引越多，需要的磁盘空间就越大。修改表时，对索引的重构和更新很麻烦。越多的索引，会使更新表变得很浪费时间。 5、尽量使用数据量少的索引：如果索引的值很长，那么查询的速度会受到影响。例如，对一个 CHAR(100)类型的字段进行全文检索需要的时间肯定要比对 CHAR(10)类型的字段需要的时间要多。 6、尽量使用前缀来索引：如果索引字段的值很长，最好使用值的前缀来索引。例如，TEXT 和 BLOG 类型的字段，进行全文检索会很浪费时间。如果只检索字段的前面的若干个字符，这样可以提高检索速度。 7、删除不再使用或者很少使用的索引：表中的数据被大量更新，或者数据的使用方式被改变后，原有的一些索引可能不再需要。数据库管理员应当定期找出这些索引，将它们删除，从而减少索引对更新操作的影响。 8、最左前缀匹配原则，非常重要的原则：Mysql 会一直向右查找直到遇到范围操作（&gt;，&lt;，like、between）就停止匹配。比如 a=1 and b=2 and c&gt;3 and d=6；此时如果建立了（a,b,c,d）索引，那么后面的 d 索引是完全没有用到，当换成了（a,b,d,c）就可以用到。 9、= 和 in 可以乱序：比如 a = 1 and b = 2 and c = 3 建立 (a,b,c) 索引可以任意顺序，mysql 的查询优化器会帮你优化成索引可以识别的形式。 10、尽量选择区分度高的列作为索引：区分度的公式是 count(distinct col)/count(*)，表示字段不重复的比例，比例越大我们扫描的记录数越少，唯一键的区分度是 1，而一些状态、性别字段可能在大数据面前区分度就 是 0，那可能有人会问，这个比例有什么经验值吗？使用场景不同，这个值也很难确定，一般需要 join 的字段我们都要求是 0.1 以上，即平均 1 条扫描 10 条记录。 11、尽量的扩展索引，不要新建索引：比如表中已经有 a 的索引，现在要加 (a,b) 的索引，那么只需要修改原来的索引即可。 注意：选择索引的最终目的是为了使查询的速度变快。上面给出的原则是最基本的准则，但不能拘泥于上面的准则。读者要在以后的学习和工作中进行不断的实践。根据应用的实际情况进行分析和判断，选择最合适的索引方式。 参考博文[1]. 剖析 Mysql 的 InnoDB 索引[2]. 数据库索引 B 树、B+Tree、Hash 索引 MySQL 进阶讲解系列 MySQL 进阶讲解（一）：MySQL 索引原理 MySQL 进阶讲解（二）：快速生成测试数据以及 EXPLAIN 详解]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>索引</tag>
        <tag>Btree</tag>
        <tag>Hash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java8 那些事儿（四）：增强的 Map 集合]]></title>
    <url>%2Farchives%2F139a123e.html</url>
    <content type="text"><![CDATA[前言在 Jdk 8 中 Map 接口提供了一些新的便利的方法。因为在本文中我所提到的所有 Map 方法都是以默认值方法的方式实现的，所以现有的 Map 接口的实现可以直接拥有这些在默认值方法中定义的默认行为，而不需要新增一行代码。本文涵盖的 Jdk 8 中引进的 Map 方法有：getOrDefault(Object, V)，putIfAbsent(K, V)，remove(Object, Object)，replace(K, V)，andreplace(K, V, V）。 Map 范例Map.getOrDefault(Object key, V defaultValue)Map 的新方法 getOrDefault(Object key, V defaultValue) 允许调用者在代码语句中规定获得在 map 中符合提供的键的值，否则在没有找到提供的键的匹配项的时候返回一个 “默认值”。 123456default V getOrDefault(Object key, V defaultValue) &#123; V v; return (((v = get(key)) != null) || containsKey(key)) ? v : defaultValue;&#125; Map.putIfAbsent(K key, V value)Map 的新方法 putIfAbsent(K key, V value) 允许调用者在代码语句中规定添加在 map 中符合提供的键的值，若 map 中有对应 K 映射的 V 且不为 null 则直接返回，否则执行 put。 12345678default V putIfAbsent(K key, V value) &#123; V v = get(key); if (v == null) &#123; v = put(key, value); &#125; return v;&#125; Map.computeIfAbsent(K key, Function&lt; ? super K, ? extends V&gt; mappingFunction)Map.computeIfAbsent 方法原型 V computeIfAbsent(K key, Function&lt; ? super K, ? extends V&gt; mappingFunction)，如果指定的 key 不存在或相关的 value 为 null 时，设置 key 与关联一个计算出的非 null 值，计算出的值为 null 的话什么也不做(不会去删除相应的 key)。如果 key 存在并且对应 value 不为 null 的话什么也不做。同样，方法的返回值也是最终的 map.get(key)。 12345678910111213default V computeIfAbsent(K key, Function&lt;? super K, ? extends V&gt; mappingFunction) &#123; Objects.requireNonNull(mappingFunction); V v; if ((v = get(key)) == null) &#123; V newValue; if ((newValue = mappingFunction.apply(key)) != null) &#123; put(key, newValue); return newValue; &#125; &#125; return v;&#125; Map.computeIfPresent(K key, BiFunction&lt; ? super K, ? super V, ? extends V&gt; remappingFunction)Map.computeIfPresent 方法原型 V computeIfPresent(K key, BiFunction&lt; ? super K, ? super V, ? extends V&gt; remappingFunction)，如果指定的 key 存在并且相关联的 value 不为 null 时，根据旧的 key 和 value 计算 newValue 替换旧值，newValue 为 null 则从 map 中删除该 key；key 不存在或相应的值为 null 时则什么也不做，方法的返回值为最终的 map.get(key)。 1234567891011121314151617default V computeIfPresent(K key, BiFunction&lt;? super K, ? super V, ? extends V&gt; remappingFunction) &#123; Objects.requireNonNull(remappingFunction); V oldValue; if ((oldValue = get(key)) != null) &#123; V newValue = remappingFunction.apply(key, oldValue); if (newValue != null) &#123; put(key, newValue); return newValue; &#125; else &#123; remove(key); return null; &#125; &#125; else &#123; return null; &#125;&#125; Map.remove(Object key, Object value)Map 的新方法 remove(Object key, Object value) 超越了长期有效的 Map.remove(Object key)方法，只有在提供的键和值都匹配的时候才会删除该 map 项（之前的有效版本只是查找 “键” 的匹配来删除）。 123456789default boolean remove(Object key, Object value) &#123; Object curValue = get(key); if (!Objects.equals(curValue, value) || (curValue == null &amp;&amp; !containsKey(key))) &#123; return false; &#125; remove(key); return true;&#125; Map.replace(K key, V value) / Map.replace(K key, V oldValue, V newValue)Map.replace(K key, V value) 方法只有在指定的键已经存在并且有与之相关的映射值时才会将指定的键映射到指定的值（新值）。 1234567default V replace(K key, V value) &#123; V curValue; if (((curValue = get(key)) != null) || containsKey(key)) &#123; curValue = put(key, value); &#125; return curValue;&#125; Map.replace(K key, V oldValue, V newValue) 方法接受一个额外的（第三个）参数，只有在指定的键和值都匹配的情况下才会替换。 123456789default boolean replace(K key, V oldValue, V newValue) &#123; Object curValue = get(key); if (!Objects.equals(curValue, oldValue) || (curValue == null &amp;&amp; !containsKey(key))) &#123; return false; &#125; put(key, newValue); return true;&#125; Map.merge(K key, V value, BiFunction&lt; ? super V, ? super V, ? extends V&gt; remappingFunction)Map.merge(K key, V value, BiFunction&lt; ? super V, ? super V, ? extends V&gt; remappingFunction) 方法，如果指定的 key 不存在，或相应的值为 null 时，则设置 value 为相关联的值。否则根据 key 对应的旧值和 value 计算出新的值 newValue，newValue 为 null 时，删除该 key，否则设置 key 对应的值为 newValue。方法的返回值也是最终的 map.get(key) 值。 1234567891011121314default V merge(K key, V value, BiFunction&lt;? super V, ? super V, ? extends V&gt; remappingFunction) &#123; Objects.requireNonNull(remappingFunction); Objects.requireNonNull(value); V oldValue = get(key); V newValue = (oldValue == null) ? value : remappingFunction.apply(oldValue, value); if(newValue == null) &#123; remove(key); &#125; else &#123; put(key, newValue); &#125; return newValue;&#125; Map.forEach(BiConsumer&lt; ? super K, ? super V&gt; action)Map.forEach(BiConsumer&lt; ? super K, ? super V&gt; action) 方法用于遍历 map。 12345678910111213141516171819202122232425262728293031// JDK8 之前的实现方式 方式一 这是最常见的并且在大多数情况下也是最可取的遍历方式for (Map.Entry&lt;String, String&gt; entry : statesAndCapitals.entrySet()) &#123; System.out.println("Key =" + entry.getKey() + ", Value =" + entry.getValue());&#125;// JDK8 之前的实现方式 方法二 在 for-each 循环中遍历 keys 或 values// 遍历 map 中的键for (String key : statesAndCapitals.keySet()) &#123; System.out.println("Key =" + key);&#125;// 遍历 map 中的值for (String value : statesAndCapitals.values()) &#123; System.out.println("Value =" + value);&#125;// JDK8 之前的实现方式 方法三使用 Iterator 遍历Iterator&lt;Map.Entry&lt;String, String&gt;&gt; entries = statesAndCapitals.entrySet().iterator();while (entries.hasNext()) &#123; Map.Entry&lt;String, String&gt; entry = entries.next(); System.out.println("Key =" + entry.getKey() + ", Value =" + entry.getValue());&#125;// JDK8 之前的实现方式 通过键找值遍历（效率低）for (String key : statesAndCapitals.keySet()) &#123; String value = statesAndCapitals.get(key); System.out.println("Key =" + key + ", Value =" + value);&#125;// JDK8 的实现方式statesAndCapitals.forEach((s, s2) -&gt; System.out.println("Key =" + s + ", Value =" + s2)); Map.sortedMap.sorted 方法用于排序 map。 123456789101112131415161718192021222324252627282930313233343536373839404142/** * 1. 第一层括弧实际是定义了一个匿名内部类 (Anonymous Inner Class) * 2. 第二层括弧实际上是一个实例初始化块 (instance initializer block), 这个块在内部匿名类构造时被执行, 这个块之所以被叫做 “实例初始化块” 是因为它们被定义在了一个类的实例范围内 * 3. 如果 value 为 java 对象, 则需要实现 Comparable 接口, 重写 compareTo 方法 */Map&lt;String, Integer&gt; map = new HashMap&lt;String, Integer&gt;() &#123; &#123; put("A", 3); put("B", 5); put("C", 1); put("D", 1); put("E", 9); &#125;&#125;;System.out.println(map);// 1. 根据 value 对 map 进行 ASC 排序Map&lt;String, Integer&gt; sortedMap = new LinkedHashMap&lt;&gt;();map.entrySet() .stream() .sorted(Map.Entry.comparingByValue()) .forEachOrdered(x -&gt; sortedMap.put(x.getKey(), x.getValue()));System.out.println(sortedMap);// 2. 根据 value 对 map 进行 DESC 排序Map&lt;String, Integer&gt; sortedMap2 = new LinkedHashMap&lt;&gt;();map.entrySet() .stream() // DESC .sorted(Map.Entry.&lt;String, Integer&gt;comparingByValue().reversed()) .sorted(Collections.reverseOrder(Map.Entry.comparingByValue())) .forEachOrdered(x -&gt; sortedMap2.put(x.getKey(), x.getValue()));System.out.println(sortedMap2);// 3.Collectors.toMap 直接返回排好序的 mapMap&lt;String, Integer&gt; sortedMap3;sortedMap3 = map.entrySet() .stream() .sorted(Map.Entry.&lt;String, Integer&gt;comparingByValue().reversed()) // .collect(Collectors.toMap(x -&gt; x.getKey(), x -&gt; x.getValue(), (x1, x2) -&gt; x2, LinkedHashMap::new)); .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue, (x1, x2) -&gt; x2, LinkedHashMap::new));System.out.println(sortedMap3); 参考博文[1]. Handy New Map Default Methods in JDK 8[2]. Java8 之 Stream/Map[3]. Java8 Map 中新增的方法使用总结 Java8 那些事儿系列 Java8 那些事儿（一）：Stream 函数式编程 Java8 那些事儿（二）：Optional 类解决空指针异常 Java8 那些事儿（三）：Date/Time API(JSR 310) Java8 那些事儿（四）：增强的 Map 集合 Java8 那些事儿（五）：函数式接口]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java8</tag>
        <tag>Map</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 并发编程之美（三）：异步执行框架 Eexecutor]]></title>
    <url>%2Farchives%2Fq1e3r5t42.html</url>
    <content type="text"><![CDATA[前言在 Java5 之后，并发编程引入了一堆新的启动、调度和管理线程的 API。Executor 框架便是 Java5 中引入的，其内部使用了线程池机制，它在 java.util.cocurrent 包下，通过该框架来控制线程的启动、执行和关闭，可以简化并发编程的操作。Eexecutor 作为灵活且强大的异步执行框架，其支持多种不同类型的任务执行策略，提供了一种标准的方法将任务的提交过程和执行过程解耦开发，基于生产者 - 消费者模式，其提交任务的线程相当于生产者，执行任务的线程相当于消费者，并用 Runnable 来表示任务，Executor 的实现还提供了对生命周期的支持，以及统计信息收集，应用程序管理机制和性能监视等机制。 Executor 包括：ThreadPoolExecutor，Executor，Executors，ExecutorService，CompletionService，Future，Callable 等。 Executor 简介Executor 的 UML 图：（常用的几个接口和子类） Executor：一个接口，其定义了一个接收 Runnable 对象的方法 executor，其方法签名为 executor(Runnable command)。 ExecutorService：是一个比 Executor 使用更广泛的子类接口，其提供了生命周期管理的方法，以及可跟踪一个或多个异步任务执行状况返回 Future 的方法；ExecutorService 接口继承自 Executor 接口，它提供了更丰富的实现多线程的方法，比如：ExecutorService 提供了关闭自己的方法，以及可为跟踪一个或多个异步任务执行状况而生成 Future 的方法。可以调用 ExecutorService 的 shutdown()方法来平滑地关闭 ExecutorService，调用该方法后，将导致 ExecutorService 停止接受任何新的任务且等待已经提交的任务执行完成(已经提交的任务会分两类：一类是已经在执行的，另一类是还没有开始执行的)，当所有已经提交的任务执行完毕后将会关闭 ExecutorService。因此我们一般用该接口来实现和管理多线程。 AbstractExecutorService：ExecutorService 执行方法的默认实现。 ScheduledExecutorService：一个可定时调度任务的接口。 ScheduledThreadPoolExecutor：ScheduledExecutorService 的实现，一个可定时调度任务的线程池。 ThreadPoolExecutor：线程池，是线程池的核心实现类，用来执行被提交的任务。可以通过调用 Executors 以下静态工厂方法来创建线程池并返回一个 ExecutorService 对象。 Executors：提供了一系列静态工厂方法用于创建各种线程池。 Executors 简介Executors：提供了一系列静态工厂方法用于创建各种线程池。 Executors 提供的线程池配置方案newFixedThreadPool构造一个固定线程数目的线程池，配置的 corePoolSize 与 maximumPoolSize 大小相同，同时使用了一个无界 LinkedBlockingQueue 存放阻塞任务，因此多余的任务将存放在阻塞队列，不会由 RejectedExecutionHandler 处理。每次提交一个任务就创建一个线程，直到线程达到线程池的最大大小。线程池的大小一旦达到最大值就会保持不变，如果某个线程因为执行异常而结束，那么线程池会补充一个新线程。 方法签名： 12345public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125; newCachedThreadPool构造一个缓冲功能的线程池，配置 corePoolSize=0，maximumPoolSize=Integer.MAX_VALUE，keepAliveTime=60s，以及一个无容量的阻塞队列 SynchronousQueue，因此任务提交之后，将会创建新的线程执行；线程空闲超过 60s 将会销毁。如果线程池的大小超过了处理任务所需要的线程，那么就会回收部分空闲（60 秒不执行任务）的线程，当任务数增加时，此线程池又可以智能的添加新线程来处理任务。 此线程池不会对线程池大小做限制，线程池大小完全依赖于操作系统（或者说 JVM）能够创建的最大线程大小，极端情况下会因为创建过多线程而耗尽系统资源。这里虽然指定 maximumPool 为 Integer.MAX_VALUE，但没什么意义，如果不能满足任务执行需求，CachedThreadPool 还会继续创建新的线程。 方法签名： 12345public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125; newSingleThreadExecutor构造一个只支持一个线程的线程池，配置 corePoolSize=maximumPoolSize=1，无界阻塞队列 LinkedBlockingQueue；保证任务由一个线程串行执行；如果这个唯一的线程因为异常结束，那么会有一个新的线程来替代它。此线程池保证所有任务的执行顺序按照任务的提交顺序执行。 方法签名： 123456public static ExecutorService newSingleThreadExecutor() &#123; return new Executors.FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()));&#125; 使用案例： 12345678910111213141516171819ScheduledExecutorService scheduledThreadPool = Executors.newScheduledThreadPool(5); /** * 1.schedule: 初始化延迟 3s 开始执行. */ scheduledThreadPool.schedule(() -&gt; System.out.println("delay 3 seconds"), 3, TimeUnit.SECONDS); /** * 2.scheduleAtFixedRate: 按指定频率周期执行某个任务; * 初始化延迟 3s 开始执行, 每隔 3s 重新执行一次任务[以第一个任务开始计时]. * 以第一个任务开始的时间计时, 3 秒过去后, 检测上一个任务是否执行完毕, 如果上一个任务执行完毕, 则当前任务立即执行, 如果上一个任务没有执行完毕, 则需要等上一个任务执行完毕后立即执行. */ scheduledThreadPool.scheduleAtFixedRate(() -&gt; System.out.println("delay 3 seconds, and scheduleAtFixedRate every 3 seconds"), 3, 3, TimeUnit.SECONDS); /** * 3.scheduleAtFixedRate: 按指定频率间隔执行某个任务; * 初始化时延时 3s 开始执行, 本次执行结束后延迟 3s 开始下次执行. */ scheduledThreadPool.scheduleWithFixedDelay(() -&gt; System.out.println("delay 3 seconds, and scheduleWithFixedDelay every 3 seconds"), 3, 3, TimeUnit.SECONDS); newScheduledThreadPool构造一个有定时功能的线程池，配置 corePoolSize，无界延迟阻塞队列 DelayedWorkQueue；有意思的是：maximumPoolSize=Integer.MAX_VALUE，由于 DelayedWorkQueue 是无界队列，所以这个值是没有意义的。 方法签名： 12345678public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) &#123; return new ScheduledThreadPoolExecutor(corePoolSize);&#125;public ScheduledThreadPoolExecutor(int corePoolSize) &#123; super(corePoolSize, Integer.MAX_VALUE, 0, NANOSECONDS, new DelayedWorkQueue());&#125; newWorkStealingPoolnewWorkStealingPool 是 Jdk1.8 新增一个线程池，会根据所需的并行层次来动态创建和关闭线程，通过使用多个队列减少竞争，底层用的 ForkJoinPool 来实现的。ForkJoinPool 的优势在于，可以充分利用多 CPU，多核 CPU 的优势，把一个任务拆分成多个 “小任务”，把多个“小任务” 放到多个处理器核心上并行执行；当多个 “小任务” 执行完成之后，再将这些执行结果合并起来即可。 方法签名： 123456public static ExecutorService newWorkStealingPool() &#123; return new ForkJoinPool (Runtime.getRuntime().availableProcessors(), ForkJoinPool.defaultForkJoinWorkerThreadFactory, null, true);&#125; 五种线程池的使用场景 newSingleThreadExecutor：一个单线程的线程池，可以用于需要保证顺序执行的场景，并且只有一个线程在执行。 newFixedThreadPool：一个固定大小的线程池，可以用于已知并发压力的情况下，对线程数做限制。FixedThreadPool 满足了资源管理的需求，可以限制当前线程数量。适用于负载较重的服务器环境。 newCachedThreadPool：一个可以无限扩大的线程池，比较适合处理执行时间比较小的任务。CachedThreadPool 适用于执行很多短期异步任务的小程序，适用于负载较轻的服务器。 newScheduledThreadPool：一个有定时功能的线程池，适用于需要多个后台线程执行周期任务的场景，并且为了满足资源管理需求而限制后台线程数量的场景。 newWorkStealingPool：一个拥有多个任务队列的线程池，可以减少连接数，创建当前可用 CPU 数量的线程来并行执行。 参考博文[1]. java 并发编程 –Executor 框架 Java 并发编程之美系列 Java 并发编程之美（一）：并发队列 Queue 原理剖析 Java 并发编程之美（二）：线程池 ThreadPoolExecutor 原理探究 Java 并发编程之美（三）：异步执行框架 Eexecutor Java 并发编程之美（四）：深入剖析 ThreadLocal Java 并发编程之美（五）：揭开 InheritableThreadLocal 的面纱 Java 并发编程之美（六）：J.U.C 之线程同步辅助工具类]]></content>
      <categories>
        <category>并发</category>
      </categories>
      <tags>
        <tag>Eexecutor</tag>
        <tag>Executors</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 工匠精神（二）：理解 Linux 的处理器负载均值]]></title>
    <url>%2Farchives%2F275d83e6.html</url>
    <content type="text"><![CDATA[前言在 Linux 系统中，当系统响应缓慢时，一般会用 uptime 命令查看系统负载是否过高（w 命令和 top 命令也行）。而系统负载（System Load）也作为我们判断系统运行是否正常的一个非常重要的指标。 什么是 Load Average？系统负载（System Load）是系统 CPU 繁忙程度的度量，即有多少进程在等待被 CPU 调度（进程等待队列的长度）。系统负载表示每分钟处于可运行状态 (运行态和就绪态) 以及不可中断状态 (等待 io) 的进程数目，且没有做归一化处理。 平均负载（Load Average）是一段时间内系统的平均负载，这个一段时间一般取 1 分钟、5 分钟、15 分钟。 如何查看 LoadAverage？top 命令，w 命令，uptime 等命令都可以查看系统负载； 很多人会这样理解负载均值：三个数分别代表不同时间段的系统平均负载（一分钟、五分钟、以及十五分钟），它们的数字当然是越小越好。数字越高，说明服务器的负载越大，这也可能是服务器出现某种问题的信号。 而事实不完全如此，是什么因素构成了负载均值的大小，以及如何区分它们目前的状况是「好」还是「糟糕」？什么时候应该注意哪些不正常的数值？ 回答这些问题之前，首先需要了解下这些数值背后的些知识。我们先用最简单的例子说明，一台只配备一块单核处理器的服务器。 行车过桥一只单核的处理器可以形象得比喻成一条单车道。设想下，你现在需要收取这条道路的过桥费 – 忙于处理那些将要过桥的车辆。你首先当然需要了解些信息，例如车辆的载重、以及还有多少车辆正在等待过桥。如果前面没有车辆在等待，那么你可以告诉后面的司机通过。如果车辆众多，那么需要告知他们可能需要稍等一会。 因此，需要些特定的代号表示目前的车流情况，例如： 0.00 表示目前桥面上没有任何的车流。实际上这种情况与 0.00 和 1.00 之间是相同的，总而言之很通畅，过往的车辆可以丝毫不用等待的通过。 1.00 表示刚好是在这座桥的承受范围内。这种情况不算糟糕，只是车流会有些堵，不过这种情况可能会造成交通越来越慢。 超过 1.00，那么说明这座桥已经超出负荷，交通严重的拥堵。那么情况有多糟糕？例如 2.00 的情况说明车流已经超出了桥所能承受的一倍，那么将有多余过桥一倍的车辆正在焦急的等待。3.00 的话情况就更不妙了，说明这座桥基本上已经快承受不了，还有超出桥负载两倍多的车辆正在等待。 上面的情况和处理器的负载情况非常相似。一辆汽车的过桥时间就好比是处理器处理某线程的实际时间。Unix 系统定义的进程运行时长为所有处理器内核的处理时间加上线程在队列中等待的时间。 和收过桥费的管理员一样，你当然希望你的汽车（操作）不会被焦急的等待。所以，理想状态下，都希望负载平均值小于 1.00。当然不排除部分峰值会超过 1.00，但长此以往保持这个状态，就说明会有问题，这时候你应该会很焦急。 系统负荷的经验法则所以你说的理想负荷为 1.00？ 嗯，这种情况其实并不完全正确。负荷 1.00 说明系统已经没有剩余的资源了。在实际情况中，有经验的系统管理员都会将这条线划在 0.70： 「需要进行调查法则」：如果长期你的系统负载在 0.70 上下，那么你需要在事情变得更糟糕之前，花些时间了解其原因。 「现在就要修复法则」：1.00。如果你的服务器系统负载长期徘徊于 1.00，那么就应该马上解决这个问题。否则，你将半夜接到你上司的电话，这可不是件令人愉快的事情。 「凌晨三点半锻炼身体法则」：5.00。如果你的服务器负载超过了 5.00 这个数字，那么你将失去你的睡眠，还得在会议中说明这情况发生的原因，总之千万不要让它发生。 多处理器那么多个处理器呢？我的均值是 3.00，但是系统运行正常！ 在多处理器系统中，负载均值是基于内核的数量决定的。以 100% 负载计算，1.00 表示单个处理器，而 2.00 则说明有两个双处理器，那么 4.00 就说明主机具有四个处理器。 所以，2 个 CPU 表明系统负荷可以达到 2.0，此时每个 CPU 都达到 100% 的工作量。推广开来，n 个 CPU 的电脑，可接受的系统负荷最大为 n.0。 多核与多处理器先脱离下主题，我们来讨论下多核心处理器与多处理器的区别。从性能的角度上理解，一台主机拥有多核心的处理器与另台拥有同样数目的处理性能基本上可以认为是相差无几。当然实际情况会复杂得多，不同数量的缓存、处理器的频率等因素都可能造成性能的差异。 但即便这些因素造成的实际性能稍有不同，其实系统还是以处理器的核心数量计算负载均值。这使我们有了两个新的法则：「有多少核心即为有多少负荷」法则：在多核处理中，你的系统均值不应该高于处理器核心的总数量。 「核心的核心」法则：核心分布在分别几个单个物理处理中并不重要，其实两颗四核的处理器等于四个双核处理器等于八个单处理器。所以，它应该有八个处理器内核。 在 Linux 下，可以使用 1cat/proc/cpuinfo 获取你系统上的每个处理器的信息。如果你只想得到数字，那么就使用下面的命令： 1grep'modelname'/proc/cpuinfo|wc-l 最佳观察时长“load average” 一共返回三个平均值 —1 分钟系统负荷、5 分钟系统负荷，15 分钟系统负荷，应该参考哪个值？ 如果只有 1 分钟的系统负荷大于 1.0，其他两个时间段都小于 1.0，这表明只是暂时现象，问题不大。 如果 15 分钟内，平均系统负荷大于 1.0（调整 CPU 核心数之后），表明问题持续存在，不是暂时现象。所以，你应该主要观察 “15 分钟系统负荷”，将它作为电脑正常运行的指标。 参考博文[1]. 理解 Linux 系统负荷[2]. Understanding Linux CPU Load - when should you be worried? Linux 工匠精神系列 Linux 工匠精神（一）：Systemd 命令完全指南 Linux 工匠精神（二）：理解 Linux 的处理器负载均值]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>系统负载</tag>
        <tag>uptime</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解 Java 虚拟机（三）：内存分配与回收策略]]></title>
    <url>%2Farchives%2F2dc6708f.html</url>
    <content type="text"><![CDATA[概述Java 技术体系所提倡的自动内存管理最终可以归纳为自动化的解决两个问题：给对象分配内存以及回收分配给对象的内存。关于回收内存这一点，在本系列的第二章介绍了虚拟机中的垃圾收集器体系以及运作原理，本小节我们主要探讨一下 HotSpot 虚拟机在 Java 堆中对象分配、布局和访问全过程以及内存分配与回收策略。 对象的内存分配，往大的方向讲，就是在堆上分配（但也可能经过 JIT 编译后被拆散为标量类型并间接的在栈上分配），对象主要分配在新生代 Eden 区上，如果启动了本地线程分配缓冲，将按线程优先在 TLAB（本地线程分配缓冲）上分配。少数情况下，也可能直接分配在老年代中，分配的规则并不是百分之百固定的，其细节取决于当前使用的是哪一种垃圾收集器组合，还有虚拟机中与内存相关的参数的设置。 本节下面的代码在测试时使用 Client 模式虚拟机运行，没有手工指定收集器组合，换句话说，验证的是在使用 Serial / SerialOld 收集器下（ParNew / SerialOld 收集器组合的规则也基本一致）的内存分配和回收策略。 HotSpot 虚拟机在 Java 堆中对象分配、布局和访问全过程对象的创建虚拟机遇到一条 new 指令时，首先将去检查这个指令的参数是否能在常量池中定位到一个类的符号引用，并检查这个符号引用代表的类是否已被加载、解析和初始化过。如果还没有，那必须先执行相应的类加载过程。 在类加载检查通过后，接来下虚拟机将为新生对象分配内存。对象所需内存的大小在类加载完成后便可完全确定，为对象分配空间的任务等同于把一块确定大小的内存从 Java 堆中划分出来。假设 Java 堆中内存是绝对规整的，所有用过的内存都放在一边，空闲的内存放在另一边，中间放着一个指针作为分界点的指示器，那所分配内存就仅仅是把那个指针向空闲空间那边挪动一段与对象大小相等的距离，这种分配方式称为指针碰撞“（Bump the Pointer）。如果 Java 堆中的内存并不是规整的，已使用的内存和空闲的内存相互交错，那么虚拟机就必须维护一个列表，记录上哪些内存块是可用的，在分配的时候从列表中找到一块足够大的空间划分为对象实例，并更新列表上的记录，这种分配方式称为“空闲列表”（Free List）。选择哪种分配方式由 Java 堆是否规整决定，而 Java 堆是否规整又由所采用的垃圾收集器是否带有压缩整理功能决定。因此，在使用 Serial、ParNew 等带 Compact 过程的收集器时，系统采用的分配算法是指针碰撞，而使用 CMS 这种基于 Mark-Sweep 算法的收集器时，通常采用空闲列表。 除如何划分可用空间这之外，还有另外一个需要考虑的问题是对象创建在虚拟机中是非常频繁的行为，即使是仅仅修改一个指针所指向的位置，在并发情况下也并不是线程安全的，可能出现正在给对象 A 分配内存，指针还没来得及修改，对象 B 又同时使用了原来的指针来分配内存的情况。解决这个问题有两种方案，一种是对分配内存空间的动作进行同步处理——实际上虚拟机采用 CAS 配上失败重试的方式保证更新操作的原子性；另一种是把内存分配的动作按照线程划分在不同的空间之中进行，即每个线程在 Java 堆中预先分配一块内存，称为本地线程分配缓冲（Thread Local Alloccation Buffer，TLAB）。哪个线程要分配内存，就在哪个线程的 TLAB 上分配，只有 TLAB 用完并分配新的 TLAB时，才需要同步锁定。虚拟机是否使用 TLAB，可以通过 -XX:+/-UseTLAB 参数来决定。 对象的内存布局在 HotSpot 虚拟机中，对象在内存中存储的布局可以分为 3 块区域：对象头（Header）、实例数据（Instance Data）和对其填充（Padding）。 HotSpot 虚拟机的对象头包括两部分信息，第一部分用于存储对象自身的运行时数据，如哈希码（HashCode）、GC 分代年龄、锁状态标志、线程持有的锁、偏向线程 ID、偏向时间戳，这部分数据的长度在 32 位和 64 位虚拟机（未开启压缩指针）中分别为 32 bit 和 64 bit，官方称它为“Mark Word”，Mark Word 被设计成一个非固定的数据结构以便在极小的空间内存存储尽量多的信息；对象头的另外一部分是类型指针，即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。 第二部分实例数据部分是对象真正存储的有效信息，也是在程序代码中所定义的各种类型的字段内容。无论是从父类继承下来的，还是在子类中定义的，都需要记录起来。 第三部分对其填充并不是必然存在的，也没有特别的含义，它仅仅起着占位符的作用。由于 HotSpot VM 的自动内存管理系统要求对象 起始地址必须是 8 字节的整数倍，换句话说，就是对象的大小必须是 8 字节的整数倍。因此，当对象实例数据部分没有对齐时，就需要通过对齐填充来补全。 对象的访问定位建立对象是为了使用对象，我们的 Java 程序需要通过栈上的 reference 数据来操作堆上的具体对象。由于 reference 类型在 Java 虚拟机规范中只规定了一个指向对象的引用，并没有定义和这个引用应该通过何种方式去定位、访问对堆中的对象的具体位置，所以对象访问方式也是取决于虚拟机实现而定义的。目前主流的方式方式有使用句柄和直接指针两种。 如果使用句柄访问的话，那么 Java 堆中将会划分出一块内存来作为句柄池，reference 中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自的具体地址信息。 若果使用直接指针访问，那么 Java 堆对象的布局中就必须考虑如何方式类型数据的相关信息，而 reference 中存储的直接就是对象地址。 这两种对象访问方式各有优势，使用句柄来访问的最大好处就是 reference 中存储的是稳定的句柄地址，在对象被移动（垃圾收集时移动对象是非常普遍的行为）时只会改变句柄中实例数据指针，而 reference 本身不需要修改；使用直接指针访问凡是的最大好处就是速度更快，它节省了一次指针定位的时间开销，由于对象的访问在 Java 中非常频繁，因此这类开销积少成多后也是一项非常可观的执行成本。因此，虚拟机 Sun HotSpot 使用的就是直接指针方式进行对象访问，但从整个软件开发的范围来看，各种语言和框架使用句柄来访问的情况也十分常见。 内存分配策略对象优先在 Eden 分配大多数情况下，对象在新生代 Eden 区中分配。当 Eden 区没有足够的空间进行分配时，虚拟机将发起一次 MonitorGC[1]。 代码清单 3-5 的 testAllocation()方法中，尝试分配 3 个 2MB 大小和 1 个 4MB 大小的对象，在运行时通过 -Xms20M、-Xmx20M、-Xmn10M 这 3 个参数限制了 Java 堆大小为 20MB，不可扩张，其中 10MB 分配给新生代，剩下的 10MB 分配给老年代。-XX:SurvivorRatio=8 决定了新生代中 Eden 区与一个 Survivor 区的空间比例是 8:1，从输出的结果也可以清楚地看到 “eden space 8192K、from space 1024K、to space 1024K” 的信息，新生代总可用空间为 9216K（Eden 区 + 1 个 Survivor 区的总容量）。 执行 testAllocation()中分配 allocation4 对象的语句时会发生一次 Minor Gc，这次 GC 的结果是新生代 6651KB 变为 148KB，而总内存占用量几乎没有减少（因为 allocation1、allocation2、allocation3 三个对象都是存活的，虚拟机几乎没有找到可回收的对象）。这次 GC 发生的原因是给 allocation4 分配内存的时候，发现 Eden 已经被占用了 6MB，剩余空间已不足以分配 allocation4 所需的 4MB 内存，因此发生 Minor GC。GC 期间虚拟机又发现已有的 3 个 3MB 大小的对象全部无法进入 Survivor 空间（Survivor 空间只有 1MB 大小），所以只好通过分配担保机制提前转移到老年代去。 这次 GC 结束后，4MB 的 allocation4 对象顺序分配在 Eden 中，因此程序执行完的结果是 Eden 占用 4MB（被 allocation4 占用），Survivor 空闲，老年代被占用 6MB（被 allocation1、allocation2、allocation3 占用）。 大对象直接进入老年代所谓的大对象是指，需要大量连续内存空间的 Java 对象，最典型的大对象就是那种很长的字符串以及数组（上例中的 byte[] 数组就是典型的大对象）。大对象对虚拟机的内存分配来说就是一个坏消息（比遇到大对象更加块的消息就是遇到一群 “照生夕灭” 的“短命大对象”，写程序的时候应当避免），经常出现大对象容易导致内存还有不少空间时就提前触发垃圾收集以获取足够的连续空间来 “安置” 它们。 虚拟机提供了一个—XX:PretenureSizeThreshold[2] 参数，令大于这个设置指的对象直接在老年代分配。这样做的目的是避免在 Eden 区及两个 Survivor 区之间发生大量的内存复制（新生代采用复制算法收集内存）。 执行代码清单 testPretenureSizeThreshold()方法后，我们看到 Eden 空间几乎没有被使用，而老年代的 10MB 空间被使用了 40%，也就是 4MB 的 allocation 对象直接就分配在老年代中，这是因为 PretenureSizeTheashold 被设置为 3MB（就是 3145728，这个参数不能像—Xmx 之类的参数一样直接写 3MB），因此超过 3MB 的对象都会直接在老年代进行分配。 长期存活的对象将进入老年代既然虚拟机采用了分代收集的思想来管理内存，那么内存回收时就必须能识别哪些对象应放在新生代，哪些对象应放在老年代中。为了做到这点，虚拟机给每个对象定义了一个对象年龄 (Age) 计数器。如果对象在 Eden 出生并经过第一次 Minor GC 后仍然存过，并且能被 Survivor 区容纳的话，将被移动到 Survivor 空间中，并且对象年龄设为 1。对象在 Survivor 区中每 “熬过” 一次 Minor GC，年龄就增加 1 岁，当它的年龄增加到一定程度（默认为 15 岁），就将会被晋升到老年代中。对象晋升老年代的年龄阀值，可以通过参数 -XX:MaxTenuringThreshold 设置。 动态对象年龄判定为了能更好地适应不同程序的内存状况，虚拟机并不是永远地要求对象的年龄必须达到了 MaxTenuringThreshold 才能晋升老年代，如果 Survivor 空间中相同年龄所有对象大小的总和大于 Survivor 空间的一般，年龄大于或者等于该年龄的对象就可以直接进入老年代，无须等到 MaxTenuringThreshold 中要求的年龄。 空间分配担保在发生 Minor GC 之前，虚拟机会先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果这个条件成立，那么 Minor GC 可以确保是安全的。如果不成立，则虚拟机会查看 HandlePromotionfailure 设置值是否允许担保失败。如果允许，那么会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试着进行一次 Minor GC，尽管这次 Minor GC 是有风险的；如果小于，或者 HandlePromotionfailure 设置不允许冒险，那这时也要改为进行一次 Full GC。 在 JDK 6 Update 24 之后，HandlePromotionfailure 参数不会再影响到虚拟机的空间分配担保策略，虽然源码中还定义了 HandlePromotionfailure 参数，但是在代码中已经不会再使用它。JDK 6 Update 24 之后的规则变为只要老年代的连续空间大于新生代对象总大小或者历次晋升的平均大小就会进行 Minor GC，否则将进行 Full GC。 参考博文[1].《深入理解 Java 虚拟机：JVM 高级特效与最佳实现》，第 3 章 注脚[1]. Monitor GC 和 Full GC 有什么不一样？新生代 GC（Monitor GC）：指发生在新生代的垃圾收集动作，因为 Java 对象大多都具备朝生夕灭的特征，所有 Monitor GC 非常频繁，一般回收速度也比较快。老年代 GC（Major GC / Full GC）：指发生在老年代的垃圾收集动作，出现了 Major GC，经常会伴随至少一次的 Monitor GC（但非绝对的，在 ParallelScavenge 收集器的收集策略里就有直接进行 Major GC 的策略选择过程）。Major GC 的速度一般会比 Monitor GC 慢 10 倍以上。[2]. -XX:PretenureSizeThreshold：PretenureSizeThreshold 参数只对 Serial 和 ParNew 两款收集器有效，Parallel Scavenge 收集器不认识这个参数，Parallel Scavenge 一般不需要设置。如果遇到必须使用此参数的场合，可以考虑 ParNew 加 CMS 的收集器组合。 深入理解 Java 虚拟机系列 深入理解 Java 虚拟机（一）：Java 内存区域与内存溢出异常 深入理解 Java 虚拟机（二）：JVM 垃圾收集器 深入理解 Java 虚拟机（三）：内存分配与回收策略 深入理解 Java 虚拟机（四）：Jvm 性能监控与调优]]></content>
      <categories>
        <category>Jvm</category>
      </categories>
      <tags>
        <tag>Jvm</tag>
        <tag>内存分配</tag>
        <tag>回收策略</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一网打尽 NoSQL：当下 NoSQL 类型、适用场景及使用公司]]></title>
    <url>%2Farchives%2F7475b55d.html</url>
    <content type="text"><![CDATA[前言在互联网和大数据的背景下，越来越多的网站、应用系统需要支撑海量数据存储、高并发请求、高可用、高可扩展性等特性要求。传统的关系型数据库 RDBMS 已经难以应对类似的需求，各种各样的 NoSQL（Not Only SQL）数据库凭借易扩展、大数据量和高性能以及灵活的数据模型成功的在数据库领域站稳了脚跟。本文将分析传统数据库的存在的问题，以及几类 NoSQL 如何解决这些问题。在不同的业务场景下，作出正确的数据存储技术选型。 传统数据库缺点 缺点 解释说明 大数据场景下 I/O 较高 因为数据是按行存储，即使只针对其中某一列进行运算，关系型数据库也会对整行数据进行扫描，从存储设备中读入内存，导致 I/O 较高 结构化存储不够灵活 存储的是行记录，无法存储灵活的数据结构 表结构 schema 扩展不方便 如要需要修改表结构，需要执行执行 DDL（data definition language）语句修改，修改期间会导致锁表，部分服务不可用。 全文搜索功能较弱 关系型数据库只能够进行子字符串的匹配查询，当表的数据逐渐变大的时候，即使在有索引的情况下，like 扫表查询的匹配会非常慢 难以存储和处理复杂关系型数据 传统的关系数据库，并不擅长处理数据点之间的关系 NoSQL 简介NoSQL，指的是非关系型的数据库，可以理解为关系型数据库的一个有力补充。NoSQL 有时也称作 Not Only SQL 的缩写，是对不同于传统的关系型数据库的数据库管理系统的统称，它具有非关系型、分布式、不提供 ACID 的数据库设计模式等特征。 NoSQL 用于超大规模数据的存储（例如谷歌或 Facebook 每天为他们的用户收集万亿比特的数据）。这些类型的数据存储不需要固定的模式，无需多余操作就可以横向扩展。 NoSQL 在许多方面性能大大优于非关系型数据库的同时，往往也伴随一些特性的缺失。比较常见的是事务功能的缺失。数据库事务正确执行的四个基本要素 ACID 如下： 名称 描述 A Atomicity（原子性） 一个事务中的所有操作，要么全部完成，要么全部不完成，不会在中间某个环节结束。事务在执行过程中发生错误，会被回滚到事务开始前的状态，就像这个事务从来没有执行过一样。 C Consistency（一致性） 在事务开始之前和事务结束以后，数据库的完整性没有被破坏。 I Isolation（隔离性） 数据库允许多个并发事务同时对数据进行读写和修改的能力。隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。 D Durability（持久性） 事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 目前大家基本认同将 NoSQL 数据库分为四大类： 键值（Key-Value）数据库、列存储（Wide Column Store/Column-Family）数据库、 图（Graph-Oriented）数据库以及面向文档（Document-Oriented）数据库，其中每一种类型的数据库都能够解决关系型数据不能解决的问题。在实际应用中，NoSQL 数据库的分类界限其实没有那么明显，往往会是多种类型的组合体。 键值（Key-Value）数据库使用键值（key-value）存储的数据库，其数据按照键值对的形式进行组织、索引和存储。KV 存储非常适合不涉及过多数据关系业务的数据。它能够有效减少读写磁盘的次数，比关系型数据库存储拥有更好的读写性能，能够解决关系型数据库无法存储的数据结构的问题。 常见 K-V 数据库 ：Redis、Memcached、LevelDB、Cassandra RedisRedis 是一个使用 ANSI C 编写的开源、支持网络、基于内存、可选持久性的键值对存储数据库。Redis 是目前最流行的键值对存储数据库之一。 最佳应用场景：适用于数据变化快且数据库大小可遇见（适合内存容量）的应用程序。 例如：股票价格、数据分析、实时数据搜集、实时通讯。 MemcachedMemcached 是一个开放源代码、高性能、分配的内存对象缓存系统。用于加速动态 web 应用程序，减轻关系型数据库负载。它可以应对任意多个连接，使用非阻塞的网络 IO。由于它的工作机制是在内存中开辟一块空间，然后建立一个 Hash 表，Memcached 自管理这些 Hash 表。Memcached 简单而强大。它简单的设计促进迅速部署，易于发现所面临的问题，解决了很多大型数据缓存。 CassandraApache Cassandra（社区内一般简称为 C*）是一套开源的分布式 NoSQL 数据库系统。它最初由 Facebook 开发，用于储存收件箱等简单格式数据，集 Google BigTable 的数据模型与 Amazon Dynamo 的完全分布式架构于一身。Cassandra 是一种流行的分布式结构化数据存储方案。 最佳应用场景：当使用写操作多过读操作（记录日志） 例如：银行业，金融业（虽然对于金融交易不是必须的，但这些产业对数据库的要求会比它们更大）写比读更快，所以一个自然的特性就是实时数据分析 LevelDBLevelDB 是一个由 Google 所研发的键／值对（Key/Value Pair）嵌入式数据库管理系统编程库，以开源的 BSD 许可证发布。LevelDB 是一个基于本地文件的存储引擎，非分布式存储引擎，原理基于 BigTable（LSM 文件树），无索引机制，存储条目为 Key-value。适用于保存数据缓存、日志存储、高速缓存等应用，主要是避免 RPC 请求带来的延迟问题。 相关特性K-V 数据库的相关特性，以 Redis 为例说明：： 优点 性能极高：Redis 单机最高能支持超过 10W 的 TPS。 丰富的数据类型：Redis 支持包括 String，Hash，List，Set，SortedSet，Bitmap 和 Hyperloglog 等数据结构。 丰富的特性：Redis 还支持 publish/subscribe，通知，key 过期等特性。 缺点 Redis 事务不能支持原子性和持久性（A 和 D），只支持隔离性和一致性（I 和 C）。 应用场景适用场景内容缓存，主要用于处理大量数据的高访问负载，也用于一些日志系统等等。适合存储用户信息（比如会话）、配置文件、参数、购物车等等。这些信息一般都和 ID 挂钩。 不适用场景 不适合需要通过值来查询，而不是键来查询。Key-Value 数据库中根本没有通过值查询的途径。 不适合需要储存数据之间的关系。在 Key-Value 数据库中不能通过两个或以上的键来关联数据。 不适合需要支持事务的场景。在 Key-Value 数据库中故障产生时不可以进行回滚。 列存储（Wide Column Store/Column-Family）数据库列式数据库是以列相关存储架构进行数据存储的数据库，主要适合于批量数据处理和即时查询。相对应的是行式数据库，数据以行相关的存储架构进行空间分配，主要适合于小批量的数据处理，常用于联机事务型数据处理。基于列式数据库的列存储特性，可以解决某些特定场景下关系型数据库高 I/O 的问题。 常见列式数据库 ：HBase、BigTable HBaseHBase 是一个开源的非关系型分布式数据库（NoSQL），它参考了谷歌的 BigTable 建模，实现的编程语言为 Java。它是 Apache 软件基金会的 Hadoop 项目的一部分，运行于 HDFS 文件系统之上，为 Hadoop 提供类似于 BigTable 规模的服务。因此，它可以容错地存储海量稀疏的数据。 最佳应用场景：适用于偏好 BigTable；对数据有版本查询需求，并且需要对大数据进行随机、实时访问的场合。 例如：Facebook 消息数据库 BigTableBigTable 是一种压缩的、高性能的、高可扩展性的，基于 Google 文件系统（Google File System，GFS）的数据存储系统，用于存储大规模结构化数据，适用于云计算。 相关特性优点 高效的储存空间利用率：列式数据库针对不同列的数据特征而发明了不同算法，使其比行式数据库高的多的压缩率。普通的行式数据库一般压缩率在 3：1 到 5：1 左右，而列式数据库的压缩率一般在 8：1 到 30：1 左右。 查询效率高：读取多条数据的同一列效率高，因为这些列都是存储在一起的，一次磁盘操作可以把数据的指定列全部读取到内存中。 适合做聚合操作 适合大量的数据而不是小数据 缺点 不适合扫描小量数据 不适合随机的更新 不适合做含有删除和更新的实时操作 单行数据支持 ACID 的事务操作，多行数据的事务操作，不支持事务的正常回滚，支持（Isolation）隔离性、(Durability）持久性，不能保证(Atomicity）原子性、（Consistency）一致性 应用场景列数据库的适用场景，以 HBase 为例说明： 适合大数据量(100TB 级数据），有快速随机访问的需求。 适合写密集型应用，每天写入量巨大，而读数量相对较小的应用，比如 IM 的历史消息，游戏日志等等。 适合不需要复杂查询条件来查询数据的应用。HBase 只支持基于 rowkey 的查询，对于 HBase 来说，单条记录或者小范围的查询是可以接受的。大范围的查询由于分布式的原因，可能在性能上有点影响。HBase 不适用于有 join，多级索引，表关系复杂的数据模型。 对性能和可靠性要求非常高的应用。 由于 HBase 本身没有单点故障，可用性非常高。 适合数据量较大，而且增长量无法预估的应用，需要进行优雅的数据扩展的应用。HBase 支持在线扩展，即使在一段时间内，数据量呈井喷式增长，也可以通过 HBase 横向扩展来满足功能。 存储结构化和半结构化的数据。 图（Graph-Oriented）数据库图形数据库应用图形理论存储实体之间的关系信息。最常见例子就是社会网络中人与人之间的关系。关系型数据库用于存储这种关系型数据的效果并不好，其查询复杂、缓慢、超出预期。图形数据库的独特设计弥补了这个缺陷，解决关系型数据库存储和处理复杂关系型数据功能较弱的问题。 常见图形数据库 ：Neo4j、ArangoDB Neo4jNeo4j 是一个高性能的，NOSQL 图形数据库，它将结构化数据存储在 “图形网络上” 而不是“表中”。它是一个嵌入式的、基于磁盘的、具备完全的事务特性的 Java 持久化引擎。Neo4j 也可以被看作是一个高性能的图引擎。 最佳应用场景：适用于图形一类数据。这是 Neo4j 与其他 nosql 数据库的最显著区别 例如：社会关系，公共交通网络，地图及网络拓谱 ArangoDBArangoDB 是一个原生多模型数据库系统。数据库系统支持三个重要的数据模型（键 / 值，文档，图形）。ArangoDB 包含一个数据库核心和统一查询语言 AQL（ArangoDB 查询语言）。查询语言是声明性的，允许在单个查询中组合不同的数据访问模式。ArangoDB 是一个 NoSQL 数据库系统，但 AQL 在很多方面与 SQL 都类似。 相关特性优点 高性能表现：图的遍历是图数据结构所具有的独特算法，即从一个节点开始，根据其连接的关系，可以快速和方便地找出它的邻近节点。这种查找数据的方法不受数据量大小的影响，因为邻近查询始终查找的是有限的局部数据，不会对整个数据库进行搜索。利用图结构相关算法。比如最短路径寻址，N 度关系查找等。 设计的灵活性：数据结构的自然伸展特性，以及其非结构化的数据格式，让图数据库设计可以具有很大的伸缩性和灵活性。因为随着需求的变化而增加的节点、关系及其属性，并不会影响到原来数据的正常使用。 开发的敏捷性：数据模型直接明了，从需求的讨论开始，到程序开发和实现，基本上不会有大的变化。 完全支持 ACID：不像别的 NoSQL 数据库，Neo4j 还完全具有事务管理特性，完全支持 ACID 事务管理。 缺点 节点，关系和它们的属性的数量被限制。 不支持拆分，图数据库结构不太好做分布式的集群方案。 应用场景适用场景专注于构建关系图谱，善于处理大量复杂、互连接、低结构化的数据，数据往往变化迅速，且查询频繁。（1）在一些关系性强的数据应用，例如社交网络。（2）推荐引擎，将数据以图的形式表现，非常有益于推荐的制定。 不适用场景 记录大量基于事件的数据，如日志记录、传感器数据。 对大规模分布式数据进行处理，类似于 Hadoop。 不适用于应该保存在关系型数据库中的结构化数据。 二进制数据存储。 面向文档（Document-Oriented）数据库文档数据库用于将半结构化数据存储为文档的一种数据库。文档数据库通常以 JSON 或 XML 格式存储数据。（1）由于文档数据库的 no-schema 特性，可以存储和读取任意数据。（2）由于使用的数据格式是 JSON 或者 BSON，因为 JSON 数据是自描述的，无需在使用前定义字段，读取一个 JSON 中不存在的字段也不会导致 SQL 那样的语法错误，可以解决关系型数据库表结构 schema 扩展不方便的问题。 常见文档数据库 ：MongoDB、ArangoDB MongoDBMongoDB 是一个基于分布式文件存储的数据库。由 C++ 语言编写。旨在为 WEB 应用提供可扩展的高性能数据存储解决方案。MongoDB 是一个介于关系数据库和非关系数据库之间的产品，是非关系数据库当中功能最丰富，最像关系数据库的 NoSQL。 最佳应用场景：适用于需要动态查询支持；需要使用索引而不是 map/reduce 功能；需要对大数据库有性能要求；需要使用 CouchDB 但因为数据改变太频繁而占满内存的应用程序。 例如：你本打算采用 MySQL 或 PostgreSQL，但因为它们本身自带的预定义栏让你望而却步。 CouchDBCouchDB 是用 Erlang 开发的面向文档的分布式数据库，用于存储半结构化的数据，比较类似 lucene 的 index 结构。CouchDB 支持 RESTful API，它使用 JSON 作为存储格式，JavaScript 作为查询语言，MapReduce 和 HTTP 作为 API 的 NoSQL 数据库。其中一个显著的功能就是多主复制功能。除此之外，CouchDB 构建在强大的 B - 树储存引擎之上。 最佳应用场景：适用于数据变化较少，执行预定义查询，进行数据统计的应用程序。适用于需要提供数据版本支持的应用程序。 例如： CRM、CMS 系统。 master-master 复制对于多站点部署是非常有用的。 相关特性文档型数据库的相关特性，以 MongoDB 为例进行说明： 优点 新增字段简单不需要像关系型数据库一样，先执行 DDL 语句修改表结构，程序代码直接读写即可。 容易兼容历史数据。对于历史数据，即使没有新增的字段，也不会导致错误，只会返回空值，此时代码兼容处理即可。 易存储复杂数据。JSON 是一种强大的描述语言，能够描述复杂的数据结构。 缺点 Atomicity（原子性）：仅支持单行 / 文档级原子性，不支持多行、多文档、多语句原子性。 Isolation（隔离性）：隔离级别仅支持已提交读（Read committed）级别，可能导致不可重复读，幻读的问题。 不支持复杂查询。例如 join 查询，如果需要 join 查询，需要多次操作数据库。 应用场景适用场景 数据量很大或者未来会变得很大。 表结构不明确，且字段在不断增加，例如内容管理系统，信息管理系统。 不适用场景 在不同的文档上需要添加事务。Document-Oriented 数据库并不支持文档间的事务。 多个文档之间需要复杂的查询，例如 join 操作。 参考博文[1]. 浅谈常见的 NoSQL 技术方案和选型]]></content>
      <categories>
        <category>NoSQL</category>
      </categories>
      <tags>
        <tag>NoSQL</tag>
        <tag>RDB</tag>
        <tag>ACID</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java8 那些事儿（三）：Date/Time API(JSR 310)]]></title>
    <url>%2Farchives%2Fd66032e3.html</url>
    <content type="text"><![CDATA[前言Java8 引入了新的 Date-Time API(JSR 310)来改进时间、日期的处理。时间和日期的管理一直是最令 Java 开发者痛苦的问题。java.util.Date 和后来的 java.util.Calendar 一直没有解决这个问题（甚至令开发者更加迷茫）。 因为上面这些原因，诞生了第三方库 Joda-Time，可以替代 Java 的时间管理 API。Java8 中新的时间和日期管理 API 深受 Joda-Time 影响，并吸收了很多 Joda-Time 的精华。新的 java.time 包包含了所有关于瞬时时间（Instant），持续时间（Duration），日期（Date），时间（Time），时区（Time-Zone）以及时间段（Period）操作的类。新设计的 API 认真考虑了这些类的不变性（从 java.util.Calendar 吸取的教训），如果某个实例需要修改，则返回一个新的对象。 ZoneId: 时区 ID，用来确定 Instant 和 LocalDateTime 互相转换的规则 Instant: 用来表示时间线上的一个点 LocalDate: 表示没有时区的日期，LocalDate 是不可变并且线程安全的 LocalTime: 表示没有时区的时间，LocalTime 是不可变并且线程安全的 LocalDateTime: 表示没有时区的日期时间，LocalDateTime 是不可变并且线程安全的 Clock: 用于访问当前时刻、日期、时间，用到时区 Duration: 用秒和纳秒表示时间的数量 最常用的就是 LocalDate、LocalTime、LocalDateTime 了，从它们的名字就可以看出是操作日期和时间的。 LocalDateLocalDate 是一个不可变的类，它表示默认格式 (yyyy-MM-dd) 的日期，我们可以使用 now()方法得到当前时间，也可以提供输入年份、月份和日期的输入参数来创建一个 LocalDate 实例。该类为 now()方法提供了重载方法，我们可以传入 ZoneId 来获得指定时区的日期。 在 Java8 中如何获取当天的日期? 1234567891011LocalDate localDate = LocalDate.now();System.out.println("Today's Local date : " + localDate);// Output// Today's Local date : 2017-12-07LocalDate todayKolkata = LocalDate.now(ZoneId.of("Asia/Kolkata"));System.out.println("Today's local date in Kolkata is : " + todayKolkata);// Output// Today's local date in Kolkata is : 2017-12-07 在 Java8 中如何获取当前的年月日? 12345678LocalDate today = LocalDate.parse("2017-12-07");int year = today.getYear();int month = today.getMonthValue();int day = today.getDayOfMonth();System.out.printf("Year : %d Month : %d day : %d \t %n", year, month, day);// Output// Year : 2017 Month : 12 day : 7 在 Java8 中如何获取某个特定的日期？ 1234567891011121314151617181920// 1. 根据年月日取日期LocalDate dateOfBirth = LocalDate.of(1994, 11, 15);// 根据字符串取: 严格按照 ISO yyyy-MM-dd 验证LocalDate dateOfBirth = LocalDate.of("1994-11-15");System.out.println("Your Date of birth is :" + dateOfBirth);// Output// Your Date of birth is : 1994-11-15// 2. 获取 2017 年的第 100 天的日期LocalDate hundredDay2017 = LocalDate.ofYearDay(2017, 100);System.out.println("Hundred Day Of 2014 is :" + hundredDay2017);// Output// Hundred Day Of 2014 is : 2017-04-10// 3. 根据有效输入创建日期, 以下代码会抛异常, 无效输入, 2017 年 2 月没有 29 日LocalDate feb29_2017 = LocalDate.of(2017, Month.FEBRUARY, 29);// Exception in thread "main" java.time.DateTimeException:Invalid date 'February 29' as '2014' is not a leap year 在 Java8 中如何检查两个日期是否相等？ 12345678LocalDate today = LocalDate.parse("2017-12-07");LocalDate date1 = LocalDate.of(2017, 12, 7);if (date1.equals(today)) &#123; System.out.printf("Today %s and date1 %s are same date %n", today, date1);&#125;// Output// Today 2017-12-07 and date1 2017-12-07 are same date 在 Java8 中如何检查重复事件，比如说生日? 123456789101112LocalDate today = LocalDate.parse("2017-12-07");LocalDate dateOfBirth = LocalDate.of(1994, 11, 15);MonthDay birthday = MonthDay.of(dateOfBirth.getMonth(), dateOfBirth.getDayOfMonth());MonthDay currentMonthDay = MonthDay.from(today);if (currentMonthDay.equals(birthday)) &#123; System.out.println("Many Many happy returns of the day !!");&#125; else &#123; System.out.println("Sorry, today is not your birthday");&#125;// Output// Sorry, today is not your birthday 在 Java8 中如何获取 1 周后的日期? 1234567891011121314LocalDate today = LocalDate.parse("2017-12-12");LocalDate nextWeek = today.plus(1, ChronoUnit.WEEKS);LocalDate plusWeek = today.plusWeeks(2);LocalDate periodWeek = today.plus(Period.ofWeeks(3));System.out.println("Today is :" + today);System.out.println("Date after 1 week :" + nextWeek);System.out.println("Date after 2 week :" + plusWeek);System.out.println("Date after 3 week :" + periodWeek);// Output// Today is : 2017-12-12// Date after 1 week : 2017-12-19// Date after 2 week : 2017-12-26// Date after 3 week : 2017-01-02 在 Java8 中如何获取一年前后的日期？ 123456789LocalDate today = LocalDate.parse("2017-12-12");LocalDate previousYear = today.minus(1, ChronoUnit.YEARS);System.out.println("Date before 1 year :" + previousYear);LocalDate nextYear = today.plus(1, ChronoUnit.YEARS);System.out.println("Date after 1 year :" + nextYear);// Output// Date before 1 year : 2016-12-12// Date after 1 year : 2017-12-12 在 Java8 中如何检查闰年？ 123456789LocalDate today = LocalDate.now();if (today.isLeapYear()) &#123; System.out.println("This year is Leap year");&#125; else &#123; System.out.println("2017 is not a Leap year");&#125;// Output// 2017 is not a Leap year 在 Java8 中如何获取这个月的第一天？ 123456789LocalDate today = LocalDate.parse("2017-12-12");LocalDate firstDayOfMonth = today.with(TemporalAdjusters.firstDayOfMonth());LocalDate firstDayOfMonth2 = today.withDayOfMonth(1);System.out.println("First Day Of Month is :" + firstDayOfMonth);System.out.println("First Day Of Month is :" + firstDayOfMonth2);// Output// First Day Of Month is : 2017-12-01// First Day Of Month is : 2017-12-01 在 Java8 中如何判断某个日期是在另一个日期的前面还是后面？ 12345678910111213LocalDate today = LocalDate.parse("2017-12-12");LocalDate tomorrow = LocalDate.of(2017, 12, 13);LocalDate yesterday = today.minus(1, ChronoUnit.DAYS);if (tomorrow.isAfter(today)) &#123; System.out.println("Tomorrow comes after today");&#125;if (yesterday.isBefore(today)) &#123; System.out.println("Yesterday is day before today");&#125;// Output// Tomorrow comes after today// Yesterday is day before today LocalTimeLocalTime 是一个不可变的类，它的实例代表一个符合人类可读格式的时间，默认格式是 hh:mm:ss.zzz。像 LocalDate 一样，该类也提供了时区支持，同时也可以传入小时、分钟和秒等输入参数创建实例。 在 Java8 中如何获取当天的时间? 12345678910111213// 1.Current TimeLocalTime time = LocalTime.now();System.out.println("local time now :" + time);// Output// local time now : 16:16:45.219// 2.Current date in "Asia/Kolkata", you can get it from ZoneId javadocLocalTime timeKolkata = LocalTime.now(ZoneId.of("Asia/Kolkata"));System.out.println("Current Time in IST :" + timeKolkata);// Output// Current Time in IST : 13:47:55.482 在 Java8 中如何获取当前的小时、分钟? 12345678LocalTime nowTime = LocalTime.parse("15:02:53");int hour = nowTime.getHour();int minute = nowTime.getMinute();int second = nowTime.getSecond();System.out.printf("Hour : %d Minute : %d Second : %d \t %n", hour, minute, second);// Output// Hour : 15 Minute : 2 Second : 53 在 Java8 中如何获取某个特定的时间？ 12345678910111213// 1.Creating LocalTime by providing input argumentsLocalTime specificTime = LocalTime.of(12, 20, 25);System.out.println("Specific Time of Day :" + specificTime);// Output// Specific Time of Day : 12:20:25// 2.Getting date from the base date i.e 01/01/1970LocalTime specificSecondTime = LocalTime.ofSecondOfDay(10000);System.out.println("10000th second time :" + specificSecondTime);// Output// 10000th second time : 02:46:40 LocalDateTimeLocalDateTime 是一个不可变的日期 - 时间对象，它表示一组日期 - 时间，默认格式是 yyyy-MM-ddTHH-mm-ss.zzz。它提供了一个工厂方法，通过接收 LocalDate 和 LocalTime 输入参数，来创建 LocalDateTime 实例。 在 Java8 中如何获取当天的日期时间? 12345678910111213141516171819202122232425262728293031323334// 1.Current DateLocalDateTime today = LocalDateTime.now();System.out.println("Current DateTime :" + today);// Output// Current DateTime : 2017-12-12T16:35:49.504// 2.Current Date using LocalDate and LocalTimetoday = LocalDateTime.of(LocalDate.now(), LocalTime.now());System.out.println("Current DateTime :" + today);// Output// Current DateTime : 2017-12-12T16:35:49.504// 3.Current date in "Asia/Kolkata", you can get it from ZoneId javadocLocalDateTime todayKolkata = LocalDateTime.now(ZoneId.of("Asia/Kolkata"));System.out.println("Current Date in IST :" + todayKolkata);// Output// Current Date in IST : 2017-12-12T14:08:39.289// 4.Current date in systemDefault, you can get it from ZoneId javadocLocalDateTime todayKolkata = LocalDateTime.now(ZoneId.systemDefault());System.out.println("Current Date in SystemDefault :" + todayKolkata);// Output// Current Date in SystemDefault : 2017-12-12T17:02:20.421// 5.Create LocalDateTime from LocalDateLocalDate localDate = LocalDate.now();System.out.println("Current Time :" + localDate.atTime(LocalTime.now()));// Output// Current Date in SystemDefault : 2017-12-12T17:02:20.421 InstantInstant 类是用在机器可读的时间格式上的，它以 Unix 时间戳 &lt; sup&gt;[1] 的形式存储日期时间。java.time.Instant 类在时间线上模拟单个瞬时点。Instant 对象包含两个值：秒数和纳秒数。其中秒数指的是 epoch 时间戳，而纳秒数指的是该秒内的纳秒时间。 在 Java8 中如何获取当前的时间戳？ 12345678910111213// 1.Current timestamp[Instant.now()使用等是 UTC 时间 Clock.systemUTC().instant(), 输出 2017-12-12T08:48:16.253Z 和北京时间相差 8 个时区]Instant timestamp = Instant.now();System.out.println("Current Timestamp :" + timestamp);// Output// Current Timestamp : 2017-12-12T08:48:16.253Z// 2.Instant from timestampInstant specificTime = Instant.ofEpochMilli(timestamp.toEpochMilli());System.out.println("Specific Time :" + specificTime);// Output// Specific Time : 2017-12-12T08:48:16.253Z 在 Java8 中如何获取从 1970 年 1 月 1 日到现在的毫秒？ 1234567891011121314151617181920// 1. 将此瞬间转换为 1970-01-01T00:00:00Z 时代的毫秒数。Instant instant = Instant.now();System.out.println(instant.toEpochMilli());// Output// 1544605635276// 2. 获取 1970-01-01T00:00:00Z 的 Java 纪元的秒数[Instant.getEpochSecond() 获取秒数部分，Instant.getNano() 获取纳秒部分。]Instant instant = Instant.now();System.out.println(instant.getEpochSecond());// Output// 1544605635// 3. 从时间线开始，获取从第二个开始的纳秒数Instant instant = Instant.now();System.out.println(instant.getNano());// Output// 276000000 在 Java8 中如何获取两个时间的时间差？ 12345Instant instant1 = Instant.now();Instant instant2 = instant1.plus(Duration.ofSeconds(100)); // 添加 100 秒System.out.println(instant2.isAfter(instant1)); // trueSystem.out.println(instant1.until(instant2, ChronoUnit.SECONDS)); // 100System.out.println(instant2.until(instant1, ChronoUnit.SECONDS)); // -100 Durationjava.time.Duration 类以秒和纳秒为单位模拟一个数量或时间量。可以使用其他基于持续时间的单位访问它，例如分钟和小时。Duration 表示以秒为单位的时长，精确到纳秒。 在 Java8 中如何获取持续时间？ 123456// 表示十九小时二十六分钟三十二点二六八秒Duration duration1 = Duration.parse("PT19H26M32.268S");System.out.println(duration1);// Output// PT19H26M32.268S 在 Java8 中如何获取两个时间的持续时间？ 12345678Duration duration1 = Duration.between(Instant.EPOCH, Instant.now());Duration duration2 = Duration.between(LocalTime.parse("00:00:00"), LocalTime.now());System.out.println(duration1);System.out.println(duration2);// Output// PT429059H31M34.837S// PT19H31M35.163S Periodjava.time.Period 类根据年，月和日来模拟一个数量或时间量。Period 表示以天为单位的时长，精确到天。 在 Java8 中如何获取时间段？ 123456789// 表示一年两个月零三天Period period1 = Period.parse("P1Y2M3D");Period period2 = Period.of(1, 2, 3);System.out.println(period1);System.out.println(period2);// Output// P1Y2M3D// P1Y2M3D 在 Java8 中如何获取两个日期的时间段？ 1234567891011LocalDate today = LocalDate.parse("2017-02-23");LocalDate lastDayOfYear = today.with(TemporalAdjusters.lastDayOfYear());Period period = today.until(lastDayOfYear);System.out.println("Period Format =" + period); // P1Y3M15D: 表示一年三个月十五天System.out.println("Months remaining in the year =" + period.getMonths());System.out.println("Days remaining in the year =" + period.getDays());// Output// Period Format = P10M8D// Months remaining in the year = 10// Days remaining in the year = 8 在 Java8 中如何具体计算两个时间点之间的秒数或天数？ 123456// 离那个什么中华民族的伟大复兴还有多少天long days = ChronoUnit.DAYS.between(LocalDate.now(), LocalDate.of(2049, 10, 1));System.out.println(days);// Output// 11251 DateTimerFormatter将一个日期格式转换为不同的格式，之后再解析一个字符串，得到日期时间对象。 在 Java8 中如何使用预定义的格式器来对日期进行解析 / 格式化？ 123456String dayAfterTommorrow = "20171216";LocalDate formatted = LocalDate.parse(dayAfterTommorrow, DateTimeFormatter.BASIC_ISO_DATE);System.out.printf("Date generated from String %s is %s %n", dayAfterTommorrow, formatted);// Output :// Date generated from String 20171216 is 2017-12-16 在 Java8 中如何使用自定义的格式器来解析日期？ 1234567891011121314String goodFriday = "十月 18 2014";DateTimeFormatter formatter = DateTimeFormatter.ofPattern("MMM dd yyyy");LocalDate holiday = LocalDate.parse(goodFriday, formatter);System.out.printf("Successfully parsed String %s, date is %s%n", goodFriday, holiday);// Output :// Successfully parsed String 十月 18 2014, date is 2014-10-18LocalDateTime dt = LocalDateTime.parse("27:: 四月::2014 21::39::48", DateTimeFormatter.ofPattern("d::MMM::uuuu HH::mm::ss"));System.out.println("Default format after parsing =" + dt);// Output :// Default format after parsing = 2014-04-27T21:39:48 在 Java8 中如何对日期进行格式化，转换成字符串？ 12345678910111213141516171819202122// Format examplesLocalDate date = LocalDate.now();// default formatSystem.out.println("Default format of LocalDate=" + date);// specific formatSystem.out.println(date.format(DateTimeFormatter.ofPattern("d::MMM::uuuu")));System.out.println(date.format(DateTimeFormatter.BASIC_ISO_DATE));LocalDateTime dateTime = LocalDateTime.now();// default formatSystem.out.println("Default format of LocalDateTime=" + dateTime);// specific formatSystem.out.println(dateTime.format(DateTimeFormatter.ofPattern("d::MMM::uuuu HH::mm::ss")));System.out.println(dateTime.format(DateTimeFormatter.BASIC_ISO_DATE));// Output :// Default format of LocalDate = 2017-12-12// 12:: 十二月::2017// 20171212// Default format of LocalDateTime = 2017-12-12T17:51:30.569// 12:: 十二月::2017 17::51::30// 20171212 旧的日期时间支持旧的日期 / 时间类已经在几乎所有的应用程序中使用，因此做到向下兼容是必须的。这也是为什么会有若干工具方法帮助我们将旧的类转换为新的类，反之亦然。 1.Date 和 Instant 互相转换 12345// Instant to DateDate date = Date.from(Instant.now());// Date to InstantInstant instant = date.toInstant(); 2.Date 和 LocalDateTime 相互转换 123456// Date to LocalDateTimeLocalDateTime localDateTime1 = LocalDateTime.ofInstant(new Date().toInstant(), ZoneId.systemDefault());LocalDateTime localDateTime2 = new Date().toInstant().atZone(ZoneId.systemDefault()).toLocalDateTime();// LocalDateTime to DateDate date = Date.from(LocalDateTime.now().atZone(ZoneId.systemDefault()).toInstant()); 3.Date 和 LocalDate 相互转换 12345// LocalDate to Date[LocalDate -&gt; LocalDateTime -&gt; Date]Date date = Date.from(LocalDate.now().atStartOfDay().atZone(ZoneId.systemDefault()).toInstant());// Date to LocalDateLocalDate localDate = new Date().toInstant().atZone(ZoneId.systemDefault()).toLocalDate(); 注脚[1].Unix 时间戳: unix 时间戳是从格林威治时间 1970 年 1 月 1 日（UTC/GMT 的午夜）开始所经过的秒数，不考虑闰秒。 Java8 那些事儿系列 Java8 那些事儿（一）：Stream 函数式编程 Java8 那些事儿（二）：Optional 类解决空指针异常 Java8 那些事儿（三）：Date/Time API(JSR 310) Java8 那些事儿（四）：增强的 Map 集合 Java8 那些事儿（五）：函数式接口]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java8</tag>
        <tag>Date-Time</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis 深度探险（一）：那些绕不过去的 Redis 知识点]]></title>
    <url>%2Farchives%2F8fb486e9.html</url>
    <content type="text"><![CDATA[前言Redis 是我们工作中接触最多的非关系型数据库，我所在的公司也是 Redis 的深度用户，我们线上的大部分的业务都使用到了 Redis。与传统数据库不同的是 Redis 的数据是存在内存中的，所以存写速度非常快，因此 Redis 被广泛应用于缓存方向。值得注意的是，Redis 也经常用来做分布式锁。Redis 提供了多种数据类型来支持不同的业务场景。除此之外，Redis 支持事务 、持久化、LUA 脚本、LRU 驱动事件、多种集群方案。以前在使用 Redis 的时候，只是简单地使用它提供的基本数据类型和接口，并没有深入研究它底层的数据结构。最近打算重新学习梳理一下 Redis 方面的知识。本篇是我学习 Redis 系列的开篇，主要内容讲述 Redis 概述及其相关内容，然后介绍一下 Redis 数据结构，最后介绍一下 Redis 对象以及应用场景。 Redis 概述Redis 是一个开源的使用ANSI C语言编写、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的API。 Redis 是互联网技术领域使用最为广泛的存储中间件，它是「Remote Dictionary Service」的首字母缩写，也就是「远程字典服务」。Redis 以其超高的性能、完美的文档、简洁易懂的源码和丰富的客户端库支持在开源中间件领域广受好评。 Redis 是一个开源（BSD 许可），内存存储的数据结构服务器，可用作数据库，高速缓存和消息队列代理。它支持字符串、哈希表、列表、集合、有序集合、位图、hyperloglogs 等数据类型。内置复制、LUA 脚本、LRU 收回、事务以及不同级别磁盘持久化功能，同时通过 Redis Sentinel 提供高可用，通过 Redis Cluster 提供自动分区。 Redis 为什么这么快1、完全基于内存，绝大部分请求是纯粹的内存操作，非常快速。数据存在内存中，类似于 HashMap，HashMap 的优势就是查找和操作的时间复杂度都是 O(1)； 2、数据结构简单，对数据操作也简单，Redis 中的数据结构是专门进行设计的； 3、采用单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 CPU，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗； 4、使用多路 I/O 复用模型[1]，非阻塞 IO； 5、使用底层模型不同，它们之间底层实现方式以及与客户端之间通信的应用协议不一样，Redis 直接自己构建了 VM 机制 ，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求； 主要由以上几点造就了 Redis 具有很高的吞吐量。 Redis 为什么是单线程的？因为 CPU 不是 Redis 的瓶颈。Redis 的瓶颈最有可能是机器内存或者网络带宽。既然单线程容易实现，而且 CPU 不会成为瓶颈，那就顺理成章地采用单线程的方案了。 Redis 数据结构与对象Redis 作为目前最流行的 Key-Value 类型的内存数据库，对于数据库的操作都在内存中进行，并可定期的将数据异步的持久化到磁盘之上。由于是纯内存的操作，因此它的性能比普通的关系型数据库高出很多，同时由于是单线程串行的执行指令，因此也避免了加锁和释放锁的开销。相比于 Memcache，Redis 的每个 value 值最大可存储 1GB，而 Memcache 只有 10MB，同时 Redis 在速度上也快于 Memcache，还可以持久化。Redis 最大的特点则是，它可以支持五种基本数据类型，分别是string（字符串），list（列表），hash（字典），set（集合）以及 zet（有序集合），下面具体介绍下它们的特点以及内部实现。 Redis 数据结构简单动态字符串Redis 没有直接使用 C 语言传统的字符串表示（以空字符结尾的字符数组，以下简称 C 字符串）， 而是自己构建了一种名为简单动态字符串（simple dynamic string，SDS）的抽象类型， 并将 SDS 用作 Redis 的默认字符串表示。 在 Redis 里面， C 字符串只会作为字符串字面量（string literal）， 用在一些无须对字符串值进行修改的地方， 比如打印日志；当 Redis 需要的不仅仅是一个字符串字面量， 而是一个可以被修改的字符串值时， Redis 就会使用 SDS 来表示字符串值： 比如在 Redis 的数据库里面， 包含字符串值的键值对在底层都是由 SDS 实现的。 SDS 的内部数据结构： 12345678910typedef struct sdshdr &#123; // 记录 buf 数组中已使用字节的数量：等于 SDS 所保存字符串的长度 int len; // 记录 buf 数组中未使用字节的数量 int free; // 字节数组，用于保存字符串 char buf[];&#125;; 可见，其底层是一个 char 数组。buf 最大容量为 512M，里面可以放字符串、浮点数和字节。为什么没有直接使用数组，而是包装成了这样的数据结构呢？因为 buf 会有动态扩容和缩容的需求。如果直接使用数组，那每次对字符串的修改都会导致重新分配内存，效率很低。 buf 的扩容过程如下： 如果修改后 len 长度将小于 1M， 这时分配给 free 的大小和 len 一样， 例如修改过后为 10 字节， 那么给 free 也是 10 字节，buf 实际长度变成了 10byte + 10byte + 1byte[2] 如果修改后 len 长度将大于等于 1M， 这时分配给 free 的长度为 1M， 例如修改过后为 30M， 那么给 free 是 1M.buf 实际长度变成了 30M + 1M + 1byte 重点回顾： Redis 只会使用 C 字符串作为字面量， 在大多数情况下， Redis 使用 SDS （Simple Dynamic String，简单动态字符串）作为字符串表示。 比起 C 字符串， SDS 具有以下优点：1、常数复杂度获取字符串长度。2、杜绝缓冲区溢出。3、减少修改字符串长度时所需的内存重分配次数。4、二进制安全。5、兼容部分 C 字符串函数。 链表链表提供了高效的节点重排能力， 以及顺序性的节点访问方式， 并且可以通过增删节点来灵活地调整链表的长度。 作为一种常用数据结构， 链表内置在很多高级的编程语言里面， 因为 Redis 使用的 C 语言并没有内置这种数据结构， 所以 Redis 构建了自己的链表实现。 链表在 Redis 中的应用非常广泛， 比如列表键的底层实现之一就是链表： 当一个列表键包含了数量比较多的元素， 又或者列表中包含的元素都是比较长的字符串时， Redis 就会使用链表作为列表键的底层实现。除了链表键之外， 发布与订阅、慢查询、监视器等功能也用到了链表， Redis 服务器本身还使用链表来保存多个客户端的状态信息， 以及使用链表来构建客户端输出缓冲区（output buffer）。 list 的内部数据结构： 123456789101112131415161718192021typedef struct list &#123; // 表头节点 listNode *head; // 表尾节点 listNode *tail; // 链表所包含的节点数量 unsigned long len; // 节点值复制函数 void *(*dup)(void *ptr); // 节点值释放函数 void (*free)(void *ptr); // 节点值对比函数 int (*match)(void *ptr, void *key);&#125; list; list 结构为链表提供了表头指针 head 、表尾指针 tail ， 以及链表长度计数器 len ， 而 dup 、 free 和 match 成员则是用于实现多态链表所需的类型特定函数。 listNode 的内部数据结构： 123456789101112typedef struct listNode &#123; // 前置节点 struct listNode *prev; // 后置节点 struct listNode *next; // 节点的值 void *value;&#125; listNode; 多个 listNode 可以通过 prev 和 next 指针组成双端链表。 重点回顾： 链表被广泛用于实现 Redis 的各种功能， 比如列表键， 发布与订阅， 慢查询， 监视器 等等。 每个链表节点由一个 listNode 结构来表示， 每个节点都有一个指向前置节点和后置节点的指针， 所以 Redis 的链表实现是双端链表。 每个链表使用一个 list 结构来表示， 这个结构带有表头节点指针、表尾节点指针、以及链表长度等信息。 因为链表表头节点的前置节点和表尾节点的后置节点都指向 NULL ， 所以 Redis 的链表实现是无环链表。 通过为链表设置不同的类型特定函数， Redis 的链表可以用于保存各种不同类型的值。 字典字典， 又称符号表（symbol table）、关联数组（associative array）或者映射（map）， 是一种用于保存键值对（key-value pair）的抽象数据结构。在字典中， 一个键（key）可以和一个值（value）进行关联（或者说将键映射为值）， 这些关联的键和值就被称为键值对。字典中的每个键都是独一无二的， 程序可以在字典中根据键查找与之关联的值， 或者通过键来更新值， 又或者根据键来删除整个键值对， 等等。 字典经常作为一种数据结构内置在很多高级编程语言里面， 但 Redis 所使用的 C 语言并没有内置这种数据结构， 因此 Redis 构建了自己的字典实现。 字典在 Redis 中的应用相当广泛， 比如 Redis 的数据库就是使用字典来作为底层实现的， 对数据库的增、删、查、改操作也是构建在对字典的操作之上的。除了用来表示数据库之外， 字典还是哈希键的底层实现之一： 当一个哈希键包含的键值对比较多， 又或者键值对中的元素都是比较长的字符串时， Redis 就会使用字典作为哈希键的底层实现。 dict 的内部数据结构： 12345678910111213141516typedef struct dict &#123; // 类型特定函数 dictType *type; // 私有数据 void *privdata; // 哈希表 dictht ht[2]; // rehash 索引 // 当 rehash 不在进行时，值为 -1 int rehashidx; /* rehashing not in progress if rehashidx == -1 */&#125; dict; dictht 的内部数据结构： 12345678910111213141516typedef struct dictht &#123; // 哈希表数组 dictEntry **table; // 哈希表大小 unsigned long size; // 哈希表大小掩码，用于计算索引值 // 总是等于 size - 1 unsigned long sizemask; // 该哈希表已有节点的数量 unsigned long used;&#125; dictht; dictEntry 的内部数据结构： 12345678910111213141516typedef struct dictEntry &#123; // 键 void *key; // 值 union &#123; void *val; uint64_t u64; int64_t s64; &#125; v; // 指向下个哈希表节点，形成链表 struct dictEntry *next;&#125; dictEntry; 重点回顾： 字典被广泛用于实现 Redis 的各种功能， 其中包括数据库和哈希键。 Redis 中的字典使用哈希表作为底层实现， 每个字典带有两个哈希表， 一个用于平时使用， 另一个仅在进行 rehash 时使用。 当字典被用作数据库的底层实现， 或者哈希键的底层实现时， Redis 使用 MurmurHash2 算法来计算键的哈希值。 哈希表使用链地址法来解决键冲突， 被分配到同一个索引上的多个键值对会连接成一个单向链表。 在对哈希表进行扩展或者收缩操作时， 程序需要将现有哈希表包含的所有键值对 rehash 到新哈希表里面， 并且这个 rehash 过程并不是一次性地完成的， 而是渐进式[3]地完成的。 跳跃表跳跃表（skiplist）是一种有序数据结构， 它通过在每个节点中维持多个指向其他节点的指针， 从而达到快速访问节点的目的。在大部分情况下， 跳跃表的效率可以和平衡树相媲美， 并且因为跳跃表的实现比平衡树要来得更为简单， 所以有不少程序都使用跳跃表来代替平衡树。 Redis 使用跳跃表作为有序集合键的底层实现之一： 如果一个有序集合包含的元素数量比较多， 又或者有序集合中元素的成员（member）是比较长的字符串时， Redis 就会使用跳跃表来作为有序集合键的底层实现。和链表、字典等数据结构被广泛地应用在 Redis 内部不同， Redis 只在两个地方用到了跳跃表， 一个是实现有序集合键， 另一个是在集群节点中用作内部数据结构， 除此之外， 跳跃表在 Redis 里面没有其他用途。 Redis 的跳跃表由 zskiplistNode 和 zskiplist 两个结构定义，其中 zskiplistNode 结构用于表示跳跃表节点，而 zskiplist 结构则用于保存跳跃表节点的相关信息，比如节点的数量，以及指向表头节点和表尾节点的指针等等。 zskiplist 的内部数据结构： 123456789101112typedef struct zskiplist &#123; // 表头节点和表尾节点 struct zskiplistNode *header, *tail; // 表中节点的数量 unsigned long length; // 表中层数最大的节点的层数 int level;&#125; zskiplist; zskiplistNode 的内部数据结构： 1234567891011121314151617181920212223typedef struct zskiplistNode &#123; // 后退指针 struct zskiplistNode *backward; // 分值 double score; // 成员对象 robj *obj; // 层 struct zskiplistLevel &#123; // 前进指针 struct zskiplistNode *forward; // 跨度 unsigned int span; &#125; level[];&#125; zskiplistNode; 重点回顾： 跳跃表是有序集合的底层实现之一， 除此之外它在 Redis 中没有其他应用。 Redis 的跳跃表实现由 zskiplist 和 zskiplistNode 两个结构组成， 其中 zskiplist 用于保存跳跃表信息（比如表头节点、表尾节点、长度）， 而 zskiplistNode 则用于表示跳跃表节点。 每个跳跃表节点的层高都是 1 至 32 之间的随机数。 在同一个跳跃表中， 多个节点可以包含相同的分值， 但每个节点的成员对象必须是唯一的。 跳跃表中的节点按照分值大小进行排序， 当分值相同时， 节点按照成员对象的大小进行排序。 整数集合整数集合（intset）是集合键的底层实现之一： 当一个集合只包含整数值元素， 并且这个集合的元素数量不多时， Redis 就会使用整数集合作为集合键的底层实现。 整数集合（intset）是 Redis 用于保存整数值的集合抽象数据结构， 它可以保存类型为 int16_t 、 int32_t 或者 int64_t 的整数值， 并且保证集合中不会出现重复元素。 intset 的内部数据结构： 123456789101112typedef struct intset &#123; // 编码方式 uint32_t encoding; // 集合包含的元素数量 uint32_t length; // 保存元素的数组 int8_t contents[];&#125; intset; 重点回顾： 整数集合是集合键的底层实现之一。 整数集合的底层实现为数组， 这个数组以有序、无重复的方式保存集合元素， 在有需要时， 程序会根据新添加元素的类型， 改变这个数组的类型。 升级操作为整数集合带来了操作上的灵活性， 并且尽可能地节约了内存。 整数集合只支持升级操作， 不支持降级操作。 压缩列表压缩列表（ziplist）是列表键和哈希键的底层实现之一。当一个列表键只包含少量列表项， 并且每个列表项要么就是小整数值， 要么就是长度比较短的字符串， 那么 Redis 就会使用压缩列表来做列表键的底层实现。另外， 当一个哈希键只包含少量键值对， 并且每个键值对的键和值要么就是小整数值， 要么就是长度比较短的字符串， 那么 Redis 就会使用压缩列表来做哈希键的底层实现。 压缩列表是 Redis 为了节约内存而开发的， 由一系列特殊编码的连续内存块组成的顺序型（sequential）数据结构。一个压缩列表可以包含任意多个节点（entry）， 每个节点可以保存一个字节数组或者一个整数值。 重点回顾： 压缩列表是一种为节约内存而开发的顺序型数据结构。 压缩列表被用作列表键和哈希键的底层实现之一。 压缩列表可以包含多个节点，每个节点可以保存一个字节数组或者整数值。 添加新节点到压缩列表， 或者从压缩列表中删除节点， 可能会引发连锁更新操作， 但这种操作出现的几率并不高。 压缩列表压缩列表（ziplist）是列表键和哈希键的底层实现之一。当一个列表键只包含少量列表项， 并且每个列表项要么就是小整数值， 要么就是长度比较短的字符串， 那么 Redis 就会使用压缩列表来做列表键的底层实现。另外， 当一个哈希键只包含少量键值对， 并且每个键值对的键和值要么就是小整数值， 要么就是长度比较短的字符串， 那么 Redis 就会使用压缩列表来做哈希键的底层实现。 压缩列表是 Redis 为了节约内存而开发的， 由一系列特殊编码的连续内存块组成的顺序型（sequential）数据结构。一个压缩列表可以包含任意多个节点（entry）， 每个节点可以保存一个字节数组或者一个整数值。 重点回顾： 压缩列表是一种为节约内存而开发的顺序型数据结构。 压缩列表被用作列表键和哈希键的底层实现之一。 压缩列表可以包含多个节点，每个节点可以保存一个字节数组或者整数值。 添加新节点到压缩列表， 或者从压缩列表中删除节点， 可能会引发连锁更新操作， 但这种操作出现的几率并不高。 Redis 对象redisObjectRedis 并没有直接使用这些数据结构来实现键值对数据库， 而是基于这些数据结构创建了一个对象系统， 这个系统包含字符串对象、列表对象、哈希对象、集合对象和有序集合对象这五种类型的对象， 每种对象都用到了至少一种我们前面所介绍的数据结构。通过这五种不同类型的对象， Redis 可以在执行命令之前， 根据对象的类型来判断一个对象是否可以执行给定的命令。 使用对象的另一个好处是， 我们可以针对不同的使用场景， 为对象设置多种不同的数据结构实现， 从而优化对象在不同场景下的使用效率。 Redis 使用对象来表示数据库中的键和值，每次当我们在 Redis 的数据库中新创建一个键值对时，我们至少会创建两个对象，一个对象用作键值对的键（键对象），另一个对象用作键值对的值（值对象）。 举个例子，以 “ SET msg “hello world” ” 命令在数据库中创建了一个新的键值对，其中键值对的键是一个包含了字符串值 “msg” 的对象，而键值对的值则是一个包含了字符串值 “hello world” 对象。 Redis 中每个对象都由一个 redisObject 结构表示，该结构中和保存数据有关的三个属性分别是 type 属性、 encoding 属性和 ptr 属性： 123456789101112typedef struct redisObject &#123; // 数据类型：就是我们熟悉的 string、hash、list 等五种数据类型 unsigned type:4; // 内部编码：其实就是数据结构 unsigned encoding:4; // 指向底层实现数据结构的指针：指向以 encoding 的方式实现这个对象的实际地址 void *ptr; // 当前对象可以保留的时长 unsigned lru:REDIS_LRU_BITS; /* lru time (relative to server.lruclock) */ // 对象引用计数：用于 GC int refcount;&#125; robj; 类型对象的 type 属性记录了对象的类型。对于 Redis 数据库保存的键值对来说，键总是一个字符串对象，而值则可以是字符串对象、列表对象、哈希对象、集合对象或者有序集合对象其中一种。 TYPE 命令返回的结果为数据库键对应的值对象的类型。 12345678910111213141516171819202122232425262728293031323334# 键为字符串对象，值为字符串对象redis&gt; SET msg "hello world""OK"redis&gt; TYPE msg"string"# 键为字符串对象，值为列表对象redis&gt; RPUSH numbers 1 3 5"3"redis&gt; TYPE numbers"list"# 键为字符串对象，值为哈希对象redis&gt; HMSET profile name Tom age 25 career Programmer"OK"redis&gt; TYPE profile"hash"# 键为字符串对象，值为集合对象redis&gt; SADD fruits apple banana cherry"3"redis&gt; TYPE fruits"set"# 键为字符串对象，值为有序集合对象redis&gt; ZADD price 8.5 apple 5.0 banana 6.0 cherry"3"redis&gt; TYPE price"zset" 编码和底层实现对象的 ptr 指针指向对象的底层实现数据结构， 而这些数据结构由对象的 encoding 属性决定。 每种类型的对象都至少使用了两种不同的编码。 通过 encoding 属性来设定对象所使用的编码， 而不是为特定类型的对象关联一种固定的编码， 极大地提升了 Redis 的灵活性和效率， 因为 Redis 可以根据不同的使用场景来为一个对象设置不同的编码， 从而优化对象在某一场景下的效率。 举个例子， 在列表对象包含的元素比较少时， Redis 使用压缩列表作为列表对象的底层实现：因为压缩列表比双端链表更节约内存， 并且在元素数量较少时， 在内存中以连续块方式保存的压缩列表比起双端链表可以更快被载入到缓存中；随着列表对象包含的元素越来越多， 使用压缩列表来保存元素的优势逐渐消失时， 对象就会将底层实现从压缩列表转向功能更强、也更适合保存大量元素的双端链表上面； string（字符串对象）字符串对象的编码可以是 int 、 raw 或者 embstr 。 如果一个字符串对象保存的是整数值， 并且这个整数值可以用 long 类型来表示， 那么字符串对象会将整数值保存在字符串对象结构的 ptr属性里面（将 void 转换成 long ）， 并将字符串对象的编码设置为 int 。 如果字符串对象保存的是一个字符串值， 并且这个字符串值的长度大于 39 字节（在 Redis 3.2 版本之后，变成了 44 字节为分界）， 那么字符串对象将使用一个简单动态字符串（SDS）来保存这个字符串值， 并将对象的编码设置为 raw 。 如果字符串对象保存的是一个字符串值， 并且这个字符串值的长度小于等于 39 字节， 那么字符串对象将使用 embstr 编码的方式来保存这个字符串值。 常用命令：set、get、decr、incr、mget 等。 常见应用：缓存、限流、计数器、分布式锁、分布式 Session 等。 list（列表对象）列表对象的编码可以是 ziplist 或者 linkedlist 。 ziplist 编码的列表对象使用压缩列表作为底层实现， 每个压缩列表节点（entry）保存了一个列表元素。当一个列表键只包含少量列表项， 并且每个列表项要么就是小整数值， 要么就是长度比较短的字符串， 那么 Redis 就会使用压缩列表来做列表键的底层实现。 linkedlist 编码的列表对象使用双端链表作为底层实现， 每个双端链表节点（node）都保存了一个字符串对象， 而每个字符串对象都保存了一个列表元素。 当一个列表键包含了数量比较多的元素， 又或者列表中包含的元素都是比较长的字符串时， Redis 就会使用链表作为列表键的底层实现。 常用命令：lpush、rpush、lpop、rpop、lrange 等。 常见应用：微博关注人时间轴列表、简单队列、文章列表。 hash（哈希对象）哈希对象的编码可以是 ziplist 或者 hashtable 。 ziplist 编码的哈希对象使用压缩列表作为底层实现， 每当有新的键值对要加入到哈希对象时， 程序会先将保存了键的压缩列表节点推入到压缩列表表尾， 然后再将保存了值的压缩列表节点推入到压缩列表表尾。当一个哈希键只包含少量键值对， 并且每个键值对的键和值要么就是小整数值， 要么就是长度比较短的字符串， 那么 Redis 就会使用压缩列表来做哈希键的底层实现。 hashtable 编码的哈希对象使用字典作为底层实现， 哈希对象中的每个键值对都使用一个字典键值对来保存。 当一个哈希键包含的键值对比较多， 又或者键值对中的元素都是比较长的字符串时， Redis 就会使用字典作为哈希键的底层实现。 常用命令：hget、hset、hgetall 等。 常见应用：存储用户信息、用户主页访问量、组合查询。 set（集合对象）集合对象的编码可以是 intset 或者 hashtable 。 intset 编码的集合对象使用整数集合作为底层实现， 集合对象包含的所有元素都被保存在整数集合里面。当一个集合只包含整数值元素， 并且这个集合的元素数量不多时， Redis 就会使用整数集合作为集合键的底层实现。 hashtable 编码的集合对象使用字典作为底层实现， 字典的每个键都是一个字符串对象， 每个字符串对象包含了一个集合元素， 而字典的值则全部被设置为 NULL 。 常用命令： sadd、spop、smembers、sunion 等。 常见应用：赞、踩、标签、好友关系。 zset（有序集合对象）有序集合的编码可以是 ziplist 或者 skiplist 。 ziplist 编码的有序集合对象使用压缩列表作为底层实现， 每个集合元素使用两个紧挨在一起的压缩列表节点来保存， 第一个节点保存元素的成员（member）， 而第二个元素则保存元素的分值（score）。压缩列表内的集合元素按分值从小到大进行排序， 分值较小的元素被放置在靠近表头的方向， 而分值较大的元素则被放置在靠近表尾的方向。 skiplist 编码的有序集合对象使用 zset 结构作为底层实现， 一个 zset 结构同时包含一个字典和一个跳跃表。zset 结构中的 zsl 跳跃表按分值从小到大保存了所有集合元素， 每个跳跃表节点都保存了一个集合元素： 跳跃表节点的 object 属性保存了元素的成员， 而跳跃表节点的 score 属性则保存了元素的分值。 通过这个跳跃表， 程序可以对有序集合进行范围型操作， 比如 ZRANK 、ZRANGE 等命令就是基于跳跃表 API 来实现的。 如果一个有序集合包含的元素数量比较多， 又或者有序集合中元素的成员（member）是比较长的字符串时， Redis 就会使用跳跃表来作为有序集合键的底层实现。 常用命令：zadd、zrange、zrem、zcard 等。 常见应用：排行榜。 参考博文[1]. 《Redis设计与实现》，第一部分 数据结构与对象[2]. 图解Redis之数据结构篇 注脚[1]. 多路 I/O 复用模型：多路 I/O 复用模型是利用 select、poll、epoll 可以同时监察多个流的 I/O 事件的能力，在空闲的时候，会把当前线程阻塞掉，当有一个或多个流有 I/O 事件时，就从阻塞态中唤醒，于是程序就会轮询一遍所有的流（epoll 是只轮询那些真正发出了事件的流），并且依次顺序的处理就绪的流，这种做法就避免了大量的无用操作。这里 “多路” 指的是多个网络连接，“复用” 指的是复用同一个线程。采用多路 I/O 复用技术可以让单个线程高效的处理多个连接请求（尽量减少网络 IO 的时间消耗），且 Redis 在内存中操作数据的速度非常快，也就是说内存内的操作不会成为影响 Redis 性能的瓶颈。[2]. 1byte：SDS 遵循 C 字符串以空字符结尾的惯例， 保存空字符的 1 字节空间不计算在 SDS 的 len 属性里面， 并且为空字符分配额外的 1 字节空间， 以及添加空字符到字符串末尾等操作都是由 SDS 函数自动完成的， 所以这个空字符对于 SDS 的使用者来说是完全透明的。[3]. 渐进式：1、因为在进行渐进式 rehash 的过程中，字典会同时使用 ht[0] 和 ht[1] 两个哈希表，所以在渐进式 rehash 进行期间，字典的删除（delete）、查找（find）、更新（update）等操作会在两个哈希表上进行。2、在渐进式 rehash 执行期间，新添加到字典的键值对一律会被保存到 ht[1] 里面，而 ht[0] 则不再进行任何添加操作：这一措施保证了 ht[0] 包含的键值对数量会只减不增，并随着 rehash 操作的执行而最终变成空表。 Redis 深度探险系列 Redis 深度探险（一）：那些绕不过去的 Redis 知识点 Redis 深度探险（二）：Redis 深入之道 Redis 深度探险（三）：Redis 单机环境搭建以及配置说明 Redis 深度探险（四）：Redis 高可用性解决方案之哨兵与集群]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>对象</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 并发编程之美（二）：线程池 ThreadPoolExecutor 原理探究]]></title>
    <url>%2Farchives%2Fe230219b.html</url>
    <content type="text"><![CDATA[前言在面向对象编程中，创建和销毁对象是很费时间的，因为创建一个对象要获取内存资源或者其它更多资源。在 Java 中更是如此，虚拟机将试图跟踪每一个对象，以便能够在对象销毁后进行垃圾回收。所以提高服务程序效率的一个手段就是尽可能减少创建和销毁对象的次数，特别是一些很耗资源的对象创建和销毁，这就是 “池化资源” 技术产生的原因。线程池顾名思义就是事先创建若干个可执行的线程放入一个池（容器）中，需要的时候从池中获取线程，不用自行创建；使用完毕不需要销毁线程而是放回池中，从而减少创建和销毁线程对象的开销。但是要做到合理的利用线程池，必须对其原理了如指掌。 线程池的好处以及使用场景使用线程池的好处？合理利用线程池能够带来三个好处。第一：降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。第二：提高响应速度。当任务到达时，任务可以不需要的等到线程创建就能立即执行。第三：提高线程的可管理性。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。 什么时候使用线程池？假设一个服务器完成一项任务所需时间为：T1 创建线程时间，T2 在线程中执行任务的时间，T3 销毁线程时间。如果：T1+T3 远大于 T2，则可以采用线程池，以提高服务器性能。线程池技术正是关注如何缩短或调整 T1，T3 时间的技术，从而提高服务器程序性能的。它把 T1，T3 分别安排在服务器程序的启动和结束的时间段或者一些空闲的时间段，这样在服务器程序处理客户请求时，不会有 T1，T3 的开销了。 即，第一：单个任务处理时间比较短；第二：需要处理的任务数量很大。 Java 中的 ThreadPoolExecutor 类线程池的创建可以通过创建 ThreadPoolExecutor 对象或者调用 Executors 的工厂方法来创建线程池。ThreadPoolExecutor 是 Executors 类的底层实现。但是在阿里巴巴的 Java 开发手册中提到： 【强制】线程池不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险。说明：Executors 返回的线程池对象的弊端如下：1）FixedThreadPool 和 SingleThreadPool:允许的请求队列长度为 Integer.MAX_VALUE，可能会堆积大量的请求，从而导致 OOM。2）CachedThreadPool 和 ScheduledThreadPool:允许的创建线程数量为 Integer.MAX_VALUE，可能会创建大量的线程，从而导致 OOM。 线程池的创建在 ThreadPoolExecutor 类中提供了四个构造方法： 123456789public class ThreadPoolExecutor extends AbstractExecutorService &#123; public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue)&#123;&#125; public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory)&#123;&#125; public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, RejectedExecutionHandler handler)&#123;&#125; public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler)&#123;&#125;&#125; 从上面的代码可以得知，ThreadPoolExecutor 继承了 AbstractExecutorService 类，并提供了四个构造器，事实上，通过观察每个构造器的源码具体实现，发现前面三个构造器都是调用的第四个构造器进行的初始化工作。 下面解释下一下构造器中各个参数的含义： corePoolSize：核心线程池大小，这个参数跟后面讲述的线程池的实现原理有非常大的关系。在创建了线程池后，默认情况下，线程池中并没有任何线程，而是等待有任务到来才创建线程去执行任务，除非调用了 prestartAllCoreThreads()或者 prestartCoreThread()方法，从这 2 个方法的名字就可以看出，是预创建线程的意思，即在没有任务到来之前就创建 corePoolSize 个线程或者一个线程。默认情况下，在创建了线程池后，线程池中的线程数为 0，当有任务来之后，就会创建一个线程去执行任务，当线程池中的线程数目达到 corePoolSize 后，就会把到达的任务放到缓存队列当中。当有新任务在 execute()方法提交时，会执行以下判断： 如果运行的线程少于 corePoolSize，则创建新线程来处理任务，即使线程池中的其他线程是空闲的； 如果线程池中的线程数量大于等于 corePoolSize 且小于 maximumPoolSize，则只有当 workQueue 满时才创建新的线程去处理任务； 如果设置的 corePoolSize 和 maximumPoolSize 相同，则创建的线程池的大小是固定的，这时如果有新任务提交，若 workQueue 未满，则将请求放入 workQueue 中，等待有空闲的线程去 workQueue 中取任务并处理； 如果运行的线程数量大于等于 maximumPoolSize，这时如果 workQueue 已经满了，则通过 handler 所指定的策略来处理任务； 所以，任务提交时，判断的顺序为 corePoolSize –&gt; workQueue –&gt; maximumPoolSize。 maximumPoolSize：最大线程池大小，线程池允许创建的最大线程数。如果队列满了，并且已创建的线程数小于最大线程数，则线程池会再创建新的线程执行任务。值得注意的是如果使用了无界的任务队列这个参数就没什么效果。 keepAliveTime：线程池中超过 corePoolSize 数目的空闲线程最大存活时间；默认情况下，只有当线程池中的线程数大于 corePoolSize 时，keepAliveTime 才会起作用，直到线程池中的线程数不大于 corePoolSize，即当线程池中的线程数大于 corePoolSize 时，如果一个线程空闲的时间达到 keepAliveTime，则会终止，直到线程池中的线程数不超过 corePoolSize。但是如果调用了 allowCoreThreadTimeOut(boolean)方法，在线程池中的线程数不大于 corePoolSize 时，keepAliveTime 参数也会起作用，直到线程池中的线程数为 0。 unit：参数 keepAliveTime 的时间单位，有 7 种取值，在 TimeUnit 类中有 7 种静态属性。 workQueue：阻塞任务队列，用来存储等待执行的任务，这个参数的选择也很重要，会对线程池的运行过程产生重大影响。保存等待执行的任务的阻塞队列，当提交一个新的任务到线程池以后，线程池会根据当前线程池中正在运行着的线程的数量来决定对该任务的处理方式，主要有以下几种处理方式： 直接切换：这种方式常用的队列是 SynchronousQueue。 使用无界队列：一般使用基于链表的阻塞队列 LinkedBlockingQueue。如果使用这种方式，那么线程池中能够创建的最大线程数就是 corePoolSize，而 maximumPoolSize 就不会起作用了。当线程池中所有的核心线程都是 RUNNING 状态时，这时一个新的任务提交就会放入等待队列中。 使用有界队列：一般使用 ArrayBlockingQueue。使用该方式可以将线程池的最大线程数量限制为 maximumPoolSize，这样能够降低资源的消耗，但同时这种方式也使得线程池对线程的调度变得更困难，因为线程池和队列的容量都是有限的值，所以要想使线程池处理任务的吞吐率达到一个相对合理的范围，又想使线程调度相对简单，并且还要尽可能的降低线程池对资源的消耗，就需要合理的设置这两个数量： 如果要想降低系统资源的消耗（包括 CPU 的使用率，操作系统资源的消耗，上下文环境切换的开销等），可以设置较大的队列容量和较小的线程池容量，但这样也会降低线程处理任务的吞吐量。 如果提交的任务经常发生阻塞，那么可以考虑通过调用 setMaximumPoolSize() 方法来重新设定线程池的容量。 如果队列的容量设置的较小，通常需要将线程池的容量设置大一点，这样 CPU 的使用率会相对的高一些。但如果线程池的容量设置的过大，则在提交的任务数量太多的情况下，并发量会增加，那么线程之间的调度就是一个要考虑的问题，因为这样反而有可能降低处理任务的吞吐量。 threadFactory：它是 ThreadFactory 类型的变量，用来创建新线程。默认使用 Executors.defaultThreadFactory()来创建线程。使用默认的 ThreadFactory 来创建线程时，会使新创建的线程具有相同的 NORM_PRIORITY 优先级并且是非守护线程，同时也设置了线程的名称。 handler：饱和策略，当队列和线程池都满了，说明线程池处于饱和状态，那么必须采取一种策略处理提交的新任务。这个策略默认情况下是 AbortPolicy，表示无法处理新任务时抛出异常。以下是 JDK 1.5 提供的四种策略： AbortPolicy：直接抛出异常，这是默认策略； CallerRunsPolicy：用调用者所在的线程来执行任务；显然这样不会真的丢弃任务，但是，调用者线程性能可能急剧下降； DiscardOldestPolicy：丢弃阻塞队列中靠最前的任务，也就是丢弃一个即将被执行的任务，并尝试再次提交当前任务，重复此过程； DiscardPolicy：直接丢弃任务； 当然也可以根据应用场景需要来实现 RejectedExecutionHandler 接口自定义策略，如记录日志或持久化不能处理的任务； 向线程池提交任务 提交任务有 execute()和 submit()两个方法，下面看看他俩的区别：1、接收参数不同：execute()的参数是 Runnable，submit()参数可以是 Runnable，也可以是 Callable。2、返回值不同：execute()没有返回值，submit()有返回值 Future。通过 Future 可以获取各个线程的完成情况，是否有异常，还能试图取消任务的执行。 1234567891011121314151617181920212223242526272829303132333435363738394041424344public void execute(Runnable command) &#123; // 1. 判断提交的任务是否为 null, 是则抛出异常 if (command == null) throw new NullPointerException(); /* 2. 获取线程池控制状态: clt 记录着 runState 和 workerCount * ctl 是一个 AtomicInteger 变量 * jdk 8 中通过一个 int 值的前 28 位表示工作线程数量 workerCount, 剩余高位来表示 线程池状态 * 计算 workerCount 和 runState 时通过掩码计算。 CAPACITY = (1 &lt;&lt; 29) - 1 */ int c = ctl.get(); // 3.workerCountOf 方法取出低 29 位的值, 表示当前活动的线程数; 如果当前活动线程数小于 corePoolSize, 则新建一个线程放入线程池中; 并把任务添加到该线程中. if (workerCountOf(c) &lt; corePoolSize) &#123; /* 3.1 addWorker 中的第二个参数表示限制添加线程的数量是根据 corePoolSize 来判断还是 maximumPoolSize 来判断; * 如果为 true, 根据 corePoolSize 来判断; 如果为 false, 则根据 maximumPoolSize 来判断. */ if (addWorker(command, true)) return; // 3.2 如果添加失败, 则重新获取 ctl 值. c = ctl.get(); &#125; // 4.（worker 线程数量大于核心线程池容量时）如果线程池处于 RUNNING 状态，将命令加入 workQueue 队列 if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; // 4.1 重新获取 ctl 值, 再次检查防止状态突变 int recheck = ctl.get(); // 4.2 再次判断线程池的运行状态, 如果不是运行状态, 由于之前已经把 command 添加到 workQueue 中了, 这时需要移除该 command, 执行过后通过 handler 使用拒绝策略对该任务进行处理, 整个方法返回 if (!isRunning(recheck) &amp;&amp; remove(command)) // 4.2.1 使用拒绝策略对该任务进行处理, 整个方法返回 reject(command); else if (workerCountOf(recheck) == 0) /* 4.3 获取线程池中的有效线程数, 如果数量是 0(即核心线程数为 0), 则执行 addWorker 方法. 如果判断 workerCount 大于 0, 则直接返回, 在 workQueue 中新增的 command 会在将来的某个时刻被执行. * 这里传入的参数表示: 1. 第一个参数为 null, 表示在线程池中创建一个线程, 但不去启动; 2. 第二个参数为 false, 将线程池的有限线程数量的上限设置为 maximumPoolSize, 添加线程时根据 maximumPoolSize 来判断; */ addWorker(null, false); &#125; /* 5. 如果执行到这里, 有两种情况: 1. 线程池已经不是 RUNNING 状态; 2. 线程池是 RUNNING 状态, 但 workerCount &gt;= corePoolSize 并且 workQueue 已满. * 这时, 再次调用 addWorker 方法, 但第二个参数传入为 false, 将线程池的有限线程数量的上限设置为 maximumPoolSize; */ else if (!addWorker(command, false)) // 5.1 如果失败则拒绝该任务. reject(command);&#125; 如果运行的线程小于 corePoolSize，则尝试使用用户定义的 Runnalbe 对象创建一个新的线程。调用 addWorker()函数会原子性的检查 runState 和 workCount，通过返回 false 来防止在不应该添加线程时添加了线程 如果一个任务能够成功入队列，在添加一个线程时仍需要进行双重检查（可能因为在前一次检查后该线程死亡了），或者当进入到此方法时，线程池已经 shutdown 了，所以需要再次检查状态。若线程此时的状态不是 RUNNING，则需要回滚入队列操作；或者当线程池没有工作线程时，需要创建一个新的工作线程。 如果无法入队列，那么需要增加一个新工作线程，如果此操作失败，那么就意味着线程池已经 SHUTDOWN 或者已经饱和了，所以拒绝任务 线程池的关闭shutdown()shutdown()方法要将线程池切换到 SHUTDOWN 状态，并调用 interruptIdleWorkers()方法请求中断所有空闲的 worker，最后调用 tryTerminate()尝试结束线程池。 123456789101112131415161718public void shutdown() &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; // 安全策略判断 checkShutdownAccess(); // 切换状态为 SHUTDOWN advanceRunState(SHUTDOWN); // 中断空闲线程 interruptIdleWorkers(); // hook for ScheduledThreadPoolExecutor onShutdown(); &#125; finally &#123; mainLock.unlock(); &#125; // 尝试结束线程池 tryTerminate();&#125; 1、停止接收新的 submit 的任务；2、已经提交的任务（包括正在跑的和队列中等待的），会继续执行完成；3、等到第 2 步完成后，才真正停止； shutdownNow()shutdown() 方法要将线程池切换到 STOP 状态，并调用 interruptIdleWorkers() 方法请求中断所有工作线程，无论是否是空闲的，然后取出阻塞队列中没有被执行的任务并返回，最后调用 tryTerminate() 尝试结束线程池。 123456789101112131415161718192021public List&lt;Runnable&gt; shutdownNow() &#123; List&lt;Runnable&gt; tasks; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; // 安全策略判断 checkShutdownAccess(); // 切换状态为 STOP advanceRunState(STOP); // 中断所有工作线程，无论是否空闲 interruptWorkers(); // 取出队列中没有被执行的任务 tasks = drainQueue(); &#125; finally &#123; mainLock.unlock(); &#125; // 尝试结束线程池 tryTerminate(); // 返回没有被执行的任务 return tasks;&#125; 1、跟 shutdown()一样，先停止接收新 submit 的任务；2、忽略队列里等待的任务；3、尝试将正在执行的任务 interrupt 中断；4、返回未执行的任务列表； 说明：它试图终止线程的方法是通过调用 Thread.interrupt()方法来实现的，这种方法的作用有限，如果线程中没有 sleep、wait、Condition、定时锁等应用，interrupt()方法是无法中断当前的线程的。所以，shutdownNow()并不代表线程池就一定立即就能退出，它也可能必须要等待所有正在执行的任务都执行完成了才能退出。但是大多数时候是能立即退出的。 深入剖析线程池实现原理线程池状态123456789101112131415private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));private static final int COUNT_BITS = Integer.SIZE - 3;private static final int CAPACITY = (1 &lt;&lt; COUNT_BITS) - 1;// runState is stored in the high-order bits// 即高 3 位为 111, 该状态的线程池会接收新任务, 并处理阻塞队列中的任务;private static final int RUNNING = -1 &lt;&lt; COUNT_BITS;// 即高 3 位为 000, 该状态的线程池不会接收新任务, 但会处理阻塞队列中的任务;private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS;// 即高 3 位为 001, 该状态的线程不会接收新任务, 也不会处理阻塞队列中的任务, 而且会中断正在运行的任务;private static final int STOP = 1 &lt;&lt; COUNT_BITS;// 即高 3 位为 010;private static final int TIDYING = 2 &lt;&lt; COUNT_BITS;// 即高 3 位为 011;private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS; ctl 是对线程池的运行状态和线程池中有效线程的数量进行控制的一个字段，它包含两部分的信息：线程池的运行状态 (runState) 和线程池内有效线程的数量(workerCount)，这里可以看到，使用了 Integer 类型来保存，高 3 位保存 runState，低 29 位保存 workerCount。COUNT_BITS 就是 29，CAPACITY 就是 1 左移 29 位减 1（29 个 1），这个常量表示 workerCount 的上限值，大约是 5 亿。 ctl 相关方法： 123456// 获取运行状态private static int runStateOf(int c) &#123; return c &amp; ~CAPACITY; &#125;// 获取活动线程数private static int workerCountOf(int c) &#123; return c &amp; CAPACITY; &#125;// 获取运行状态和活动线程数的值private static int ctlOf(int rs, int wc) &#123; return rs | wc; &#125; 下面再介绍下线程池的运行状态。 线程池一共有五种状态，分别是： RUNNING（运行状态）：能接受新提交的任务，并且也能处理阻塞队列中已保存的任务； SHUTDOWN（关闭状态）：不能接受新提交的任务，但却可以处理阻塞队列中已保存的任务。在线程池处于 RUNNING 状态时，调用 shutdown()方法会使线程池进入到该状态。（finalize()方法在执行过程中也会调用 shutdown()方法进入该状态）； STOP（停止状态）：不能接受新提交的任务，也不能处理阻塞队列中已保存的任务，并且会中断正在处理任务的线程。在线程池处于 RUNNING 或 SHUTDOWN 状态时，调用 shutdownNow()方法会使线程池进入到该状态； TIDYING（整理状态）：如果所有的任务都已终止了，workerCount(有效线程数)为 0，线程池进入该状态后会调用 terminated()方法进入 TERMINATED 状态； TERMINATED（终止状态）：在 terminated()方法执行完后进入该状态，默认 terminated()方法中什么也没有做； 源码分析 addWorkeraddWorker 方法的主要工作是在线程池中创建一个新的线程并执行，firstTask 参数用于指定新增的线程执行的第一个任务，core 参数为 true 表示在新增线程时会判断当前活动线程数是否少于 corePoolSize，false 表示新增线程前需要判断当前活动线程数是否少于 maximumPoolSize。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101private boolean addWorker(Runnable firstTask, boolean core) &#123; retry: for (; ; ) &#123; // 1. 获取线程池控制状态 clt int c = ctl.get(); // 2. 获取线程池运行状态 int rs = runStateOf(c); /* 3. 检查线程池状况, 确保此时可以添加新的线程. * 如果是 RUNNING, 表示线程池处于运行状态, 那么跳过 if. * 如果 rs&gt;=SHUTDOWN, 同时不等于 SHUTDOWN, 即为 SHUTDOWN 以上的状态, 那么不接受新线程. * 如果 rs&gt;=SHUTDOWN, 同时等于 SHUTDOWN, 接着判断以下 3 个条件, 只要有 1 个不满足, 则返回 false; * 1. rs == SHUTDOWN, 这时表示关闭状态, 不再接受新提交的任务, 但却可以继续处理阻塞队列中已保存的任务; * 2. firsTask 为空; rs == SHUTDOWN, 不再接受新提交的任务, 所以在 firstTask 不为空的时候会返回 false; * 3. 阻塞队列不为空; 因为队列中已经没有任务了, 不需要再添加线程了; */ // Check if queue empty only if necessary. if (rs &gt;= SHUTDOWN &amp;&amp; !(rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; !workQueue.isEmpty())) return false; for (; ; ) &#123; // 4. 获取线程池线程数 int wc = workerCountOf(c); /* * 5. 判断线程池是否已满, 如果线程池已满返回 false * 如果线程数大于等于最大容量 CAPACITY 直接返回 false * core 是一个 boolean 参数, 表明调用者想把此线程添加到哪个线程池, 根据 core 的值判断要添加的线程池是否已满 */ if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) return false; // 6.CAS 操作增加工作线程数, 尝试增加 workerCount, 如果成功, 则跳出第一个 for 循环 if (compareAndIncrementWorkerCount(c)) break retry; // 7. 如果增加 workerCount 失败, 则重新获取 ctl 的值 c = ctl.get(); // Re-read ctl // 8. 如果当前的运行状态不等于 rs, 说明状态已被改变, 返回第一个 for 循环继续执行 if (runStateOf(c) != rs) continue retry; // else CAS failed due to workerCount change; retry inner loop &#125; &#125; boolean workerStarted = false; boolean workerAdded = false; Worker w = null; try &#123; // 9. 根据 firstTask 来创建 Worker 对象, 每一个 Worker 对象都会创建一个线程 w = new Worker(firstTask); final Thread t = w.thread; if (t != null) &#123; final ReentrantLock mainLock = this.mainLock; // 10. 获取到锁 mainLock.lock(); try &#123; // 11. 再次检查状态, 因为状态可能在获取锁之前改变 // Recheck while holding lock. // Back out on ThreadFactory failure or if // shut down before lock acquired. int rs = runStateOf(ctl.get()); /* 12. 判断线程池状态 * rs &lt; SHUTDOWN 表示是 RUNNING 状态; 如果 rs 是 RUNNING 状态或者 rs 是 SHUTDOWN 状态并且 firstTask 为 null, 向线程池中添加线程. * 因为在 SHUTDOWN 时不会在添加新的任务, 但还是会执行 workQueue 中的任务 */ if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) &#123; if (t.isAlive()) // precheck that t is startable throw new IllegalThreadStateException(); // workers 是一个 HashSet workers.add(w); int s = workers.size(); // largestPoolSize 记录着线程池中出现过的最大线程数量 if (s &gt; largestPoolSize) largestPoolSize = s; workerAdded = true; &#125; &#125; finally &#123; mainLock.unlock(); &#125; if (workerAdded) &#123; // 13. 线程添加成功后就可以启动线程准备执行了 t.start(); workerStarted = true; &#125; &#125; &#125; finally &#123; // 14. 若线程启动失败, 则进入线程添加失败方法 if (!workerStarted) addWorkerFailed(w); &#125; return workerStarted;&#125; runWorker线程池创建线程时，会将线程封装成工作线程 Worker，ThreadPool 维护的其实就是一组 Worker 对象，Worker 在执行完任务后，还会无限循环获取工作队列里的任务来执行。我们可以从 Worker 的 runWorker 方法里看到： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960final void runWorker(Worker w) &#123; // 1. 获取当前线程 Thread wt = Thread.currentThread(); // 2. 获取 w 的 firstTask Runnable task = w.firstTask; // 3. 设置 w 的 firstTask 为 null w.firstTask = null; // 4. 释放锁（设置 state 为 0, 允许中断） w.unlock(); // allow interrupts boolean completedAbruptly = true; try &#123; // 5. 如果 task 为空, 则通过 getTask 来获取任务 while (task != null || (task = getTask()) != null) &#123; // 6. 获取锁 w.lock(); // 7. 这里的检查主要是确保线程池此时还能接收新的任务去执行, 如果不在接收新的任务, 则应该中断当前线程 // If pool is stopping, ensure thread is interrupted; // if not, ensure thread is not interrupted. This // requires a recheck in second case to deal with // shutdownNow race while clearing interrupt if ((runStateAtLeast(ctl.get(), STOP) || (Thread.interrupted() &amp;&amp; runStateAtLeast(ctl.get(), STOP))) &amp;&amp; !wt.isInterrupted()) wt.interrupt(); try &#123; // 8. 在执行之前调用钩子函数 beforeExecute(wt, task); Throwable thrown = null; try &#123; // 9. 运行给定的任务 task.run(); &#125; catch (RuntimeException x) &#123; thrown = x; throw x; &#125; catch (Error x) &#123; thrown = x; throw x; &#125; catch (Throwable x) &#123; thrown = x; throw new Error(x); &#125; finally &#123; // 10. 执行完后调用钩子函数 afterExecute(task, thrown); &#125; &#125; finally &#123; task = null; // 11. 增加给 worker 完成的任务数量 w.completedTasks++; // 12. 释放锁 w.unlock(); &#125; &#125; completedAbruptly = false; &#125; finally &#123; // 13. 处理完成后, 调用钩子函数 processWorkerExit(w, completedAbruptly); &#125;&#125; 总结一下 runWorker 方法的执行过程： while 循环不断地通过 getTask()方法获取任务； getTask()方法从阻塞队列中取任务； 如果线程池正在停止，那么要保证当前线程是中断状态，否则要保证当前线程不是中断状态； 调用 task.run()执行任务； 如果 task 为 null 则跳出循环，执行 processWorkerExit()方法； runWorker 方法执行完毕，也代表着 Worker 中的 run 方法执行完毕，销毁线程。 getTaskgetTask 方法的主要工作是从阻塞队列中获取任务。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667private Runnable getTask() &#123; // 1.timeOut 变量的值表示上次从阻塞队列中取任务时是否超时 boolean timedOut = false; // Did the last poll() time out? for (; ; ) &#123; int c = ctl.get(); int rs = runStateOf(c); /* * 2. 检查线程池状态 * 如果线程池状态 rs &gt;= SHUTDOWN, 也就是非 RUNNING 状态, 再进行以下判断: * 1. rs &gt;= STOP, 线程池是否正在 stop; * 2. 阻塞队列是否为空. * 如果以上条件满足, 则将 workerCount 减 1 并返回 null. * 因为如果当前线程池状态的值是 SHUTDOWN 或以上时, 不允许再向阻塞队列中添加任务. */ // Check if queue empty only if necessary. if (rs &gt;= SHUTDOWN &amp;&amp; (rs &gt;= STOP || workQueue.isEmpty())) &#123; decrementWorkerCount(); return null; &#125; int wc = workerCountOf(c); /* * 3.timed 变量用于判断是否需要进行超时控制. * allowCoreThreadTimeOut 默认是 false, 也就是核心线程不允许进行超时; * wc &gt; corePoolSize, 表示当前线程池中的线程数量大于核心线程数量; * 对于超过核心线程数量的这些线程, 需要进行超时控制 */ // Are workers subject to culling? boolean timed = allowCoreThreadTimeOut || wc &gt; corePoolSize; /* * 4. 对线程池进行超时控制. * wc &gt; maximumPoolSize 的情况是因为可能在此方法执行阶段同时执行了 setMaximumPoolSize 方法; * timed &amp;&amp; timedOut 如果为 true, 表示当前操作需要进行超时控制, 并且上次从阻塞队列中获取任务发生了超时 * 接下来判断, 如果有效线程数量大于 1, 或者阻塞队列是空的, 那么尝试将 workerCount 减 1; * 如果减 1 失败, 则返回重试. * 如果 wc == 1 时, 也就说明当前线程是线程池中唯一的一个线程了. */ if ((wc &gt; maximumPoolSize || (timed &amp;&amp; timedOut)) &amp;&amp; (wc &gt; 1 || workQueue.isEmpty())) &#123; if (compareAndDecrementWorkerCount(c)) return null; continue; &#125; try &#123; /* * 5. 根据 timed 来判断. * 如果为 true, 则通过阻塞队列的 poll 方法进行超时控制, 如果在 keepAliveTime 时间内没有获取到任务, 则返回 null; * 否则通过 take 方法, 如果这时队列为空, 则 take 方法会阻塞直到队列不为空. */ Runnable r = timed ? workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) : workQueue.take(); if (r != null) return r; // 6. 如果 r == null, 说明已经超时, timedOut 设置为 true timedOut = true; &#125; catch (InterruptedException retry) &#123; // 7. 如果获取任务时当前线程发生了中断, 则设置 timedOut 为 false 并返回循环重试 timedOut = false; &#125; &#125;&#125; processWorkerExitgetTask 方法返回 null 时，在 runWorker 方法中会跳出 while 循环，然后会执行 processWorkerExit 方法。 123456789101112131415161718192021222324252627282930313233343536private void processWorkerExit(Worker w, boolean completedAbruptly) &#123; // 1. 如果 completedAbruptly 值为 true, 则说明线程执行时出现了异常, 需要将 workerCount 减 1; 如果线程执行时没有出现异常, 说明在 getTask()方法中已经已经对 workerCount 进行了减 1 操作, 这里就不必再减了. if (completedAbruptly) // If abrupt, then workerCount wasn't adjusted decrementWorkerCount(); final ReentrantLock mainLock = this.mainLock; mainLock.lock();, try &#123; // 2. 统计完成的任务数 completedTaskCount += w.completedTasks; // 3. 从 workers 中移除, 也就表示着从线程池中移除了一个工作线程 workers.remove(w); &#125; finally &#123; mainLock.unlock(); &#125; // 4. 根据线程池状态进行判断是否结束线程池 tryTerminate(); int c = ctl.get(); /* * 5. 当线程池是 RUNNING 或 SHUTDOWN 状态时, 如果 worker 是异常结束, 那么会直接 addWorker; * 如果 allowCoreThreadTimeOut=true, 并且等待队列有任务, 至少保留一个 worker; * 如果 allowCoreThreadTimeOut=false,workerCount 不少于 corePoolSize. */ if (runStateLessThan(c, STOP)) &#123; if (!completedAbruptly) &#123; int min = allowCoreThreadTimeOut ? 0 : corePoolSize; if (min == 0 &amp;&amp; !workQueue.isEmpty()) min = 1; if (workerCountOf(c) &gt;= min) return; // replacement not needed &#125; addWorker(null, false); &#125;&#125; 至此，processWorkerExit 执行完之后，工作线程被销毁，以上就是整个工作线程的生命周期，从 execute 方法开始，Worker 使用 ThreadFactory 创建新的工作线程，runWorker 通过 getTask 获取任务，然后执行任务，如果 getTask 返回 null，进入 processWorkerExit 方法，整个线程结束。 线程池的监控通过线程池提供的参数进行监控。 getTaskCount：线程池已经执行的和未执行的任务总数； getCompletedTaskCount：线程池已完成的任务数量，该值小于等于 taskCount； getLargestPoolSize：线程池曾经创建过的最大线程数量。通过这个数据可以知道线程池是否满过，也就是达到了 maximumPoolSize； getPoolSize：线程池当前的线程数量； getActiveCount：当前线程池中正在执行任务的线程数量。 通过这些方法，可以对线程池进行监控，在 ThreadPoolExecutor 类中提供了几个空方法，如 beforeExecute（线程执行之前调用）方法，afterExecute（线程执行之后调用）方法和 terminated（线程池退出时候调用）方法，可以扩展这些方法在执行前或执行后增加一些新的操作，例如统计线程池的执行任务的时间等，可以继承自 ThreadPoolExecutor 来进行扩展。 线程池容量的动态调整ThreadPoolExecutor 提供了动态调整线程池容量大小的方法：setCorePoolSize()和 setMaximumPoolSize()。 setCorePoolSize：设置核心池大小 setMaximumPoolSize：设置线程池最大能创建的线程数目大小 当上述参数从小变大时，ThreadPoolExecutor 进行线程赋值，还可能立即创建新的线程来执行任务。 如何合理配置线程池的大小一般需要根据任务的类型来配置线程池大小： 任务性质不同的任务可以用不同规模的线程池分开处理。CPU 密集型任务配置尽可能少的线程数量，如配置 Ncpu+1 个线程的线程池。IO 密集型任务则由于需要等待 IO 操作，线程并不是一直在执行任务，则配置尽可能多的线程，如 2*Ncpu。混合型的任务，如果可以拆分，则将其拆分成一个 CPU 密集型任务和一个 IO 密集型任务，只要这两个任务执行的时间相差不是太大，那么分解后执行的吞吐率要高于串行执行的吞吐率，如果这两个任务执行时间相差太大，则没必要进行分解。我们可以通过 Runtime.getRuntime().availableProcessors()方法获得当前设备的 CPU 个数。 《服务器性能 IO 优化》中发现一个估算公式： 最佳线程数目 =（（线程等待时间 + 线程 CPU 时间）/ 线程 CPU 时间）*CPU 数目 比如平均每个线程 CPU 运行时间为 0.5s，而线程等待时间（非 CPU 运行时间，比如 IO）为 1.5s，CPU 核心数为 8，那么根据上面这个公式估算得到：((0.5+1.5)/0.5)*8=32。这个公式进一步转化为： 最佳线程数目 =（线程等待时间与线程 CPU 时间之比 + 1）*CPU 数目 可以得出一个结论：线程等待时间所占比例越高，需要越多线程。线程 CPU 时间所占比例越高，需要越少线程。当然，这只是一个参考值，具体的设置还需要根据实际情况进行调整，比如可以先将线程池大小设置为参考值，再观察任务运行情况和系统负载、资源利用率来进行适当调整。 参考博文[1]. 深入理解 Java 线程池：ThreadPoolExecutor[2]. java8 线程池[3]. Java 并发编程：线程池的使用[4]. 如何合理地估算线程池大小？ Java 并发编程之美系列 Java 并发编程之美（一）：并发队列 Queue 原理剖析 Java 并发编程之美（二）：线程池 ThreadPoolExecutor 原理探究 Java 并发编程之美（三）：异步执行框架 Eexecutor Java 并发编程之美（四）：深入剖析 ThreadLocal Java 并发编程之美（五）：揭开 InheritableThreadLocal 的面纱 Java 并发编程之美（六）：J.U.C 之线程同步辅助工具类]]></content>
      <categories>
        <category>并发</category>
      </categories>
      <tags>
        <tag>线程池</tag>
        <tag>ThreadPoolExecutor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解 Java 虚拟机（二）：JVM 垃圾收集器]]></title>
    <url>%2Farchives%2F1a0b1a8e.html</url>
    <content type="text"><![CDATA[概述从上一篇文章中，我们知道了内存运行时区域的各个部分，其中程序计数器、虚拟机栈、本地方法栈 3 个区域随线程而生，随线程而灭；栈中的栈帧随着方法的进入和退出而有条不紊地执行着出栈和入栈操作。每一个栈帧中分配多少内存基本上是在类结构能确定下来时就已知的，因此这几个区域的内存分配和回收都具备确定性，在这几个区域内就不需要过多考虑回收的问题，因为方法结束或者线程结束时，内存自然就跟随着回收了。而 Java 堆和方法区则不一样，一个接口中的多个实现类需要的内存可能不一样，一个方法中的多个分支需要的内存也可能不一样，我们只有在程序处于运行期间时才能知道会创建哪些对象，这部分内存的分配和回收都是动态的，垃圾收集器所关注的是这部分内存。 上一篇文章讲述了 Java 内存区域与内存溢出异常，本章将介绍垃圾收集的算法，然后分析几款 JDK1.7 中提供的垃圾收集器特点以及运作原理。 说明Java GC（Garbage Collection，垃圾收集，垃圾回收）机制对 JVM（Java Virtual Machine）中的内存进行标记，并确定哪些内存需要回收，根据一定的回收策略，自动的回收内存，永不停息（Nerver Stop）的保证 JVM 中的内存空间，防止出现内存泄露和溢出问题。我们将从 4 个方面学习 Java GC 机制： Who：哪些是不再使用要被当做 “垃圾” 回收处理的对象？也就是要确定垃圾对象。Where：在哪里执行垃圾回收？明确要清理的内存区域。When：什么时候执行 GC 操作？即 JVM 触发 GC 的时机。How：怎么样进行垃圾对象处理？即 GC 的实现算法。 判断对象存活（判别算法或搜索算法）在堆里面存放着 Java 世界中几乎所有的对象实例，垃圾收集器在堆进行垃圾回收前，第一件事就是要确定这些对象之中哪些还 “存活” 着，那些已经“死去”（即不可能再被任何途径使用的对象）。 引用计数算法给对象中添加一个引用计数器，每当有一个地方引用它是，计数器值就加 1；当引用失效时，计数器值就减 1；任何时刻计数器为 0 的对象就是不可能再被使用的。 客观地说，引用计数算法（Reference Counting）的实现简单，判定效率也很高，在大部分情况下它都是一个不错的算法，也有一些比较著名的应用案例，例如微软公司的 COM（Component Object Model）技术、使用 ActionScript 3 的 Flash Player、Python 语言和游戏脚本领域被广泛应用的 Squirrel 中都使用了引用计数算法进行内存管理。但是，至少主流的 Java 虚拟机里面没有选用引用计数算法来管理内存，其中最主要的原因是它很难解决对象之间相互循环引用的问题。 可达性分析算法在主流的商用程序语言中（Java、C#）的主流实现中，都是称通过可达性分析（Reachability Analusis）来判断对象是否存活的。这个算法的基本思路就是通过一系列的称为 “GC Roots” 的对象作为起始点，从这些节点开始向下搜索，搜索所走过的路径称为引用链（Reference Chain），当一个对象到 GC Roots 没有任何引用链相连，则证明此对象是不可用的。 在 Java 语言中，可作为 GC Roots 的对象包括下面几种： 虚拟机栈（栈帧中的本地变量表）中引用的对象 方法区中类静态属性引用的对象 方法区中常量引用的对象 本地方法栈 JNI（Native 方法）引用的对象 引用无论是通过引用计数算法判断对象的引用数量，还是通过可达性分析算法判断对象的引用链是否可达，判断对象是否存活都与 “引用” 有关。在 JDK 1.2 以前，Java 中引用的定义很传统：如果 reference 类型的数据中存储的数值代表的是另外一块内存的起始地址，就称这块内存代表着一个引用。 在 JDK 1.2 之后，Java 对引用的概念进行了扩充，将引用分为强引用（Strong Reference）、软引用（Soft Reference）、弱引用（Weak Reference）、虚引用（Phantom Reference）4 种，这 4 种引用强度依次逐渐减弱。 强引用就是指在程序代码之中普遍存在的，类似 “Ojbect obj = new Object()” 这类的引用，只要强引用还存在，垃圾收集器永远不会回收掉被引用的对象。 软引用是用来描述一些还在但并非必需的对象。对于软引用关联着的对象，在系统将要发生内存溢出异常之前，将会把这些对象列进回收范围之中进行第二次回收。如果这次回收还没有足够的内存，才会抛出内存溢出异常。在 JDK 1.2 之后，提供了 SoftReference 类来实现软引用。 弱引用也是用来描述非必须对象的，但是它的强度比软引用更弱一些，被弱引用关联的对象只能生存到下一次垃圾收集发生之前。当垃圾收集器工作时，无论当前内存是否足够，都会回收掉之被弱引用关联的对象。在 JDK 1.2 之后，提供了 WeakReference 类来实现弱引用。 虚引用也称为幽灵引用或者幻影引用，它是最弱的一种引用关系。一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用来取得一个对象实例。为一个对象设置虚引用关联的唯一目的就是能在这个对象被收集器回收时收到一个系统通知。在 JDk 1.2 之后，提供了 PhantomReference 类来实现虚引用。 1234567891011121314151617// 1. 强引用: 被强引用关联的对象不会被回收, 使用 new 一个新对象的方式来创建强引用.Object obj = new Object();// 2. 软引用: 被软引用关联的对象只有在内存不够的情况下才会被回收, 使用 SoftReference 类来创建软引用.Object obj = new Object();SoftReference&lt;Object&gt; sf = new SoftReference&lt;&gt;(obj);obj = null; // 使对象只被软引用关联// 3. 弱引用: 被弱引用关联的对象一定会被回收, 也就是说它只能存活到下一次垃圾回收发生之前, 使用 WeakReference 类来创建弱引用.Object obj = new Object();WeakReference&lt;Object&gt; wf = new WeakReference&lt;&gt;(obj);obj = null;// 4. 虚引用: 又称为幽灵引用或者幻影引用, 一个对象是否有虚引用的存在, 不会对其生存时间造成影响, 也无法通过虚引用得到一个对象. 为一个对象设置虚引用的唯一目的是能在这个对象被回收时收到一个系统通知. 使用 PhantomReference 来创建虚引用.Object obj = new Object();PhantomReference&lt;Object&gt; pf = new PhantomReference&lt;&gt;(obj, null);obj = null; 生存还是死亡即使在可达性分析算法中不可达的对象，也并非是 “非死不可” 的，要真正宣告一个对象死亡，至少要经历两次标记过程：如果对象在进行可达性分析后发现没有与 GC Roots 相连接的引用链，那它将会被第一次标记并且进行一次筛选，筛选的条件是此对象是否有必要执行 finalize()方法。当对象没有覆盖 finalize()方法，或者 finalize()方法已经被虚拟机调用过，虚拟机将这两种情况都视为“没有必要执行”。 如果这个对象被判定为有必要执行 finalize()方法，那么这个对象将会放置在一个叫做 F-Queue 的队列之中，并在稍后由一个由虚拟机自动建立的、低优先级的 Finalizer 线程去执行它。这里所谓的 “执行” 是指虚拟机会触发这个方法，但并不承诺会等待它运行结束，这样做的原因是，如果一个对象在 finalize()方法中执行缓慢，或者发生了死循环，将很可能导致 F-Queue 队列中其他对象永久处于等待，甚至导致整个内存回收系统崩溃。finalize()方法是对象逃脱死亡命运的最后一次机会，稍后 GC 将对 F-Queue 中的对象进行第二次小规模的标记，如果对象要在 finalize()中成功拯救自己——只要重新与引用链上的任何一个对象建立关联即可，譬如把自己（this 关键字）赋值给某个变量或者对象的成员变量，那在第二次标记时它将被移除出 “即将回收” 的集合；如果对象这时候还没有逃脱，那基本上它就真的被回收了。 finalize()方法尽量避免使用它，因为它不是 C/C++ 中的析构函数，而是 Java 刚诞生时为了使 C/C++ 程序员更容易接受它所做出的一个妥协。它的运行代价高昂，不确定性大，所以建议完成可以忘掉 Java 语言中这个方法的存在。 回收方法区很多人认为方法区（或者 HotSpot 虚拟机中的永久代）是没有垃圾收集的，Java 虚拟机规范中确实说可以不要求虚拟机在方法区实现垃圾收集，而且在方法区中进行垃圾收集的 “性价比” 一般比较低：在堆中，尤其是在新生代中，常规应用进行一次垃圾收集一般可以回收 70%～95% 的空间，而永久代的垃圾收集效率远低于此。 永久代的垃圾收集主要回收两部分的内容：废弃常量和无用的类。回收废弃常量和回收 Java 堆中的对象非常相似。以常量池中字面量的回收为例，假如一个字符串 “abc” 已经进入了常量池中，但是当前系统没有任何一个 String 对象是叫做 “abc” 的，换句话说，就是没有任何 String 对象引用常量池中的 “abc” 常量，也没有其他地方引用了这个字面量，如果这时发生内存回收，而且必要的话，这个 “abc” 常量就会被系统清理出常量池。常量池中的其他类（接口）、方法、字段的符号引用也与此类似。 判定一个常量是否是 “废弃常量” 比较简单，而要判定一个类是否是 “无用的类” 的条件则相对苛刻许多。类需要同时满足下面 3 个条件才能算是“无用的类”： 该类所有的实例都已经被回收，也就是 Java 堆中不存在该类的任何实例。 加载该类的 ClassLoader 已经被回收。 该类对应的 java.lang.Class 对象没有在任何地方被引用，无法在任务地方通过反射访问该类的方法。 虚拟机可以堆满足上述 3 个条件的无用类进行回收，这里说的仅仅是“可以”，而并不是和对象一样，不使用了就必然会回收。 在大量使用反射、动态代理、CGLib 等 ByteCode 框架、动态生成 JSP 以及 OSGi 这类频繁自定义 ClassLoader 的场景都需要虚拟机具备类卸载的功能，以保证永久代不会溢出。 垃圾收集算法标记 - 清除算法最基础的收集算法是 “标记 - 清除”（Mark-Sweep）算法，分为“标记” 和“清除”两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象。之所以说它是最基础的收集算法，是因为后续的收集算法都是基于这种思路并对其不足进行改进而得到的。它主要不足有两个：一个是效率问题，标记和清除两个过程的效率都不高；另外一个是空间问题，标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致以后在程序运行过程中需要分配较大对象时，无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。 复制算法为了解决效率问题，一种称为“复制”（Copying）的收集算法出现了，它将可用内存按容量划分为大小相等的两块，每次只使用其中一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清除掉。这样使得每次都是对整个半区进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。 现在的商业虚拟机都采用这种收集算法来回收新生代，IBM 公司的专门研究表明，新生代中的对象 98% 是 “朝生夕死” 的，所以并不需要按照 1:1 的比列来划分内存空间，而是将内存分为一块较大的 Eden 空间和两块较小的 Survivor 空间，每次使用 Eden 和其中一块 Survivor。当回收时，将 Eden 和 Survivor 中还存活着的对象一次性复制到另外一块 Survivor 空间上，最后清除掉 Eden 和刚才用过的 Survivor 空间。HotSpot 虚拟机默认 Eden 和 Survivor 的大小比列是 8:1，也就是每次新生代中可用内存空间为整个新生代容量的 90%（80%+10%），只有 10% 的内存会被“浪费”。当然，98% 的对象可回收只是一般场景下的数据，我们没有办法保证每次回收都只有不多于 10% 的对象存活，当 Survivor 空间不够时，需要依赖其他内存（这里指老年代）进行分配担保（Handle Promotion）。 标记 - 整理算法复制收集算法在对象存活率较高时就要进行较多的复制操作，效率将会变低。更关键的是，如果不想浪费 50% 的空间，就需要有额外的空间进行分配担保，以应对被使用的内存中所有对象都 100% 存活的极端情况，所以在老年代一般不能直接选用这种算法。 根据老年代的特点，有人提出了另外一种 “标记 - 整理”（Mark—Compact）算法，标记过程仍然与“标记 - 清除” 算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。 分代收集算法当前商业虚拟机的垃圾收集都采用 “分代收集”（Generrational Collection）算法，根据对象存活周期的不同将内存划分为几块。一般是把 Java 堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法， 只需要付出少量存活对象的复制成本就可以完成收集。而老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用“标记 - 清除” 或者 “标记 - 整理” 算法来进行回收。 垃圾收集器 如图展示了 7 种用于不同分代的收集器，如果两个收集器之间存在连线，就说明它们可以搭配使用。虚拟机所处的区域，则表示它是属于新生代收集器还是老年代收集器。 Serial 收集器Serial 收集器是最基本、发展历史最悠久的收集器，曾经（在 JDK 1.3.1 之前）是虚拟机新生代收集的唯一选择。这个收集器是一个单线程的收集器，但它的 “单线程” 的意义并不仅仅说明它只会使用一个 CPU 或者一条收集线程去完成垃圾收集工作，更重要的是在它进行垃圾收集时，必须暂停其他所有的工作线程，直到它收集结束。 从 JDk 1.3 开始，一直到现在的 JDk 1.7，HotSpot 虚拟机开发团队为消除或者减少工作线程因内存回收而导致停顿的努力一直在进行着，从 Serial 收集器到 Parallel 收集器，再到 Concurrent Mark Sweep（CMS）乃至 GC 收集器的最前沿成果 Garbage First（G1）收集器，用户线程的停顿时间在不断缩短，但是仍然没有办法完全消除。 Serial 收集器仍然是虚拟机运行在 Client 模式下的默认新生代收集器。它也有着优于其他收集器的地方：简答而高效，对于限定单个 CPU 的环境来说，Serial 收集器由于没有线程交互的开销，专心做垃圾收集自然可以获得最高的单线程收集效率。 ParNew 收集器ParNew 收集器是一个新生代收集器，使用复制算法的收集器，又是并行 [1] 的多线程收集器。 ParNew 收集器其实就是 Serial 收集器的多线程版本，除了使用多线程进行垃圾收集之外，其余行为包括 Serial 收集器可用的所有控制参数、收集算法、Stop The World、对象分配规则、回收策略等都与 Serial 收集器完全一样。 ParNew 收集器是许多运行在 Server 模式下虚拟机中首选的新生代收集器，其中有一个与性能无关但很重要的原因是，除了 Serial 收集器外，目前只有它能与 CMS 收集器配合工作。所以在 JDK1.5 中使用 CMS 来收集老年代的时候，新生代只能选择 ParNew 或者 Serial 收集器中的一个。ParNew 收集器也是使用 - XX:UseConcMarkSweepGC 选项后默认新生代收集器，也可以使用 - XX:+UseParNewGC 选项来强制指定它。 ParNew 收集器默认开启的收集线程数与 CPU 的数量相同，在 CPU 非常多的环境下，可以使用 - XX:ParallelGCThreads 参数来限制垃圾收集的线程数。 Parallel Scavenge 收集器Parallel Scavenge 收集器是一个新生代收集器，使用复制算法的收集器，又是并行的多线程收集器。 Parallel Scavenge 收集器的特点是它的关注点与其他收集器不同，CMS 等收集器的关注点是尽可能地缩短垃圾收集器时用户线程的停顿时间，而 Parallel Scavenge 收集器的目标则是达到一个控制的吞吐量（Throughput）。所谓吞吐量就是 CPU 用于运行用户代码的时间与 CPU 总消耗时间的比值，即吞吐量 = 运行用户代码时间 /（运行用户代码时间 + 垃圾收集时间），虚拟机总共运行了 100 分钟，其中垃圾收集花掉 1 分钟，那吞吐量就是 99%。 停顿时间越短就越适合需要与用户交互的程序，良好的响应速度能提升用户体验，而高吞吐量则可以高效率地利用 CPU 时间，尽快完成程序的运算任务，主要适合在后台运行而不需要太多交互的任务。 由于与吞吐量关系密切，Parallel Scavenge 收集器也经常称为 “吞吐量优先” 收集器。Parallel Scavenge 收集器参数 - XX:+UseAdaptiveSizePolicy 打开后就不需要手工指定新生代的大小（-Xmn）、Eden 与 Survivor 区的比例（-XX:SurvivorRatio）、晋升老年代对象年龄（-XX:PretenureSizeThreshold）等细节参数，虚拟机会根据当前系统的运行情况手机性能监控信息，动态调整这些参数以提供最适合的停顿时间或者最大的吞吐量，这种调节方式称为 GC 自适应的调节策略（GC Ergonomics）。自适应调节策略也是 Parallel Scavenge 收集器与 ParNew 收集器的一个重要区别。 Serial Old 收集器Serial Old 是 Serial 收集器的老年代版本，它同样是单线程收集器，使用 “标记 - 整理” 算法。这个收集器的主要意义也是在于给 Client 模式下的虚拟机使用。如果在 Server 模式下，那么它主要还有两大用途：一种用途是在 JDK 1.5 以及之前的版本中与 Parallel Scavenge 收集器搭配使用 [2] ，另一种用途就是作为 CMS 收集器的后备预案，在并发收集发生 Concurrent Mode Failue 时使用。 Parallel Old 收集器Parallel Old 是 Parallel Scavenge 收集器的老年代版本，使用多线程和 “标记 - 整理” 算法。这个收集器是在 JDK 1.6 中才开始提供的，在此之前，新生代的 Parallel Scavenge 收集器一直处于比较尴尬的状态。原因是，如果新生代选择了 Parallel Scavenge 收集器，老年代除了 Serial Old（PS MarkSweep）收集器外别无选择（Parallel Scavenge 收集器无法与 CMS 收集器配置工作）。由于老年代 Serial Old 收集器在服务端应用性能上的“拖累”，使用了 Parallel Scavenge 收集器也未必能在整体应用上获得吞吐量最大化的效果。 直到 Parallel Old 收集器出现后，“吞吐量优先”收集器终于有了比较名副其实的应用组合，在注重吞吐量以及 CPU 资源敏感的场合，都可以优先考虑 Parallel Scavenge 加上 Parallel Old 收集器。 CMS 收集器CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。目前很大一部分的 Java 应用集中在互联网站或者 B/S 系统的服务端上，这类应用尤其重视服务的响应速度，希望系统停顿时间最短，以给用户带来较好的体验。CMS 收集器就非常符合这类用用的需求。 CMS 收集器是基于 “标记 - 清除” 算法实现的，整个过程分为 4 个步骤，包括： 初始标记（CMS initial mark） 并发标记（CMS concurrent mark） 重新标记（CMS remark） 并发清除（CMS concurrent sweep） 其中，初始标记、重新标记这两个步骤仍然需要“Stop The World”。初始标记仅仅是标记一下 GC Roots 能直接关联到的对象，速度很快，并发标记阶段就是进行 GC Roots Tracing 的过程，而重新标记阶段则是为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般比初始标记阶段稍长一些，但远比并发标记的时间短。 由于整个过程中耗时最长的并发标记和并发清除过程收集器线程都可以与用户线程一起工作，所以，从总体上来说，CMS 收集器的内存回收过程就是与用户线程一起并发执行的。 CMS 收集器具备并发收集、低停顿等优点，但它有以下 3 分明显的缺点： CMS 收集器对 CPU 资源非常敏感。在并发阶段，它虽然不会导致用户线程停顿，但是会因为占用了一部分线程（或者说资源）而导致应用程序变慢，总吞吐量会降低。CMS 默认启动的回收线程数是（CPU 数量 + 3）/4，也就是当 CPU 在 4 个以上时，并发回收时垃圾收集线程不少于 25% 的 CPU 资源，并且随着 CPU 数量的增加而下降。但是当 CPU 不足 4 个（譬如 2 个）时，CMS 对用户程序的影响就可能变得很大，如果本来 CPU 负载就比较大，还分出一半的运算能力去执行收集器线程，就可能导致用户程序的执行速度忽然降低了 50%。 CMS 收集器无法处理浮动垃圾（Floating Garbage），可能出现 “Concurrent Mode Failure” 失败而导致另一次 Full GC 的产生。由于 CMS 并发清除阶段用户线程还在运行着，伴随程序运行自然就还会有新的垃圾不断产生，这一部分垃圾出现在标记过程之后，CMS 无法在当次收集中处理掉它们，只好留待下一次 GC 时再清理掉。这一部分垃圾就称为 “浮动垃圾”。也是由于垃圾收集阶段用户线程还需要运行，那也就还需要预留足够的内存空间给用户线程使用，因此 CMS 收集器不能像其他收集器那样等到老年代几乎完全被填满了再进行收集，需要预留一部分空间提供并发收集时的程序运作使用。在 JDK 1.6 中，CMS 收集器的启动阀值（-XX:CMSInitiatingOccupancyFraction）已经提升至 92%。要是 CMS 运行期间预留的内存无法满足程序需要，就会出现一次“Concurrent Mode Failure” 失败，这次虚拟机将启动后备预案：临时启用 Serial Ols 收集器来重新进行老年代的垃圾收集，这样停顿时间就很长了。 CMS 是一款基于 “标记 - 清除” 算法实现的收集器，收集结束时会有大量空间碎片产生。空间碎片过多时，将会给大对象分配带来很大的麻烦，往往会出现老年代还有很大空间剩余，但是无法找到足够大的连续空间来分配当前对象，不得不提前触发一次 Full GC。为了解决这个问题，CMS 收集器提供了一个 - XX:+UseCMSCompactAtFullCollection 开关参数（默认就是开启的），用于在 CMS 收集器顶不住要进行 Full GC 时开启内存碎片的合并整理过程，内存整理的过程是无法并发的，空间碎片问题没有了，但是停顿时间不得不变长。虚拟机设计者还提供了另外一个参数 - XX:CMSFullGCsBeforeCompaction，这个参数用于设置执行多少次不压缩的 Full GC 后，跟着来一次带压缩的（默认值为 0，表示每次进行 Full GC 时都进行碎片整理）。 G1 收集器G1（Grabage-First）收集器是一款面向服务端应用的垃圾收集器。HotSpot 开发团队赋予它的使命是未来可以替换掉 JDK1.5 中发布的 CMS 收集器。与其他 GC 收集器相比，G1 具备如下特点： 并行与并发：G1 能充分利用多 CPU、多核环境下的硬件优势，使用多个 CPU 来缩短 Stop—The-World 停顿时间。 分代收集：与其他收集器一样，分代概念在 G1 中依然得以保留。G1 收集器能够采用不同的方式去处理新创建的对象和已经存活了一段时间、熬过多次 GC 的旧对象以获取更好的收集效果。 空间整合：与 CMS 的 “标记 - 清除” 算法不同，G1 从整体来看是基于 “标记 - 整理” 算法实现的收集器，从局部（两个 Region 之间）上来看是基于 “复制” 算法实现的，无论如何，这两种算法都意味着 G1 运作期间不会产生内存空间碎片，收集后能提供规整的可用内存。 可预测的停顿时间：降低停顿时间是 G1 和 CMS 共同的关注点，但 G1 除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为 M 毫米的时间片段内，消耗在垃圾收集上的时间不得超过 N 毫秒。 使用 G1 收集器时，Java 堆的内存布局就与其他收集器有很大差别，它将整个 Java 堆划分为多个大小相等的独立区域（Region），虽然还保留新生代和老年代的概念，但是新生代和老年代不再是物理隔离的了，它们都是一部分 Region 的集合。 G1 收集器之所以能建立可预测的停顿时间模型，是因为它可以有计划地避免在整个 Java 堆中进行全区域的收集收集。G1 跟踪各个 Region 里面的垃圾堆积的价值大小（回收所获得的空间大小以及回收所需要时间的经验值），在后台维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的 Region。这种使用 Region 划分内存空间以及有优先级的区域回收方式，保证了 G1 收集器在有限的时间内可以获取尽可能高的收集效率。 在 G1 收集器中，Region 之间的对象引用以及其他收集器中的新生代与老年代之间的对象引用，虚拟机都是使用 RememberedSer 来避免全堆扫描的。G1 中每个 Region 都有一个与之对应的 Remembered Set，虚拟机发现程序在堆 Reference 类型的数据进行写操作时，会产生一个 WriteBarrier 暂时中断写操作，检查 Reference 引用的对象是否处于不同的 Region 之间（在分代的例子中就是检查是否老年代中的对象引用了新生代中的对象），如果是，便通过 CardTable 吧相关引用信息记录到被引用对象所属的 Region 的 RemeberadSet 之中。当进行内存回收时，在 GC 根节点的枚举范围中加入 Remembered Set 即可保证不对全堆扫描也不会有遗漏。 如果不计算维护 Remembered Set 的操作，G1 收集器的运作大致可划分为以下几个步骤： 初始标记（Initial Marking） 并发标记（Concurrent Marking） 最终标记（Final Marking） 筛选回收（Live Date Counting and Evacuation） 初始标记阶段仅仅是标记一下 GC Roots 能直接关联到的对象，并且修改 TAMS（Next Top at Mark Start）的值，让下一阶段用户程序并发运行时，能在正确可用的 Region 中创建新对象，这个阶段需要停顿线程，但耗时很短。并发标记阶段是从 GC Roots 开始对堆中对象进行可达性分析，找出存活的对象，这阶段耗时比较长，但可与用户程序并发执行。而最终标记阶段则是为了修正在并发标记期间因用户程序继续运行而导致标记苍生变化的那一部分标记记录，虚拟机将这段时间对象变化记录在线程 Remembered Set Logs 里面，最后标记阶段需要把 Remembered Set Logs 的数据合并到 Remembered Set 中，这阶段需要停顿线程，但是可并行执行。最后在筛选阶段首先对各个 Region 的回收价值和成本进行排序，根据用户所期望的 GC 停顿时间来制定回收计划。这个阶段其实也可以做到与用户程序一起并发执行，但是因为只回收一部分 Region，时间是用用户可控制的，而且停顿用户线程将大幅提高收集效率。 GC 日志理解 GC 日志我们通过使用下面的启动参数让 JVM 打印出详细的 GC 日志： -XX:+PrintGCDetails-XX:+PrintGCDateStamps-XX:+PrintGCTimeStamps 12342019-01-08T23:50:22.060-0800: 8.723: [GC (Metadata GC Threshold) [PSYoungGen: 129014K-&gt;10749K(195072K)] 138630K-&gt;27347K(247296K), 0.0257816 secs] [Times: user=0.05 sys=0.01, real=0.02 secs] 2019-01-08T23:50:22.085-0800: 8.749: [Full GC (Metadata GC Threshold) [PSYoungGen: 10749K-&gt;0K(195072K)] [ParOldGen: 16597K-&gt;22228K(91136K)] 27347K-&gt;22228K(286208K), [Metaspace: 34001K-&gt;34001K(1079296K)], 0.1026725 secs] [Times: user=0.24 sys=0.01, real=0.10 secs] 2019-01-08T23:50:24.385-0800: 11.049: [GC (Allocation Failure) [PSYoungGen: 184320K-&gt;13282K(199680K)] 206548K-&gt;35518K(290816K), 0.0257038 secs] [Times: user=0.04 sys=0.01, real=0.03 secs] 2019-01-08T23:50:27.125-0800: 13.789: [GC (Allocation Failure) [PSYoungGen: 197602K-&gt;16376K(252928K)] 219838K-&gt;38723K(344064K), 0.0423354 secs] [Times: user=0.07 sys=0.02, real=0.04 secs] 2019-01-08T23:50:24.385-0800:[1] 11.049[2]: [GC[3] (Allocation Failure)[4] [PSYoungGen[5]: 184320K-&gt;13282K(199680K)[6]] 206548K-&gt;35518K(290816K)[7], 0.0257038 secs[8]] [Times: user=0.04 sys=0.01, real=0.03 secs][9] [1]. 2019-01-08T23:50:24.385-0800 - GC 事件（GC event）开始的时间点[2]. 11.049 - GC 事件的开始时间，相对于 JVM 的启动时间，单位是秒（Measured in seconds）[3]. [GC - 用来区分（distinguish）是 Minor GC 还是 Full GC 的标志（Flag）。这里的 GC 表明本次发生的是 Minor GC。“[GC” 和“[Full GC”说明了这次垃圾收集的停顿类型，而不是用来区分新生代 GC 还是老年代 GC 的。如果有 “Full”，说明这次 GC 是发生了 Stop-The-World 的，假如新生代收集器 ParNew 的日志出现“[Full GC”，说明 GC 出现了分配担保失败之类的问题，所以才导致 STW。如果是调用 System.gc() 方法所触发的收集，那么将显示“[Full GC(System)”。[4]. (Allocation Failure) - 引起垃圾回收的原因。本次 GC 是因为年轻代中没有任何合适的区域能够存放需要分配的数据结构而触发的。[5]. PSYoungGen - 表明本次 GC 发生在年轻代并且使用的是 Parallel Scavenge 垃圾收集器。“[DefNew”、“[Tenured”、“[Perm”表示 GC 发生的区域，这里显示的区域名称与使用的 GC 收集是密切相关的，例如上面样例所使用的 Serial 收集器中的新生代名为“Default New Generation”，所以显示的是“[DefNew”。如果是 ParNew 收集器，新生代名称就会变为“[ParNew”，意为“Parallel New Generation”。如果采用 Parallel Scavenge 收集器，那它配套的新生代称为“PSYoungGen”，老年代和永久代同理，名称也是由收集器决定的。“ParOldGen”Parallel Scavenge 收集器配套的老年代。[6]. 184320K-&gt;13282K(199680K) - GC 前该内存区域已使用容量 -＞GC 后该内存区域已使用容量（该内存区域总容量）[7]. 206548K-&gt;35518K(290816K) - GC 前 Java 堆已使用容量 -＞GC 后 Java 堆已使用容量（Java 堆总容量）[8]. 0.0257038 secs - 表示该内存区域 GC 所占用的时间，单位是秒[9]. [Times: user=0.04 sys=0.01, real=0.03 secs] - 这里面的 user、sys 和 real 与 Linux 的 time 命令所输出的时间含义一致，分别代表用户态消耗的 CPU 时间、内核态消耗的 CPU 事件和操作从开始到结束所经过的墙钟时间（Wall Clock Time）。CPU 时间与墙钟时间的区别是，墙钟时间包括各种非运算的等待耗时，例如等待磁盘 I/O、等待线程阻塞，而 CPU 时间不包括这些耗时，但当系统有多 CPU 或者多核的话，多线程操作会叠加这些 CPU 时间，所以读者看到 user 或 sys 时间超过 real 时间是完全正常的。由于串行垃圾收集器（Serial Garbage Collector）只会使用单个线程，所以 real time 等于 user 以及 system time 的总和。 jvm 设置滚动记录 GC 日志1-XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:/var/www/logs/gc.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=3 -XX:GCLogFileSize=512k [1]. -XX:UseGCLogFileRotation - 打开或关闭 GC 日志滚动记录功能，要求必须设置 -Xloggc 参数[2]. -XX:NumberOfGCLogFiles - 设置滚动日志文件的个数，必须大于等于 1[3]. -XX:GCLogFileSize - 设置滚动日志文件的大小，必须大于 8k；当前写日志文件大小超过该参数值时，日志将写入下一个文件[4]. -XX:+PrintGCDetails - 输出 GC 的详细日志[5]. -XX:+PrintGCTimeStamps - 输出 GC 的时间戳（以基准时间的形式）[6]. -XX:+PrintGCDateStamps - 输出 GC 的时间戳（以标准时间的形式）[7]. -Xloggc:/var/www/logs/gc.log - 默认情况下 GC 日志直接输出到标准输出，不过使用 - Xloggc:filename 标志也能修改输出到某个文件。除非显式地使用 - PrintGCDetails 标志，否则使用 - Xloggc 会自动地开启基本日志模式（文件路径必须存在，否则不能产生 GC 日志）[8]. -XX:+PrintHeapAtGC - 了解堆的更详细的信息 在运行时开启 / 关闭 GC 日志jinfo在 JDK bundle 中隐藏着一个精悍的小工具—jinfo。作为一个命令行工具，jinfo 用于收集正在运行的 Java 进程的配置信息。jinfo 吸引眼球的地方在于，它能通过 - flag 选项动态修改指定的 Java 进程中的某些 JVMflag 的值。虽然这样的 flag 数量有限，但它们偶尔能够帮助到你。 通过以下的命令你便能看到 JVM 中哪些 flag 可以被 jinfo 动态修改： 12345678910111213141516171819202122chenxingxingdeMacBook-Pro:gc chenxingxing$ java -XX:+PrintFlagsFinal -version|grep manageable intx CMSAbortablePrecleanWaitMillis = 100 &#123;manageable&#125; intx CMSTriggerInterval = -1 &#123;manageable&#125; intx CMSWaitDuration = 2000 &#123;manageable&#125; bool HeapDumpAfterFullGC = false &#123;manageable&#125; bool HeapDumpBeforeFullGC = false &#123;manageable&#125; bool HeapDumpOnOutOfMemoryError = false &#123;manageable&#125; ccstr HeapDumpPath = &#123;manageable&#125; uintx MaxHeapFreeRatio = 100 &#123;manageable&#125; uintx MinHeapFreeRatio = 0 &#123;manageable&#125; bool PrintClassHistogram = false &#123;manageable&#125; bool PrintClassHistogramAfterFullGC = false &#123;manageable&#125; bool PrintClassHistogramBeforeFullGC = false &#123;manageable&#125; bool PrintConcurrentLocks = false &#123;manageable&#125; bool PrintGC = false &#123;manageable&#125; bool PrintGCDateStamps = false &#123;manageable&#125; bool PrintGCDetails = false &#123;manageable&#125; bool PrintGCID = false &#123;manageable&#125; bool PrintGCTimeStamps = false &#123;manageable&#125;java version "1.8.0_141"Java(TM) SE Runtime Environment (build 1.8.0_141-b15)Java HotSpot(TM) 64-Bit Server VM (build 25.141-b15, mixed mode) 如何使用 jinfo12345678chenxingxingdeMacBook-Pro:gc chenxingxing$ jps58800 Jps58727 Launcher58728 WorkerMicroService100250155 LauncherchenxingxingdeMacBook-Pro:gc chenxingxing$ jinfo -flag +PrintGCDetails 58728chenxingxingdeMacBook-Pro:gc chenxingxing$ jinfo -flag +PrintGC 58728 在 jinfo 中需要打开 - XX:+PrintGC 和 - XX:+PrintGCDetails 两个选项才能开启 GC 日志，这与用命令行参数的方式实现有着细微的差别—如果你通过启动脚本（startup script）来设置参数，仅需 - XX:+PrintGCDetails 即可，因为 - XX:+PrintGC 会被自动打开。 同理，若想关闭 GC 日志功能，只需要执行 jinfo -flag -PrintGCDetails 58728 和 jinfo -flag -PrintGC 58728 命令即可。 垃圾收集器参数总结 小知识点[1]. 查看 JVM 使用的默认的垃圾收集器:java -XX:+PrintCommandLineFlags -version；或者使用 java -XX:+PrintGCDetails -version 查看GC 的情况。[2]. 在 JDK7u4 开始的 JDK7u 系列，HotSpot VM 在选择使用 ParallelGC（-XX:+UseParallelGC 或者是 ergonomics 自动选择）的时候，会默认开启 -XX:+UseParallelOldGC。在这个改变之前，即便选择了 ParallelGC，默认情况下 ParallelOldGC 并不会随即开启，默认使用 PS-MarkSweep(Serial-Old) 进行老年代回收。如果需要 ParallelOldGC，要自己通过 -XX:+UseParallelOldGC 去选定。 参考博文[1].《深入理解 Java 虚拟机：JVM 高级特效与最佳实现》，第 3 章 注脚[1]. 并发和并行的区别（垃圾收集器上下文语境中）：并行（Parallel）：指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态。并发（Concurrent）：指用户线程与垃圾收集线程同时执行（但不一定是并行的，可能会交替执行），用户程序在继续运行，而垃圾收集程序运行于另一个 CPU 上。[2]. Serial Old 和 Ps MarkSweep 的区别：Parallel Scavenge 收集器架构中本身有 PS MarkSweep 收集器来进行老年代收集，并非直接使用了 Serial Old 收集器，但是这个 PS MarkSweep 收集器与 Serial Old 的实现非常接近，所以在官方的许多资料中都是直接以 Serial Old 代替 PS MarkSweep 进行讲解。 深入理解 Java 虚拟机系列 深入理解 Java 虚拟机（一）：Java 内存区域与内存溢出异常 深入理解 Java 虚拟机（二）：JVM 垃圾收集器 深入理解 Java 虚拟机（三）：内存分配与回收策略 深入理解 Java 虚拟机（四）：Jvm 性能监控与调优]]></content>
      <categories>
        <category>Jvm</category>
      </categories>
      <tags>
        <tag>Jvm</tag>
        <tag>GC</tag>
        <tag>垃圾收集器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java8 那些事儿（二）：Optional 类解决空指针异常]]></title>
    <url>%2Farchives%2F8eb6feba.html</url>
    <content type="text"><![CDATA[前言空指针异常是导致 Java 应用程序失败的最常见原因。以前，为了解决空指针异常，Google 公司著名的 Guava 项目引入了 Optional 类，Guava 通过使用检查空值的方式来防止代码污染，它鼓励程序员写更干净的代码。受到 Google Guava 的启发，Optional 类已经成为 Java8 类库的一部分。Optional 实际上是个容器：它可以保存类型 T 的值，或者仅仅保存 null。Optional 提供很多有用的方法，这样我们就不用显式进行空值检测。 Optional 范例构造方式Optional 的三种构造方式：Optional.of(obj)，Optional.ofNullable(obj) 和 Optional.empty()。 Optional.of()，依据一个非空值创建 Optional。 12// 如果 car 是一个 null, 这段代码会立即抛出 NullPointException, 而不是等到试图访问 car 的属性值时才返回一个错误.Optional&lt;Integer&gt; optCar = Optional.of(car); Optional.ofNullable()，可接受 null 的 Optional（实现序列化的域模型时使用）。 12// 创建一个允许 null 值的 Optional 对象, 如果 car 是 null, 那么得到的 Optional 对象就是个空对象.Optional&lt;Integer&gt; optCar = Optional.ofNullable(car); Optional.empty()，所有 null 包装成的 Optional 对象。 12// 声明一个空的 OptionalOptional&lt;Integer&gt; empty = Optional.empty(); isPresent()/ifPresent(Consumer consumer)isPresent()，判断值是否存在。 123456Optional&lt;Integer&gt; optional1 = Optional.ofNullable(1);Optional&lt;Integer&gt; optional2 = Optional.ofNullable(null);// isPresent 判断值是否存在System.out.println(optional1.isPresent() == true);System.out.println(optional2.isPresent() == false); ifPresent(Consumer consumer)：如果 option 对象保存的值不是 null，则调用 consumer 对象，否则不调用。 12345678910// 如果不是 null, 调用 Consumer; 如果 null, 不调用 Consumeroptional1.ifPresent(new Consumer&lt;Integer&gt;() &#123; @Override public void accept(Integer t) &#123; System.out.println("value is" + t); &#125;&#125;);// Lambda 表达式写法optional1.ifPresent(t -&gt; System.out.println("value is" + t)); orElse(value)/orElseGet(Supplier supplier)orElse(value)：如果 optional 对象保存的值不是 null，则返回原来的值，否则返回 value。 12345Optional&lt;Integer&gt; optional1 = Optional.ofNullable(1);Optional&lt;Integer&gt; optional2 = Optional.ofNullable(null); System.out.println(optional1.orElse(1000) == 1);// trueSystem.out.println(optional2.orElse(1000) == 1000);// true orElseGet(Supplier supplier)：功能与 orElse 一样，只不过 orElseGet 参数是一个对象，即无则由函数来产生。 123456789101112Optional&lt;Integer&gt; optional1 = Optional.ofNullable(1);Optional&lt;Integer&gt; optional2 = Optional.ofNullable(null);System.out.println(optional1.orElseGet(() -&gt; &#123; return 1000;&#125;) == 1);// trueSystem.out.println(optional2.orElseGet(() -&gt; &#123; return 1000;&#125;) == 1000);// true orElseThrow()orElseThrow()：值不存在则抛出异常，存在则什么不做，有点类似 Guava 的 Precoditions。 12345Optional&lt;Integer&gt; optional = Optional.ofNullable(null);optional.orElseThrow(() -&gt; &#123; throw new NullPointerException();&#125;); filter(Predicate)filter(Predicate)：判断 Optional 对象中保存的值是否满足 Predicate，并返回新的 Optional。 123Optional&lt;Integer&gt; optional = Optional.ofNullable(1);Optional&lt;Integer&gt; filter = optional.filter((a) -&gt; a == 1); map(Function)/flatMap(Function)map(Function) 如果有值，则对其执行调用 mapping 函数得到返回值，否则返回空 Optional。如果返回值不为 null，则创建包含 mapping 返回值的 Optional 作为 map 方法返回值，否则返回空 Optional。 12Optional&lt;String&gt; upperName = name.map((value) -&gt; value.toUpperCase());System.out.println(upperName.orElse("No value found")); flatMap(Function) 如果有值，为其执行 mapping 函数返回 Optional 类型返回值，否则返回空 Optional。flatMap 与 map（Funtion）方法类似，区别在于 flatMap 中的 mapper 返回值必须是 Optional。调用结束时，flatMap 不会对结果用 Optional 封装。[P.S. 参考博文 [3]:Cascading Optional Objects Using the flatMap Method] 12upperName = name.flatMap((value) -&gt; Optional.of(value.toUpperCase()));System.out.println(upperName.orElse("No value found")); Optional 类的链式方法错误示范 / 正确示范 示范（一） 12345678910111213141516171819202122// ** 错误示范Optional&lt;User&gt; user = ...... if (user.isPresent()) &#123; return user.getOrders();&#125; else &#123; return Collections.emptyList();&#125;// 二者实质上是没有任何分别User user = .....if (user != null) &#123; return user.getOrders();&#125; else &#123; return Collections.emptyList();&#125;// ** 正确示范return user.map(u -&gt; u.getOrders()).orElse(Collections.emptyList())// map 是可能无限级联的, 比如再深一层, 获得用户名的大写形式return user.map(u -&gt; u.getUsername()) .map(name -&gt; name.toUpperCase()) .orElse(null); 示范（二） 1234567891011121314151617// ** 错误示范String version = "UNKNOWN";if(computer != null)&#123; Soundcard soundcard = computer.getSoundcard(); if(soundcard != null)&#123; USB usb = soundcard.getUSB(); if(usb != null)&#123; version = usb.getVersion(); &#125; &#125;&#125;// ** 正确示范String version = computer.map(Computer::getSoundcard) .map(Soundcard::getUSB) .map(USB::getVersion) .orElse("UNKNOWN"); 参考博文[1]. Java 8 Optional 类深度解析[2]. 使用 Java8 Optional 的正确姿势[3]. Tired of Null Pointer Exceptions? Consider Using Java SE 8’s Optional! Java8 那些事儿系列 Java8 那些事儿（一）：Stream 函数式编程 Java8 那些事儿（二）：Optional 类解决空指针异常 Java8 那些事儿（三）：Date/Time API(JSR 310) Java8 那些事儿（四）：增强的 Map 集合 Java8 那些事儿（五）：函数式接口]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java8</tag>
        <tag>Optional</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初识 Nginx（二）：安装及配置说明]]></title>
    <url>%2Farchives%2F75c4c213.html</url>
    <content type="text"><![CDATA[前言在上一篇博文中大概描述了 Nginx 的原理和功能，本篇博文主要讲述 Nginx 的安装步骤，然后介绍一下 Nginx 常用命令，最后对 Nginx 常见配置进行一些简要说明。 Linux 安装 NginxCentOS7 中使用 yum 安装 Nginx 的方法 添加源 默认情况 Centos7 中无 Nginx 的源，最近发现 Nginx 官网提供了 Centos 的源地址。因此可以如下执行命令添加源： 1$ sudo rpm -Uvh http://nginx.org/packages/centos/7/noarch/RPMS/nginx-release-centos-7-0.el7.ngx.noarch.rpm 安装 Nginx 通过 yum search nginx 看看是否已经添加源成功。如果成功则执行下列命令安装 Nginx。 12# -y 询问你 Is this OK[y/d/N], 自动选择 y$ sudo yum install -y nginx 启动 Nginx 并设置开机自动运行 12345# 启动 Nginx$ sudo systemctl start nginx.service# 设置开机自动运行$ sudo systemctl enable nginx.service 查看是否启动（多种方式） 1234567891011# 1.ps -ef$ ps -ef|grep nginx# 2.systemctl list-units 列出所有运行中单元$ systemctl list-units|grep nginx# 3.systemctl is-active 检查某个单元是否运行$ systemctl is-active nginx.service# 4. 单元状态$ systemctl status nginx Nginx 常用命令可以用 nginx -V 命令来查看配置目录和 prefix 目录，配置文件可以在编译时单独指定，也可以在启动时指定，如果没有指定配置文件，那么默认配置文件为 prefix 目录下的 conf/nginx.conf。例如，prefix 目录为“/usr/local/nginx”，没有指定配置文件，那么默认情况，nginx 的配置文件是“/usr/local/conf/nginx.conf”。 yum 或者 apt-get 软件包管理工具安装的，配置文件通常是 / etc/nginx/nginx.conf。 1234567891011121314151617181920212223# 1. 查看 nginx 命令$ nginx -h$ nginx -? # 2. 查看 nginx 版本$ nginx -v # 3. 查看配置目录和 prefix 目录以及其他配置信息$ nginx -V # 4. 校验默认的配置文件 / 校验指定配置文件[-T 参数除了校验配置文件外，还同时将完整的配置文件打印到标准输出]$ nginx -t$ nginx -t -c /path/to/configfile# 5. 启动 nginx, 后面接 “-c” 参数来指定配置文件$ nginx -c /path/to/configfile# 6. 停止和重载, 当 nginx 启动后, 可以使用 “-s” 参数向 nginx 管理进程发送信号来控制 nginx[signal 可以是以下值: stop, quit, reopen, reload]$ nginx -s signal$ nginx -s stop // 快速关闭$ nginx -s quit // 安全关闭$ nginx -s reload // 重载配置文件$ nginx -s reopen // 重新打开一个 log 文件, 用于日志切割[删除或者移动日志文件时] 常见配置说明配置文件区域说明 Nginx 配置文件 nginx.conf 中文详解123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139# 定义 Nginx 运行的用户和用户组user www www;# nginx 进程数, 建议设置为等于 CPU 总核心数.worker_processes 8;# 全局错误日志定义类型,[ debug | info | notice | warn | error | crit ]error_log /var/log/nginx/error.log info;# 进程文件pid /var/run/nginx.pid;# 一个 nginx 进程打开的最多文件描述符数目, 理论值应该是最多打开文件数（系统的值 ulimit -n）与 nginx 进程数相除, 但是 nginx 分配请求并不均匀, 所以建议与 ulimit -n 的值保持一致.worker_rlimit_nofile 65535;# 工作模式与连接数上限events &#123; # 参考事件模型, use [ kqueue | rtsig | epoll | /dev/poll | select | poll ]; epoll 模型是 Linux 2.6 以上版本内核中的高性能网络 I/O 模型, 如果跑在 FreeBSD 上面, 就用 kqueue 模型. use epoll; # 单个进程最大连接数（最大连接数 = 连接数 * 进程数） worker_connections 65535;&#125;# 设定 http 服务器http &#123; include mime.types; # 文件扩展名与文件类型映射表 default_type application/octet-stream; # 默认文件类型 # charset utf-8; #默认编码 server_names_hash_bucket_size 128; # 服务器名字的 hash 表大小 client_header_buffer_size 32k; # 上传文件大小限制 large_client_header_buffers 4 64k; # 设定请求缓 client_max_body_size 8m; # 设定请求缓 sendfile on; # 开启高效文件传输模式, sendfile 指令指定 nginx 是否调用 sendfile 函数来输出文件, 对于普通应用设为 on, 如果用来进行下载等应用磁盘 IO 重负载应用, 可设置为 off, 以平衡磁盘与网络 I/O 处理速度, 降低系统的负载. 注意: 如果图片显示不正常把这个改 成 off. autoindex on; # 开启目录列表访问, 合适下载服务器, 默认关闭. tcp_nopush on; # 防止网络阻塞 tcp_nodelay on; # 防止网络阻塞 keepalive_timeout 120; # 长连接超时时间, 单位是秒 # FastCGI 相关参数是为了改善网站的性能: 减少资源占用, 提高访问速度. 下面参数看字面意思都能理解. fastcgi_connect_timeout 300; fastcgi_send_timeout 300; fastcgi_read_timeout 300; fastcgi_buffer_size 64k; fastcgi_buffers 4 64k; fastcgi_busy_buffers_size 128k; fastcgi_temp_file_write_size 128k; # gzip 模块设置 gzip on; # 开启 gzip 压缩输出 gzip_min_length 1k; # 最小压缩文件大小 gzip_buffers 4 16k; # 压缩缓冲区 gzip_http_version 1.0; # 压缩版本（默认 1.1, 前端如果是 squid2.5 请使用 1.0） gzip_comp_level 2; # 压缩等级 gzip_types text/plain application/x-javascript text/css application/xml; # 压缩类型, 默认就已经包含 text/html, 所以下面就不用再写了, 写上去也不会有问题, 但是会有一个 warn. gzip_vary on; # limit_zone crawler $binary_remote_addr 10m; #开启限制 IP 连接数的时候需要使用 upstream blog.ha97.com &#123; # upstream 的负载均衡, weight 是权重, 可以根据机器配置定义权重. weigth 参数表示权值, 权值越高被分配到的几率越大. server 192.168.80.121:80 weight=3; server 192.168.80.122:80 weight=2; server 192.168.80.123:80 weight=3; &#125; # 虚拟主机的配置 server &#123; # 监听端口 listen 80; # 域名可以有多个, 用空格隔开 server_name www.ha97.com ha97.com; index index.html index.htm index.php; root /data/www/ha97; location ~ .*.(php|php5)?$ &#123; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; include fastcgi.conf; &#125; # 图片缓存时间设置 location ~ .*.(gif|jpg|jpeg|png|bmp|swf)$ &#123; expires 10d; &#125; # JS 和 CSS 缓存时间设置 location ~ .*.(js|css)?$ &#123; expires 1h; &#125; # 日志格式设定 log_format access ‘$remote_addr – $remote_user [$time_local] “$request” ‘ ‘$status $body_bytes_sent “$http_referer” ‘ ‘”$http_user_agent” $http_x_forwarded_for’; # 定义本虚拟主机的访问日志 access_log /var/log/nginx/ha97access.log access; # 对 “/” 启用反向代理 location / &#123; proxy_pass http://127.0.0.1:88; proxy_redirect off; proxy_set_header X-Real-IP $remote_addr; # 后端的 Web 服务器可以通过 X-Forwarded-For 获取用户真实 IP proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # 以下是一些反向代理的配置, 可选. proxy_set_header Host $host; client_max_body_size 10m; # 允许客户端请求的最大单文件字节数 client_body_buffer_size 128k; # 缓冲区代理缓冲用户端请求的最大字节数, proxy_connect_timeout 90; # nginx 跟后端服务器连接超时时间(代理连接超时) proxy_send_timeout 90; # 后端服务器数据回传时间(代理发送超时) proxy_read_timeout 90; # 连接成功后, 后端服务器响应时间(代理接收超时) proxy_buffer_size 4k; # 设置代理服务器（nginx）保存用户头信息的缓冲区大小 proxy_buffers 4 32k; # proxy_buffers 缓冲区, 网页平均在 32k 以下的设置 proxy_busy_buffers_size 64k; # 高负荷下缓冲大小（proxy_buffers*2） proxy_temp_file_write_size 64k; # 设定缓存文件夹大小, 大于这个值, 将从 upstream 服务器传 &#125; # 设定查看 Nginx 状态的地址 location /NginxStatus &#123; stub_status on; access_log on; auth_basic “NginxStatus”; auth_basic_user_file conf/htpasswd; # htpasswd 文件的内容可以用 apache 提供的 htpasswd 工具来产生. &#125; # 本地动静分离反向代理配置 # 所有 jsp 的页面均交由 tomcat 或 resin 处理 location ~ .(jsp|jspx|do)?$ &#123; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://127.0.0.1:8080; &#125; # 所有静态文件由 nginx 直接读取不经过 tomcat 或 resin location ~ .*.(htm|html|gif|jpg|jpeg|png|bmp|swf|ioc|rar|zip|txt|flv|mid|doc|ppt|pdf|xls|mp3|wma)$ &#123; expires 15d; &#125; location ~ .*.(js|css)?$ &#123; expires 1h; &#125; &#125;&#125; 参考博文[1]. nginx（一） nginx 详解[2]. Nginx reopen reload 作用及工作过程[3]. Nginx 实用教程（一）：启动、停止、重载配置[4]. Nginx 实用教程（二）：配置文件入门 初识 Nginx 系列 初识 Nginx（一）：理解原理和功能 初识 Nginx（二）：安装及配置说明]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>安装教程</tag>
        <tag>配置说明</tag>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论编码重要性（二）：你所不了解的编码解码技术]]></title>
    <url>%2Farchives%2Ff54a7b9d.html</url>
    <content type="text"><![CDATA[前言在我的工作中，使用爬虫过程中常常会遇到各种各样的编码解码技术，对于网站返回的响应内容，常常需要一眼就要知道该网站用的什么编码技术？需要用什么解码技术进行解码？请求体内容使用了哪种编码技术？本篇是我了解编码系列的后续，主要内容讲述爬虫工作中常见的编码技术。 常见的编码技术Unicode 编码Unicode 在《论编码重要性（一）：你所不了解的字符编码》已经进行了详细说明，此处不再进行说明。 以 “Unicode 编码” 为例，说明 Unicode 编码后的内容形式。 12345// 原文Unicode 编码// Unicode 编码后\u0055\u006e\u0069\u0063\u006f\u0064\u0065\u0020\u7f16\u7801 UTF-8 编码UTF-8 在《论编码重要性（一）：你所不了解的字符编码》已经进行了详细说明，此处不再进行说明。 以 “UTF-8 编码” 为例，说明 UTF-8 编码后的内容形式。 12345// 原文UTF-8 编码// UTF-8 编码后UTF-8 &amp;#x7F16;&amp;#x7801; UrlEncode 编码Url 编码通常也被称为百分号编码（Url Encoding，also known as percent-encoding），是因为它的编码方式非常简单，使用 % 百分号加上两位的字符——0123456789ABCDEF——代表一个字节的十六进制形式。Url 编码默认使用的字符集是 US-ASCII。 对于非 ASCII 字符，需要使用 ASCII 字符集的超集进行编码得到相应的字节，然后对每个字节执行百分号编码。对于 Unicode 字符，RFC 文档建议使用 utf-8 对其进行编码得到相应的字节，然后对每个字节执行百分号编码。如 “中文” 使用 UTF-8 字符集得到的字节为 0xE4 0xB8 0xAD 0xE6 0x96 0x87，经过 Url 编码之后得到 “%E4%B8%AD%E6%96%87”。 例如，Url 参数字符串中使用 key=value 键值对这样的形式来传参，键值对之间以 &amp; 符号分隔，如 / s?q=abc&amp; ie=utf-8。如果你的 value 字符串中包含了 = 或者 &amp;，那么势必会造成接收 Url 的服务器解析错误，因此必须将引起歧义的 &amp; 和 = 符号进行转义，也就是对其进行编码。 1234567// 原文http://www.baidu.com/s?ie=utf-8&amp;f=8&amp;tn=baidu&amp;wd = 临时邮箱// UrlEncode 编码后http://www.baidu.com/s?ie=utf-8&amp;f=8&amp;tn=baidu&amp;wd=%C1%D9%CA%B1%D3%CA%CF%E4// 例如 a 在 US-ASCII 码中对应的字节是 0x61，那么 Url 编码之后得到的就是 %61，我们在地址栏上输入 http://g.cn/search?q=%61%62%63，实际上就等同于在 google 上搜索 abc 了。又如 @符号在 ASCII 字符集中对应的字节为 0x40，经过 Url 编码之后得到的是 %40。 Base64 编码Base64 是网络上最常见的用于传输 8Bit 字节码的编码方式之一，Base64 就是一种基于 64 个可打印字符来表示二进制数据的方法。这 64 个可打印字符 a-z,A-Z,0-9 就占 62 字符，剩下 2 个字符不同系统可能使用不同，经常是:“+/”。 12345// 原文Base64 是网络上最常见的用于传输 8Bit 字节码的编码方式之一// Base64 编码QmFzZTY0ysfN+MLnyc/X7rOjvPu1xNPD09q0q8rkOEJpdNfWvdrC67XEseDC67e9yr3WrtK7 Java 代码实现： 123456789import java.util.Base64;对于标准的 Base64：加密为字符串使用Base64.getEncoder().encodeToString();加密为字节数组使用Base64.getEncoder().encode();解密使用Base64.getDecoder().decode();对于 URL 安全或 MIME 的 Base64，只需将上述 getEncoder()getDecoder() 更换为 getUrlEncoder()getUrlDecoder() 或 getMimeEncoder() 和 getMimeDecoder() 即可。 Hex 编码Hex 编码就是把一个 8 位的字节数据用两个十六进制数展示出来，编码时，将 8 位二进制码重新分组成两个 4 位的字节，其中一个字节的低 4 位是原字节的高四位，另一个字节的低 4 位是原数据的低 4 位，高 4 位都补 0，然后输出这两个字节对应十六进制数字作为编码。比如 ASCII 码 A 的 Hex 编码过程为： 12345ASCII 码：A (65)二进制码：0100_0001重新分组：0000_0100 0000_0001十六进制： 4 1Hex 编码：41 参考博文[1].Web 开发须知：URL 编码与解码[2].1-Hex 编码 论编码重要性系列 论编码重要性（一）：你所不了解的字符编码 论编码重要性（二）：你所不了解的编码解码技术]]></content>
      <categories>
        <category>编码</category>
      </categories>
      <tags>
        <tag>编码</tag>
        <tag>编码解码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 并发编程之美（一）：并发队列 Queue 原理剖析]]></title>
    <url>%2Farchives%2Fa290319j.html</url>
    <content type="text"><![CDATA[前言并发编程是 Java 程序员最重要的技能之一，也是最难掌握的一种技能。而在本人的工作中也经常会接触到并发编程的情况，队列、线程池、线程本地变量等等在本人的工作项目中都有大量的应用，为了更深入的理解并发编程，了解其运行原理，本人决定对工作中接触到的并发编程技术进行进一步的梳理。 本篇是我学习 Java 并发编程系列的开篇，主要内容介绍一下队列 Queue 概念以及方法，然后对工作中常用的几个队列进行分析，介绍其主要使用场景。 什么是队列？队列是一种特殊的线性表，其插入和删除的操作分别在表的两端进行，队列的特点就是先进先出 (First In First Out)。但这并不是必须的，比如优先度队列就是一个例外，它是以元素的值来排序。我们把向队列中插入元素的过程称为入队 (Enqueue)，删除元素的过程称为出队 (Dequeue)；并把允许入队的一端称为队尾，允许出队一端称为队头，没有任何元素的队列则称为空队列。通常，队列不允许随机访问队列中的元素。 在 Java5 中新增加了 java.util.Queue 接口，用以支持队列的常见操作。该接口扩展了 java.util.Collection 接口。Queue 接口与 List、Set 同一级别，都是继承了 Collection 接口。 Queue 接口中定义了如下的几个方法： void add(Object e): 将指定元素插入到队列的尾部。如果队列已满，则抛出一个 IllegalStateException 异常。 boolean offer(Object e): 将指定的元素插入此队列的尾部。当使用容量有限的队列时，此方法通常比 add(Object e) 有效，如果队列已满，则返回 false。 object element(): 获取队列头部的元素，但是不删除该元素。如果队列为空，则抛出一个 NoSuchElementException 异常。 Object peek(): 返回队列头部的元素，但是不删除该元素。如果队列为空，则返回 null。 Object remove(): 获取队列头部的元素，并删除该元素。如果队列为空，则抛出一个 NoSuchElementException 异常。 Object poll(): 返回队列头部的元素，并删除该元素。如果队列为空，则返回 null。 通用队列 QueueQueue 接口有一个 PriorityQueue 实现类。除此之外，Queue 还有一个 Deque 接口，Deque 代表一个 “双端队列”，双端队列可以同时从两端删除或添加元素，因此 Deque 可以当作栈来使用。java 为 Deque 提供了 ArrayDeque 实现类和 LinkedList 实现类。 PriorityQueuePriorityQueue 是一个基于堆结构的优先级队列，它可以根据元素的自然排序或者给定的排序器进行排序。PriorityQueue 是一种比较标准的队列实现类，而不是绝对标准的。这是因为 PriorityQueue 保存队列元素的顺序不是按照元素添加的顺序来保存的，而是在添加元素的时候对元素的大小排序后再保存的。因此在 PriorityQueue 中使用 peek() 或 pool() 取出队列中头部的元素，取出的不是最先添加的元素，而是最小的元素。 1234567PriorityQueue priorityQueue = new PriorityQueue();priorityQueue.offer(6);priorityQueue.add(-3);priorityQueue.add(20);priorityQueue.offer(18);// 输出：[-3, 6, 20, 18]System.out.println(priorityQueue); PriorityQueue 不允许插入 null 元素，它还需要对队列元素进行排序，PriorityQueue 有两种排序方式：[1]. 自然排序：采用自然排序的 PriorityQueue 集合中的元素必须实现 Compareable 接口，重写 CompareTo 方法，而且应该是一个类的多个实例，否则可能导致 ClassCastException 异常。[2]. 定制排序：创建 PriorityQueue 队列时，传入一个 Comparator 对象，该对象负责对所有队列中的所有元素进行排序。采用定制排序不要求必须实现 Compareable 接口。 总结：[1].Jdk 内置的优先队列 PriorityQueue 内部使用一个堆维护数据，每当有数据 add 进来或者 poll 出去的时候会对堆做从下往上的调整和从上往下的调整。[2].PriorityQueue 不是一个线程安全的类，如果要在多线程环境下使用，可以使用 PriorityBlockingQueue 这个优先阻塞队列。其中 add、poll、remove 方法都使用 ReentrantLock 锁来保持同步，take() 方法中如果元素为空，则会一直保持阻塞。 Deque/ArrayDequeDeQueue(Double-endedqueue) 为接口，继承了 Queue 接口，创建双向队列，灵活性更强，可以前向或后向迭代，在队头队尾均可心插入或删除元素。它的两个主要实现类是 ArrayDeque 和 LinkedList。 Deque 接口继承自 Queue 接口，但 Deque 支持同时从两端添加或移除元素，因此又被成为双端队列。鉴于此，Deque 接口的实现可以被当作 FIFO 队列使用，也可以当作 LIFO 队列（栈）来使用。官方也是推荐使用 Deque 的实现来替代 Stack。 ArrayDeque 是 Deque 接口的一种具体实现，是依赖于可变数组来实现的。ArrayDeque 没有容量限制，可根据需求自动进行扩容。ArrayDeque 不支持值为 null 的元素。 Deque 接口是 Queue 接口的子接口，它代表一个双端队列，Deque 定义了一些方法： void addFirst(Object e): 将指定元素添加到双端队列的头部。 void addLast(Object e): 将指定元素添加到双端队列的尾部。 Iteratord descendingItrator(): 返回该双端队列对应的迭代器，该迭代器以逆向顺序来迭代队列中的元素。 Object getFirst(): 获取但不删除双端队列的第一个元素。 Object getLast(): 获取但不删除双端队列的最后一个元素。 boolean offFirst(Object e): 将指定元素添加到双端队列的头部。 boolean offLast(OBject e): 将指定元素添加到双端队列的尾部。 Object peekFirst(): 获取但不删除双端队列的第一个元素；如果双端队列为空，则返回 null。 Object peekLast(): 获取但不删除双端队列的最后一个元素；如果双端队列为空，则返回 null。 Object pollFirst(): 获取并删除双端队列的第一个元素；如果双端队列为空，则返回 null。 Object pollLast(): 获取并删除双端队列的最后一个元素；如果双端队列为空，则返回 null。 Object pop()(栈方法): pop 出该双端队列所表示的栈的栈顶元素。相当于 removeFirst()。 void push(Object e)(栈方法)： 将一个元素 push 进该双端队列所表示的栈的栈顶。相当于 addFirst()。 Object removeFirst(): 获取并删除该双端队列的第一个元素。 Object removeFirstOccurence(Object o): 删除该双端队列的第一次出现的元素 o。 Object removeLast(): 获取并删除该双端队列的最后一个元素 o。 Object removeLastOccurence(Object o): 删除该双端队列的最后一次出现的元素 o。 LinkedListLinkedList 是 List 接口的实现类，因此它可以是一个集合，可以根据索引来随机访问集合中的元素。此外，它还是 Duque 接口的实现类，因此也可以作为一个双端队列，或者栈来使用。 LinkedList 与 ArrayList、ArrayDeque 的实现机制完全不同，ArrayList 和 ArrayDeque 内部以数组的形式来保存集合中的元素，因此随机访问集合元素时有较好的性能；而 LinkedList 以链表的形式来保存集合中的元素，因此随机访问集合元素时性能较差，但是插入和删除元素时性能比较出色（只需改变指针所指的地址即可），需要指出的是，虽然 Vector 也是以数组的形式来存储集合但因为它实现了线程同步（而且实现的机制不好），故各方面的性能都比较差。 并发队列 QueueJDK 中常用的并发队列有阻塞队列和非阻塞队列，使用阻塞算法的队列可以用一个锁（入队和出队用同一把锁）或两个锁（入队和出队用不同的锁）等方式来实现，而非阻塞的实现方式则可以使用循环 CAS 的方式来实现。其中阻塞队列的典型例子是 BlockingQueue，非阻塞队列的典型例子是 ConcurrentLinkedQueue，在实际应用中要根据实际需要选用阻塞队列或者非阻塞队列。 ConcurrentLinkedQueueConcurrentLinkedQueue 是一个适用于高并发场景下的队列，通过无锁的方式，实现了高并发状态下的高性能，通常 ConcurrentLinkedQueue 性能好于 BlockingQueue，它是一个基于链接节点的无界线程安全队列，它采用的是先进先出的规则，当我们增加一个元素时，它会添加到队列的末尾，当我们取一个元素时，它会返回一个队列头部的元素。收集关于队列大小的信息会很慢，需要遍历队列。 BlockingQueue 阻塞队列（BlockingQueue）是一个支持两个附加操作的队列。这两个附加的操作是：在队列为空时，获取元素的线程会等待队列变为非空。当队列满时，存储元素的线程会等待队列可用。阻塞队列常用于生产者和消费者的场景，生产者是往队列里添加元素的线程，消费者是从队列里拿元素的线程。阻塞队列就是生产者存放元素的容器，而消费者也只从容器里拿元素。 抛出异常：是指当阻塞队列满时候，再往队列里插入元素，会抛出 IllegalStateException(“Queue full”) 异常。当队列为空时，从队列里获取元素时会抛出 NoSuchElementException 异常。 返回特殊值：插入方法会返回是否成功，成功则返回 true。移除方法，则是从队列里拿出一个元素，如果没有则返回 null。 一直阻塞：当阻塞队列满时，如果生产者线程往队列里 put 元素，队列会一直阻塞生产者线程，直到拿到数据，或者响应中断退出。当队列空时，消费者线程试图从队列里 take 元素，队列也会阻塞消费者线程，直到队列可用。 超时退出：当阻塞队列满时，队列会阻塞生产者线程一段时间，如果超过一定的时间，生产者线程就会退出。 JDK7 提供了 7 个阻塞队列，分别是： ArrayBlockingQueue ：一个由数组结构组成的有界阻塞队列。LinkedBlockingQueue ：一个由链表结构组成的可选有界阻塞队列。PriorityBlockingQueue ：一个支持优先级排序的无界阻塞队列。DelayQueue：一个使用优先级队列实现的无界阻塞队列。SynchronousQueue：一个不存储元素的阻塞队列。LinkedTransferQueue：一个由链表结构组成的无界阻塞队列。LinkedBlockingDeque：一个由链表结构组成的双向阻塞队列。 ArrayBlockingQueue基于数组的阻塞队列实现（有界缓存的等待队列），初始化时必须指定队列的容量。在 ArrayBlockingQueue 内部，维护了一个定长数组，以便缓存队列中的数据对象，这是一个常用的阻塞队列，除了一个定长数组外，ArrayBlockingQueue 内部还保存着两个整形变量，分别标识着队列的头部和尾部在数组中的位置。 ArrayBlockingQueue 在生产者放入数据和消费者获取数据，都是共用同一个锁对象，由此也意味着两者无法真正并行运行，这点尤其不同于 LinkedBlockingQueue。在创建 ArrayBlockingQueue 时，我们还可以设置内部的 ReentrantLock 是否使用公平锁，但是公平性会使你在性能上付出代价，只有在的确非常需要的时候再使用它，默认采用非公平锁。 ArrayBlockingQueue 和 LinkedBlockingQueue 间还有一个明显的不同之处在于，前者在插入或删除元素时不会产生或销毁任何额外的对象实例，而后者则会生成一个额外的 Node 对象。这在长时间内需要高效并发地处理大批量数据的系统中，其对于 GC 的影响还是存在一定的区别。 LinkedBlockingQueue基于链表的阻塞队列实现（可选有界缓存的等待队列），初始化时不需要指定队列的容量，默认是 Integer.MAX_VALUE，也可以看成容量无限大。同 ArrayBlockingQueue 类似，其内部也维持着一个数据缓冲队列（该队列由一个链表构成），当生产者往队列中放入一个数据时，队列会从生产者手中获取数据，并缓存在队列内部，而生产者立即返回；只有当队列缓冲区达到最大值缓存容量时（LinkedBlockingQueue 可以通过构造函数指定该值），才会阻塞生产者线程，直到消费者从队列中消费掉一份数据，生产者线程会被唤醒，反之对于消费者这端的处理也基于同样的原理。 而 LinkedBlockingQueue 之所以能够高效的处理并发数据，还因为其对于生产者端和消费者端分别采用了独立的锁来控制数据同步，这也意味着在高并发的情况下生产者和消费者可以并行地操作队列中的数据，以此来提高整个队列的并发性能。 作为开发者，我们需要注意的是，如果构造一个 LinkedBlockingQueue 对象，而没有指定其容量大小，LinkedBlockingQueue 会默认一个类似无限大小的容量（Integer.MAX_VALUE），这样的话，如果生产者的速度一旦大于消费者的速度，也许还没有等到队列满阻塞产生，系统内存就有可能已被消耗殆尽了。 PriorityBlockingQueue基于优先级堆的阻塞队列实现（无界缓存的优先级等待队列），初始化时不需要指定队列的容量，默认是 DEFAULT_INITIAL_CAPACITY=11，最大可分配队列容量 Integer.MAX_VALUE-8。PriorityBlockingQueue 是对 PriorityQueue 的再次包装，队列中的元素按优先级顺序被移除，在实现 PriorityBlockingQueue 时，内部控制线程同步的锁采用的是公平锁。 PriorityBlockingQueue 该类不保证同等优先级的元素顺序，如果你想要强制顺序，就需要考虑自定义顺序或者是 Comparator 使用第二个比较属性。 DelayQueue基于优先级堆支持的、基于时间的阻塞队列实现（无界缓存的优先级等待队列）。DelayQueue 是一个支持延时获取元素的无界阻塞队列，队列使用 PriorityQueue 来实现，队列中的元素必须实现 Delayed 接口，队列中存放 Delayed 元素，只有在延迟期满后才能从队列中提取元素。当一个元素的 getDelay() 方法返回值小于等于 0 时才能从队列中 poll 元素，否则 poll() 方法会返回 null。 DelayQueue 是一个没有大小限制的队列，应用场景很多，比如对缓存超时的数据进行移除、任务超时处理、空闲连接的关闭等等： 缓存系统的设计：使用 DelayQueue 保存缓存元素的有效期，使用一个线程循环查询 DelayQueue，一旦能从 DelayQueue 中获取元素时，就表示有缓存到期了。 定时任务调度：使用 DelayQueue 保存要执行的任务和执行时间，一旦从 DelayQueue 中获取到任务就开始执行，比如 Timer 就是使用 DelayQueue 实现的。 Delayed 接口使对象成为延迟对象，它使存放在 DelayQueue 类中的对象具有了激活日期。该接口强制实现下列两个方法： CompareTo(Delayed o)：Delayed 接口继承了 Comparable 接口，因此有了这个方法。 getDelay(TimeUnit unit)：这个方法返回到激活日期的剩余时间，时间单位由单位参数指定。 DelayQueue 中比较重要的字段如下： 12345678// 全局独占锁, 用于实现线程安全 [可重入锁]private final transient ReentrantLock lock = new ReentrantLock();// 根据 delay 时间排序优先级队列, 用于存储元素, 并按优先级排序private final PriorityQueue&lt;E&gt; q = new PriorityQueue&lt;E&gt;();// 用于优化阻塞通知的线程元素 leaderprivate Thread leader = null;// 用于实现阻塞和通知的 Condition 对象private final Condition available = lock.newCondition(); DelayQueue 的大致实现思路：以支持优先级的 PriorityQueue 无界队列作为一个容器，因为元素都必须实现 Delayed 接口，可以根据元素的过期时间来对元素进行排列，因此，先过期的元素会在队首，每次从队列里取出来都是最先要过期的元素。 入队由于 DelayQueue 不限制长度，添加元素的时候不会因为队列已满产生阻塞，因此 add(E e) 方法以及 put(E e) 方法通过调用 offer(E e) 方法来添加元素。 12345678910111213141516171819public boolean offer(E e) &#123; // 1. 获取全局独占锁 final ReentrantLock lock = this.lock; // 2. 执行加锁操作 lock.lock(); try &#123; // 3. 向优先队列中插入元素 q.offer(e); // 4. 如果队首元素是刚插入的元素, 则设置 leader 为 null, 并唤醒阻塞在 available 上的线程 if (q.peek() == e) &#123; leader = null; available.signal(); // 激活 available 变量条件队列里面的线程 &#125; return true; &#125; finally &#123; // 5. 释放全局独占锁 lock.unlock(); &#125;&#125; 出队1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public E take() throws InterruptedException &#123; // 1. 获取全局独占锁 final ReentrantLock lock = this.lock; // 2. 执行加锁操作 lock.lockInterruptibly(); try &#123; // 死循环 [] for (;;) &#123; // 3. 获取优先队列中队首元素 E first = q.peek(); // 4. 队首为空，则阻塞当前线程 if (first == null) available.await(); else &#123; // 5. 获取队首元素的超时时间 long delay = first.getDelay(NANOSECONDS); // 6. 已超时, 调用 poll() 方法弹出该元素, 跳出方法 if (delay &lt;= 0) return q.poll(); // 7. 释放 first 的引用, 避免内存泄漏 first = null; // don't retain ref while waiting // 8.leader != null 表明有其他线程在操作, 阻塞当前线程 if (leader != null) available.await(); else &#123; // 9.leader 指向当前线程 Thread thisThread = Thread.currentThread(); leader = thisThread; try &#123; // 10. 超时阻塞 available.awaitNanos(delay); &#125; finally &#123; // 11. 释放 leader if (leader == thisThread) leader = null; &#125; &#125; &#125; &#125; &#125; finally &#123; // 12.leader 为 null 并且队列不为空, 说明没有其他线程在等待, 那就通知条件队列 if (leader == null &amp;&amp; q.peek() != null) available.signal(); // 13. 释放全局独占锁 lock.unlock(); &#125;&#125; leader 元素的使用：leader 是等待获取队列头元素的线程，应用主从式设计减少不必要的等待。如果 leader 不等于空，表示已经有线程在等待获取队列的头元素。所以，使用 await() 方法让当前线程等待信号。如果 leader 等于空，则把当前线程设置成 leader，并使用 awaitNanos() 方法让当前线程等待接收信号或等待 delay 时间。 为什么如果不设置 first=null，则会引起内存泄漏呢？线程 A 到达，列首元素没有到期，设置 leader = 线程 A，这是线程 B 来了因为 leader!=null，则会阻塞，线程 C 一样。假如线程阻塞完毕了，获取列首元素成功，出列。这个时候列首元素应该会被回收掉，但是问题是它还被线程 B、线程 C 持有着，所以不会回收，这里只有两个线程，如果有线程 D、线程 E 等呢？这样会无限期的不能回收，就会造成内存泄漏。 SynchronousQueueSynchronousQueue 是一种无缓冲的等待队列，但是由于该 Queue 本身的特性，在某次添加元素后必须等待其他线程取走后才能继续添加；SynchronousQueue 是一个不存储元素的阻塞队列，会直接将任务交给消费者，必须等队列中的添加元素被消费后才能继续添加新的元素，isEmpty() 方法永远返回是 true，remainingCapacity() 方法永远返回是 0，remove() 和 removeAll() 方法永远返回是 false，iterator() 方法永远返回空，peek() 方法永远返回 null。 声明一个 SynchronousQueue 有两种不同的方式，它们之间有着不太一样的行为。SynchronousQueue 有一个 fair 选项，如果 fair 为 true，称为 fair 模式，否则就是 unfair 模式，公平性使用 TransferQueue，非公平性采用 TransferStack。在 fair 模式下，所有等待的生产者线程或者消费者线程会按照开始等待时间依次排队，然后按照等待先后顺序进行匹配交易。由于 SynchronousQueue 支持公平策略和非公平策略，所以 SynchronousQueue 的底层实现包含两种数据结构：队列（实现公平策略）和栈（实现非公平策略），队列与栈都是通过链表来实现的。 公平策略和非公平策略的区别：如果采用公平策略，SynchronousQueue 会采用公平锁，并配合一个 FIFO（Queue）队列来阻塞多余的生产者和消费者，从而体系整体的公平策略；如果采用非公平策略（SynchronousQueue 默认），SynchronousQueue 采用非公平锁，同时配合一个 LIFO（Stack）队列来管理多余的生产者和消费者，而后一种模式，如果生产者和消费者的处理速度有差距，则很容易出现饥渴的情况，即可能有某些生产者或者是消费者的数据永远都得不到处理。一般情况下，FIFO 通常可以支持更大的吞吐量，但 LIFO 可以更大程度的保持线程的本地化。 SynchronousQueue 的一个使用场景是在线程池里。Executors.newCachedThreadPool() 就使用了 SynchronousQueue，这个线程池根据需要（新任务到来时）创建新的线程，如果有空闲线程则会重复使用，线程空闲了 60 秒后会被回收。 LinkedTransferQueueLinkedTransferQueue 是 JDK1.7 才添加的阻塞队列，基于链表实现的 FIFO 无界阻塞队列，是 ConcurrentLinkedQueue（循环 CAS+volatile 实现的 wait-free 并发算法）、SynchronousQueue（公平模式下转交元素）、LinkedBlockingQueue（阻塞 Queue 的基本方法）的超集。而且 LinkedTransferQueue 更好用，因为它不仅仅综合了这几个类的功能，同时也提供了更高效的实现。 LinkedTransferQueue 采用的一种预占模式。意思就是消费者线程取元素时，如果队列为空，那就生成一个节点（节点元素为 null）入队，然后消费者线程被等待在这个节点上，后面生产者线程入队时发现有一个元素为 null 的节点，生产者线程就不入队了，直接就将元素填充到该节点，唤醒该节点等待的线程，被唤醒的消费者线程取走元素，从调用的方法返回。 LinkedTransferQueue 类继承自 AbstractQueue 抽象类，并且实现了 TransferQueue 接口： 12345678910111213141516public interface TransferQueue&lt;E&gt; extends BlockingQueue&lt;E&gt; &#123; // 如果存在一个消费者已经等待接收它, 则立即传送指定的元素, 否则返回 false, 并且不进入队列. boolean tryTransfer(E e); // 如果存在一个消费者已经等待接收它, 则立即传送指定的元素, 否则等待直到元素被消费者接收. void transfer(E e) throws InterruptedException; // 在上述方法的基础上设置超时时间 boolean tryTransfer(E e, long timeout, TimeUnit unit) throws InterruptedException; // 如果至少有一位消费者在等待, 则返回 true boolean hasWaitingConsumer(); // 获取所有等待获取元素的消费线程数量 int getWaitingConsumerCount();&#125; 我们知道，在普通阻塞队列中，当队列为空时，消费者线程（调用 take 或 poll 方法的线程）一般会阻塞等待生产者线程往队列中存入元素。而 LinkedTransferQueue 的 transfer 方法则比较特殊：1、当有消费者线程阻塞等待时，调用 transfer 方法的生产者线程不会将元素存入队列，而是直接将元素传递给消费者；2、如果调用 transfer 方法的生产者线程发现没有正在等待的消费者线程，则会将元素入队，然后会阻塞等待，直到有一个消费者线程来获取该元素。 和 SynchronousQueue 相比，LinkedTransferQueue 多了一个可以存储的队列，与 LinkedBlockingQueue 相比，LinkedTransferQueue 多了直接传递元素，少了用锁来同步。 LinkedBlockingDequeLinkedBlockingDeque 是双向链表实现的双向并发阻塞队列。该阻塞队列同时支持 FIFO 和 FILO 两种操作方式，即可以从队列的头和尾同时操作 (插入 / 删除)；并且，该阻塞队列是支持线程安全。此外，LinkedBlockingDeque 还是可选容量的 (防止过度膨胀)，即可以指定队列的容量。如果不指定，默认容量大小等于 Integer.MAX_VALUE。 四组不同的行为方式解释： 抛异常：如果试图的操作无法立即执行，抛一个异常。 特定值：如果试图的操作无法立即执行，返回一个特定的值 (常常是 true/false)。 阻塞：如果试图的操作无法立即执行，该方法调用将会发生阻塞，直到能够执行。 超时：如果试图的操作无法立即执行，该方法调用将会发生阻塞，直到能够执行，但等待时间不会超过给定值。返回一个特定值以告知该操作是否成功 (典型的是 true/false)。 小知识点[1]. Queue 的实现通常不允许插入 null 值，除了 LinkedList 这个例外，出于历史原因它允许插入 null 值，但是必须十分注意的是 null 也是 poll 和 peek 方法返回的特别值。[2]. 虽然 ConcurrentLinkedQueue 的性能很好，但是在调用 size() 方法的时候，会遍历一遍集合，对性能损害较大，执行很慢，因此应该尽量的减少使用这个方法，如果判断是否为空，最好用 isEmpty() 方法。 参考博文[1]. 并发队列 - 无界阻塞延迟队列 DelayQueue 原理探究 Java 并发编程之美系列 Java 并发编程之美（一）：并发队列 Queue 原理剖析 Java 并发编程之美（二）：线程池 ThreadPoolExecutor 原理探究 Java 并发编程之美（三）：异步执行框架 Eexecutor Java 并发编程之美（四）：深入剖析 ThreadLocal Java 并发编程之美（五）：揭开 InheritableThreadLocal 的面纱 Java 并发编程之美（六）：J.U.C 之线程同步辅助工具类]]></content>
      <categories>
        <category>并发</category>
      </categories>
      <tags>
        <tag>Queue</tag>
        <tag>BlockingQueue</tag>
        <tag>阻塞队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java8 那些事儿（一）：Stream 函数式编程]]></title>
    <url>%2Farchives%2F8cef11db.html</url>
    <content type="text"><![CDATA[前言Java8(又称为 Jdk1.8)是 Java 语言开发的一个主要版本。Oracle 公司于 2014 年 3 月 18 日发布 Java8，它支持函数式编程，新的 JavaScript 引擎，新的日期 API，新的 Stream API 等。Java8 API 添加了一个新的抽象称为流 Stream，可以让你以一种声明的方式处理数据。Stream API 可以极大提高 Java 程序员的生产力，让程序员写出高效率、干净、简洁的代码。 Java8 新特性 Lambda 表达式 − Lambda 允许把函数作为一个方法的参数（函数作为参数传递进方法中。 方法引用 − 方法引用提供了非常有用的语法，可以直接引用已有 Java 类或对象（实例）的方法或构造器。与 lambda 联合使用，方法引用可以使语言的构造更紧凑简洁，减少冗余代码。 默认方法 − 默认方法就是一个在接口里面有了一个实现的方法。 新工具 − 新的编译工具，如：Nashorn 引擎 jjs、类依赖分析器 jdeps。 Stream API − 新添加的 Stream API（java.util.stream）把真正的函数式编程风格引入到 Java 中。 Date Time API − 加强对日期与时间的处理。 Optional 类 − Optional 类已经成为 Java8 类库的一部分，用来解决空指针异常。 Nashorn JavaScript 引擎 − Java8 提供了一个新的 Nashorn javascript 引擎，它允许我们在 JVM 上运行特定的 javascript 应用。 为什么需要 Steam？Java8 中的 Stream 是对集合（Collection）对象功能的增强，它专注于对集合对象进行各种非常便利、高效的聚合操作，或者大批量数据操作。 StreamAPI 借助于同样新出现的 Lambda 表达式，极大的提高编程效率和程序可读性。同时它提供串行和并行两种模式进行汇聚操作，并发模式能够充分利用多核处理器的优势，使用 fork/join 并行方式来拆分任务和加速处理过程。 流的操作种类中间操作当数据源中的数据上了流水线后，这个过程对数据进行的所有操作都称为“中间操作”。中间操作仍然会返回一个流对象，因此多个中间操作可以串连起来形成一个流水线。 终端操作当所有的中间操作完成后，若要将数据从流水线上拿下来，则需要执行终端操作。终端操作将返回一个执行结果，这就是你想要的数据。 java.util.Stream 使用示例定义一个简单的学生实体类，用于后面的例子演示： 1234567891011121314151617181920212223242526272829303132333435363738public class Student &#123; /** 学号 */ private long id; /** 姓名 */ private String name; /** 年龄 */ private int age; /** 性别 */ private int grade; /** 专业 */ private String major; /** 学校 */ private String school; // 省略 getter 和 setter&#125;// 初始化List&lt;Student&gt; students = new ArrayList&lt;Student&gt;() &#123; &#123; add(new Student(20160001, "孔明", 20, 1, "土木工程", "武汉大学")); add(new Student(20160002, "伯约", 21, 2, "信息安全", "武汉大学")); add(new Student(20160003, "玄德", 22, 3, "经济管理", "武汉大学")); add(new Student(20160004, "云长", 21, 2, "信息安全", "武汉大学")); add(new Student(20161001, "翼德", 21, 2, "机械与自动化", "华中科技大学")); add(new Student(20161002, "元直", 23, 4, "土木工程", "华中科技大学")); add(new Student(20161003, "奉孝", 23, 4, "计算机科学", "华中科技大学")); add(new Student(20162001, "仲谋", 22, 3, "土木工程", "浙江大学")); add(new Student(20162002, "鲁肃", 23, 4, "计算机科学", "浙江大学")); add(new Student(20163001, "丁奉", 24, 5, "土木工程", "南京大学")); &#125;&#125;; forEachStream 提供了新的方法’forEach’来迭代流中的每个数据。ForEach 接受一个 function 接口类型的变量，用来执行对每一个元素的操作。ForEach 是一个中止操作，它不返回流，所以我们不能再调用其他的流操作。 以下代码片段使用 forEach 输出了 10 个随机数： 12345// 随机生成 10 个 0,100int 类型随机数new Random() .ints(0, 100) .limit(10) .forEach(System.out::println); 从集合 students 中筛选出所有武汉大学的学生： 1234List&lt;Student&gt; whuStudents = students .stream() .filter(student -&gt; "武汉大学".equals(student.getSchool())) .collect(Collectors.toList()); filter/distinct[过滤操作]filter 方法用于通过设置的条件过滤出元素。Filter 接受一个 predicate 接口类型的变量，并将所有流对象中的元素进行过滤。该操作是一个中间操作，因此它允许我们在返回结果的基础上再进行其他的流操作。 以下代码片段使用 filter 方法过滤出空字符串： 123456// 获取空字符串的数量Arrays.asList("abc", "","bc","efg","abcd","", "jkl") // stream() − 为集合创建串行流 .stream() .filter(string -&gt; string.isEmpty()) .count(); distinct 方法用于去除重复元素。 1234Arrays.asList("a", "c", "ac", "c", "a", "b") .stream() .distinct() .forEach(System.out::println); anyMatch/allMatch/noneMatch匹配操作有多种不同的类型，都是用来判断某一种规则是否与流对象相互吻合的。所有的匹配操作都是终结操作，只返回一个 boolean 类型的结果。 anyMatch 方法用于判断集合中是否有任一元素满足条件。 1234// 集合中是否有任一元素匹配以'a'开头boolean result = Arrays.asList("abc", "","bc","efg","abcd","", "jkl") .stream() .anyMatch(s -&gt; s.startsWith("a")); allMatch 方法用于判断集合中是否所有元素满足条件。 1234// 集合中是否所有元素匹配以'a'开头boolean result = Arrays.asList("abc", "","bc","efg","abcd","", "jkl") .stream() .allMatch(s -&gt; s.startsWith("a")); noneMatch 方法用于判断集合中是否所有元素不满足条件。 1234// 集合中是否没有元素匹配以'a'开头boolean result = Arrays.asList("abc", "","bc","efg","abcd","", "jkl") .stream() .noneMatch(s -&gt; s.startsWith("a")); limit/skiplimit 方法用于返回前面 n 个元素。 12345Arrays.asList("abc", "","bc","efg","abcd","", "jkl") .stream() .filter(string -&gt; !string.isEmpty()) .limit(3) .forEach(System.out::println); skip 方法用于舍弃前 n 个元素。 12345Arrays.asList("abc", "","bc","efg","abcd","", "jkl") .stream() .filter(string -&gt; !string.isEmpty()) .skip(1) .forEach(System.out::println); sorted[排序操作]sorted 方法用于对流进行排序。Sorted 是一个中间操作，能够返回一个排过序的流对象的视图。流对象中的元素会默认按照自然顺序进行排序，除非你自己指定一个 Comparator 接口来改变排序规则。 以下代码片段使用 filter 方法过滤掉空字符串，并对其进行自然顺序排序： 12345678910111213141516171819List&lt;String&gt; strings = Arrays.asList("abc", "","bc","efg","abcd","", "jkl");// 一定要记住, sorted 只是创建一个流对象排序的视图, 而不会改变原来集合中元素的顺序。strings .stream() .filter(string -&gt; !string.isEmpty()) .sorted() .forEach(System.out::println);// 输出原始集合元素, sorted 只是创建排序视图, 不影响原来集合顺序strings .stream() .forEach(System.out::println); // 按照字符串长度进行排序, 若两个字符串长度相同, 按照字母顺序排列strings .stream() .filter(string -&gt; !string.isEmpty()) // 1. 首先根据字符串长度倒序排序; 2. 然后根据字母顺序排列 .sorted(Comparator.comparing(String::length).reversed().thenComparing(String::compareTo)) .forEach(System.out::println); 以下代码片段根据 Person 姓名倒序排序，然后利用 Collectors 返回列表新列表： 123456789101112List&lt;Person&gt; persons = new ArrayList();// 1. 生成 5 个 Person 对象for (int i = 1; i &lt;= 5; i++) &#123; Person person = new Person(i, "name" + i); persons.add(person);&#125;// 2. 对 Person 列表进行排序, 排序规则: 根据 Person 姓名倒序排序, 然后利用 Collectors 返回列表新列表;List&lt;Person&gt; personList = persons .stream() .sorted(Comparator.comparing(Person::getName).reversed()) .collect(Collectors.toList()); parallel流操作可以是顺序的，也可以是并行的。顺序操作通过单线程执行，而并行操作则通过多线程执行。可使用并行流进行操作来提高运行效率 parallelStream 是流并行处理程序的代替方法。parallelStream()本质上基于 Java7 的 Fork-Join 框架实现，Fork-Join 是一个处理并行分解的高性能框架，其默认的线程数为宿主机的内核数。 以下实例我们使用 parallelStream 来输出空字符串的数量： 123456// 获取空字符串的数量[parallelStream 为 Collection 接口的一个默认方法]Arrays.asList("abc", "","bc","efg","abcd","", "jkl") // parallelStream() − 为集合创建并行流 .parallelStream() .filter(string -&gt; string.isEmpty()) .count(); parallelStream 中 forEachOrdered 与 forEach 区别： 12345List&lt;String&gt; strings = Arrays.asList("a", "b", "c");strings.stream().forEachOrdered(System.out::print); //abcstrings.stream().forEach(System.out::print); //abcstrings.parallelStream().forEachOrdered(System.out::print); //abcstrings.parallelStream().forEach(System.out::print); //bca 特别注意：1、千万不要任意地并行 Stream pipeline，如果源头是来自 stream.iterate，或者中间使用了中间操作的 limit，那么并行 pipeline 也不可能提升性能。因此，在 Stream 上通过并行获取的性能，最好是通过 ArrayList、HashMap、HashSet 和 CouncurrentHashMap 实例，数组，int 范围和 long 范围等。这些数据结构的共性是，都可以被精确、轻松地分成任意大小的子范围，使并行线程中的分工变得更加轻松。2、Stream pipeline 的终止操作本质上也影响了并发执行的效率。并行的最佳操作是做减法，用一个 Stream 的 reduce 方法，将所有从 pipeline 产生的元素都合并在一起，或者预先打包想 min、max、count 和 sum 这类方法。骤死式操作如 anyMatch、allMatch 和 nonMatch 也都可以并行。由 Stream 的 collect 方法执行的操作，都是可变的减法，不是并行的最好选择，因此并行集合的成本非常高。3、一般来说，程序中所有的并行 Stream pipeline 都是在一个通用的 fork-join 池中运行的。只要有一个 pipeline 运行异常，都是损害到系统中其它不相关部分的性能。因此，如果对 Stream 进行不恰当的并行操作，可能导致程序运行失败，或者造成性能灾难。 map[变换操作]map 方法用于映射每个元素到对应的结果。map 是一个对于流对象的中间操作，通过给定的方法，它能够把流对象中的每一个元素对应到另外一个对象上。以下代码片段使用 map 将集合元素转为大写 (每个元素映射到大写)-&gt; 降序排序 -&gt;迭代输出： 1234567891011Arrays.asList("abc", "","bc","efg","abcd","", "jkl") // 通过 stream()方法即可获取流对象 .stream() // 通过 filter()过滤元素 .filter(string -&gt; !string.isEmpty()) // 通过 map()方法用于映射每个元素到对应的结果 .map(String::toUpperCase) // 通过 sorted()方法用于对流进行排序 .sorted(Comparator.reverseOrder()) // 通过 forEach()方法迭代流中的每个数据 .forEach(System.out::println); 筛选出所有专业为计算机科学的学生姓名： 1234List&lt;String&gt; names = students .stream() .filter(student -&gt; "计算机科学".equals(student.getMajor())) .map(Student::getName).collect(Collectors.toList()); 计算所有专业为计算机科学学生的年龄之和： 1234int totalAge = students .stream() .filter(student -&gt; "计算机科学".equals(student.getMajor())) .mapToInt(Student::getAge).sum(); findFirst/findAny()findAny 能够从流中随便选一个元素出来，它返回一个 Optional 类型的元素。 123Optional&lt;String&gt; optional = Arrays.asList("abc", "","bc","efg","abcd","", "jkl") .stream() .findAny(); findFirst 能够从流中选第一个元素出来，它返回一个 Optional 类型的元素。 123Optional&lt;String&gt; optional = Arrays.asList("abc", "","bc","efg","abcd","", "jkl") .stream() .findFirst(); collectcollect 方法是一个终端操作，它接收的参数是将流中的元素累积到汇总结果的各种方式(称为收集器)。 Collectors 工具类提供了许多静态工具方法来为大多数常用的用户用例创建收集器，比如将元素装进一个集合中、将元素分组、根据不同标准对元素进行汇总等。 Collectors.joining()Collectors.joining()方法以遭遇元素的顺序拼接元素。我们可以传递可选的拼接字符串、前缀和后缀。 123456789101112List&lt;String&gt; strings = Arrays.asList("abc", "","bc","efg","abcd","", "jkl");// 筛选列表List&lt;String&gt; filtered = strings .stream() .filter(string -&gt; !string.isEmpty()) .collect(Collectors.toList()); // 合并字符串String mergedString = strings .stream() .filter(string -&gt; !string.isEmpty()) .collect(Collectors.joining(",")); Collectors.groupingByCollectors.groupingBy 方法根据项目的一个属性的值对流中的项目作问组，并将属性值作为结果 Map 的键。 List 里面的对象元素，以某个属性来分组。 1234567891011// 按学校对学生进行分组:Map&lt;String, List&lt;Student&gt;&gt; groups = students .stream() .collect(Collectors.groupingBy(Student::getSchool)); // 多级分组, 在按学校分组的基础之上再按照专业进行分组Map&lt;String, Map&lt;String, List&lt;Student&gt;&gt;&gt; groups2 = students .stream() .collect( Collectors.groupingBy(Student::getSchool, // 一级分组，按学校 Collectors.groupingBy(Student::getMajor))); // 二级分组，按专业 统计 List 集合重复元素出现次数。 12345678910111213141516List&lt;String&gt; items = Arrays.asList("apple", "apple", "banana", "apple", "orange", "banana", "papaya");// 方式一Map&lt;String, Long&gt; result = items .stream() // Function.identity() 返回一个输出跟输入一样的 Lambda 表达式对象, 等价于形如 t -&gt; t 形式的 Lambda 表达式. .collect(Collectors.groupingBy(Function.identity(), Collectors.counting())); // 方式二Map&lt;String, Long&gt; result2 = items .stream() // Collectors.counting() 计算流中数量 .collect(Collectors.groupingBy(String::toString, Collectors.counting()));// Output :// &#123;papaya=1, orange=1, banana=2, apple=3&#125; 统计每个组的个数： 123Map&lt;String, Long&gt; groups = students .stream() .collect(Collectors.groupingBy(Student::getSchool, Collectors.counting())); 累加求和 12345// 统计相同姓名, 总年龄大小Map&lt;String, Integer&gt; sumMap = persons .stream() // Collectors.summingInt() 返回流中整数属性求和 .collect(Collectors.groupingBy(Person::getName, Collectors.summingInt(Person::getAge))); Collectors.toMapCollectors.toMap 方法将 List 转 Map。 123456789// 根据 Person 年龄生成 MapMap&lt;Integer, Person&gt; personMap = persons .stream() .collect(Collectors.toMap(Person::getAge, person -&gt; person)); // account -&gt; account 是一个返回本身的 lambda 表达式, 其实还可以使用 Function 接口中的一个默认方法代替, 使整个方法更简洁优雅.Map&lt;Integer, Person&gt; personMap = persons .stream() .collect(Collectors.toMap(Person::getAge, Function.identity())); 当 key 重复时，会抛出异常：java.lang.IllegalStateException: Duplicate key ** 1234// 针对重复 key 的, 覆盖之前的 valueMap&lt;Integer, Person&gt; personMap = persons .stream() .collect(Collectors.toMap(Person::getAge, Function.identity(), (person, person2) -&gt; person2)); 指定具体收集的 map： 1234// 指定具体收集的 mapMap&lt;Integer, Person&gt; personMap = persons .stream() .collect(Collectors.toMap(Person::getAge, Function.identity(), (person, person2) -&gt; person2, LinkedHashMap::new)); 当 value 为 null 时，会抛出异常：java.lang.NullPointerException[Collectors.toMap 底层是基于 Map.merge 方法来实现的，而 merge 中 value 是不能为 null 的，如果为 null，就会抛出空指针异常。 123Map&lt;Integer, String&gt; personMap = persons .stream() .collect(Collectors.toMap(Person::getAge, Person::getName, (person, person2) -&gt; person2)); 12345678910// 1. 解决方式 1: 用 for 循环的方式亦或是 forEach 的方式Map&lt;Integer, String&gt; personMap = new HashMap&lt;&gt;();for (Person person : persons) &#123; personMap.put(person.getAge(), person.getName());&#125;// 2. 解决方式 2: 使用 stream 的 collect 的重载方法Map&lt;Integer, String&gt; personMap = persons .stream() .collect(HashMap::new, (m, v) -&gt; m.put(v.getAge(), v.getName()), HashMap::putAll); Collectors.collectingAndThenCollectors.collectingAndThen 方法主要用于转换函数返回的类型。 List 里面的对象元素，以某个属性去除重复元素。 123List&lt;Person&gt; unique = persons .stream() .collect(Collectors.collectingAndThen(Collectors.toCollection(() -&gt; new TreeSet&lt;&gt;(Comparator.comparingInt(Person::getAge))), ArrayList::new)); Collectors.partitioningByCollectors.partitioningBy 方法主要用于根据对流中每个项目应用谓词的结果来对项目进行分区。 “年龄小于 18”进行分组后可以看到，不到 18 岁的未成年人是一组，成年人是另外一组。 123Map&lt;Boolean, List&lt;Person&gt;&gt; groupBy = persons .stream() .collect(Collectors.partitioningBy(o -&gt; o.getAge() &gt;= 18)); Collectors 收集器静态方法： 数值流的使用在 Stream 里元素都是对象，那么，当我们操作一个数字流的时候就不得不考虑一个问题，拆箱和装箱。虽然自动拆箱不需要我们处理，但依旧有隐含的成本在里面。Java8 引入了 3 个原始类型特化流接口来解决这个问题：IntStream、DoubleStream、LongStream，分别将流中的元素特化为 int、long、double，从而避免了暗含的装箱成本。 将对象流映射为数值流1234// 将对象流映射为数值流IntStream intStream = persons .stream() .mapToInt(Person::getAge); 默认值 OptinalInt由于数值流经常会有默认值，比如默认为 0。数值特化流的终端操作会返回一个 OptinalXXX 对象而不是数值。 1234567// 每种数值流都提供了数值计算函数, 如 max、min、sum 等OptionalInt optionalInt = persons .stream() .mapToInt(Person::getAge) .max();int max = optionalInt.orElse(1); 生成一个数值范围流1234// 创建一个包含两端的数值流, 比如 1 到 10, 包含 10:IntStream intStream = IntStream.rangeClosed(1, 10);// 创建一个不包含结尾的数值流, 比如 1 到 9:IntStream range = IntStream.range(1, 9); 将数值流转回对象流12// 将数值流转回对象流Stream&lt;Integer&gt; boxed = intStream.boxed(); 流的扁平化案例：对给定单词列表 [“Hello”,”World”]，你想返回列表[“H”,”e”,”l”,”o”,”W”,”r”,”d”] 方法一：错误方式 123456789String[] words = new String[]&#123;"Hello", "World"&#125;;List&lt;String[]&gt; a = Arrays.stream(words) .map(word -&gt; word.split("")) .distinct() .collect(Collectors.toList());a.forEach(System.out::print);// Output// [Ljava.lang.String;@12edcd21[Ljava.lang.String;@34c45dca 返回一个包含两个 String[]的 list，传递给 map 方法的 lambda 为每个单词生成了一个 String[]。因此，map 返回的流实际上是 Stream&lt;String[]&gt;类型的。 方法二：正确方式 12345678910String[] words = new String[]&#123;"Hello", "World"&#125;;List&lt;String&gt; a = Arrays.stream(words) .map(word -&gt; word.split("")) .flatMap(Arrays::stream) .distinct() .collect(Collectors.toList());a.forEach(System.out::print);// Output// HeloWrd 使用 flatMap 方法的效果是，各个数组并不是分别映射一个流，而是映射成流的内容，所有使用 map(Array::stream)时生成的单个流被合并起来，即扁平化为一个流。 参考博文[1]. Java 8 中的 Streams API 详解[2]. java8 快速实现 List 转 map 、分组、过滤等操作 Java8 那些事儿系列 Java8 那些事儿（一）：Stream 函数式编程 Java8 那些事儿（二）：Optional 类解决空指针异常 Java8 那些事儿（三）：Date/Time API(JSR 310) Java8 那些事儿（四）：增强的 Map 集合 Java8 那些事儿（五）：函数式接口]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java8</tag>
        <tag>Stream</tag>
        <tag>语法详解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解 Java 虚拟机（一）：Java 内存区域与内存溢出异常]]></title>
    <url>%2Farchives%2F6cd92fa9.html</url>
    <content type="text"><![CDATA[概述JVM 的内存模型是 Java 语言绕不开的一个话题。对于 Java 程序员来说，在虚拟机自动内存管理机制的帮助下，不再需要为每一个 new 操作去写配对的 delete/free 代码，不容易出现内存泄漏问题，由虚拟机管理内存这一切看起来都很美好。不过，也正因为 Java 程序员把内存控制的权利交给了 Java 虚拟机，一旦出现内存泄漏和溢出方面的问题，如果不了解虚拟机是怎样使用内存的，那么排查错误将成为一项异常艰难的工作。即要进行 Java 的性能调优，首先就要了解其内存模型；同时，在诸多的面试笔试中，这也是很多面试官会考察的内容。 本篇是我学习 JVM 系列的开篇，同时也是我阅读周志明老师《深入理解 Java 虚拟机》一书的学习笔记，主要内容讲述 JVM 的基本概念，然后从概念上介绍 Java 虚拟机内存的各个区域，讲解这些区域的作用、服务对象以及其中可能产生的问题，最后讨论一下常见的内存泄漏和溢出的问题。 JVM 基本概念JDK 与 JRE 区别JDK：把 Java 程序设计语言、Java 虚拟机、Java API 类库这三部分统称为 JDK（Java Development Kit），JDK 是用于支持 Java 程序开发的最小环境，是 Java 语言的软件开发工具包（SDK）。 JRE：把 Java API 类库中的 Java SE API 子集和 Java 虚拟机这两部分统称为 JRE（Java Runtime Environment），JRE 是支持 Java 程序运行的标准环境。 JVM 是什么？JVM 是 Java Virtual Machine（Java 虚拟机）的缩写，是一个虚构出来的计算机，是通过在实际的计算机上仿真模拟各种计算机功能来实现的。 Java 语言的一个非常重要的特点就是与平台的无关性，而使用 Java 虚拟机是实现这一特点的关键。一般的高级语言如果要在不同的平台上运行，至少需要编译成不同的目标代码。而引入 Java 虚拟机后，Java 语言在不同平台上运行时不需要重新编译。Java 语言使用 Java 虚拟机屏蔽了与具体平台相关的信息，使得 Java 语言编译程序只需生成在 Java 虚拟机上运行的目标代码（字节码），就可以在多种平台上不加修改地运行。Java 虚拟机在执行字节码时，把字节码解释成具体平台上的机器指令执行。这就是 Java 的能够 “一次编译，到处运行” 的原因。 Sun 公司开发了 Java 语言，但任何人都可以在遵循 JVM 规范的前提下开发和提供 JVM 实现。所以目前业界有多种不同的 JVM 实现，包括 Oracle Hotspot、Oracle JRockit、IBM J9、MRJ（MacOS Runtime for Java），它们都实现了 Java 虚拟机规范，但内存管理机制的实现方式各异。在平常的学习和工作中，我们接触的最多的就是 Oracle Hotspot，它是 SunJDK 和 OpenJDK[1] 中所带的虚拟机，也是目前使用范围最广的 Java 虚拟机。 怎么区分 SunJDK 和 OpenJDK查看 Java 版本，显示 OpenJDK 则表示 OpenJDK 1234[root@VM_24_98_centos ~]# java -versionopenjdk version "1.8.0_232"OpenJDK Runtime Environment (build 1.8.0_232-b09)OpenJDK 64-Bit Server VM (build 25.232-b09, mixed mode) 查看 Java 版本，显示 Java(TM) SE 则表示 SunJDK 1234[root@iZbp11s2rh7xf6zphn2z5tZ ~]# java -versionjava version "1.8.0_161"Java(TM) SE Runtime Environment (build 1.8.0_161-b12)Java HotSpot(TM) 64-Bit Server VM (build 25.161-b12, mixed mode) 运行时数据区域 运行时数据区是 JVM 程序运行时在操作系统上分配的内存区域。Java 虚拟机在执行 Java 程序的过程中会把它所管理的内存划分为若干个不同的数据区域。这些区域都有各自的用途，以及创建和销毁的时间，有的区域随着虚拟机进程的启动而存在，有些区域则依赖用户线程的启动和结束而建立和销毁。根据《Java 虚拟机规范（Java SE 7 版）》的规定，Java 虚拟机所管理的内存将包括一下几个运行时数据区域：程序计数器、Java 虚拟机栈、本地方法栈、Java 堆、方法区。 程序计数器程序计数器（Program Counter Register）是一块较小的内存空间，它可以看作是当前线程所执行的节字码的行号指示器。 由于 Java 虚拟机的多线程是通过线程轮流切换并分配处理器执行时间的方式来实现的，在任何一个确定的时刻，一个处理器（对于多核处理器来说是一个内核）都只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各条线程之间的计数器互不影响，独立存储，我们称这类内存区域为 “线程私有” 的内存。 如果线程正在执行的是一个 Java 方法，这个计数器记录的是正在执行的虚拟机字节码指令地址；如果正在执行的是 Native 方法，这个计数器值则为空（Undefind）。此内存区域是唯一一个在 Java 虚拟机规范中没有规定任何 OutOfMemoryError 情况的区域。 Java 虚拟机栈与程序计数器一样，Java 虚拟机栈（Java Virtual Machine Stacks）也是线程私有的，它的生命周期与线程相同。虚拟机栈描述的是 Java 方法执行的内存模型：每个方法在执行的同时都会创建一个栈帧（Stack Frame - 是方法运行时的基础数据结构），用于存储局部变量表、操作数栈、动态链接、方法出口等信息。每一个方法从调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中入栈到出栈的过程。 经常有人把 Java 内存分为堆内存（Heap）和栈内存（Stack），这种划分方式的流行只能说明大多数程序员最关注的、与对象内存分配关系最密切的内存区域是这两块。其中所指的栈就是现在讲的虚拟机栈，或者说是虚拟机栈中局部变量表部分。 局部变量表存放了编译器可知的各种基本数据类型（boolean、byte、char、short、int、long、float、double）、对象引用（reference 类型，它不等同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或者其他与此对象相关的地址）和 returnAddress 类型（指向了一条节字码指令的地址）。 其中 64 位长度的 long 和 double 类型的数据会占用 2 个局部变量空间（Slot），其余的数据类型只占用 1 个。局部变量表所需的内存空间在编译期间完成分配，当进入一个方法时，这个方法需要在帧中分配多大的局部变量空间是完全确定的，在方法运行期间不会改变局部变量表的大小。 在 Java 虚拟机规范中，对这个区域规定了两种异常状况：如果线程请求的栈深度大于虚拟机所允许的深度，将抛出 StackOverflowError 异常；如果虚拟机栈可以动态扩展（当前大部分的 Java 虚拟机都可动态扩展，只不过 Java 虚拟机规范中也允许固定长度的虚拟机栈），如果扩展时无法申请到足够的内存，就会抛出 OutOfMemoryError 异常。 本地方法栈本地方法栈（Native Method Stack）与虚拟机栈所发挥的作用是非常相似的，它们之间的区别不过是虚拟机栈为虚拟机执行 Java 方法（也就是节字码）服务，而本地方法栈则为虚拟机使用到的 Native 方法服务。在虚拟机规范中对本地方法栈中方法使用的语言、使用方式与数据结构并没有强制规定，因此具体的虚拟机可以自由实现它。甚至有的虚拟机（譬如 Sun HotSpot 虚拟机）直接就把本地方法栈和虚拟机栈合二为一。 Java 堆对于大多数应用来说，Java 堆（Java Heap）是 Java 虚拟机所管理的内存中最大的一块。Java 堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有对象实例都在这里分配内存。这一点在 Java 虚拟机规范中的描述是：所有的对象实例以及数组都要在堆上分配，但是随着 JIT 编译器的发展与逃逸分析技术逐渐成熟，栈上分配、标量替换优化技术将会导致一些微妙的变化发生，所有的对象都分配在堆上也渐渐变得不是那么 “绝对” 了。 Java 堆是垃圾收集器管理的主要区域，因此很多时候也被称作“GC 堆”（Garbage Collected Heap）。从内存回收的角度来看，由于现在收集器基本都采用分代收集算法，所以 Java 堆中还可以细分为：新生代和老年代；再细致一点的有 Eden 空间、From Survivor 空间、To Survivor 空间等。从内存分配的角度来看，线程共享的 Java 堆中可能划分出多个线程私有的分配缓冲区（Thread Local Allocation Buffer，TLAB）。不过无论如何划分，都与存放内容无关，无论哪个区域，存储的都仍然是对象实例，进一步划分的目的是为了更好地回收内存，或者更快地分配内存。 根据 Java 虚拟机的规定，Java 堆可以处于物理上不连续的内存空间中，只要逻辑上是连续的即可，就像我们的磁盘空间一样。如果在堆中没有内存完成实例分配，并且堆也无法再扩展时，将会抛出 OutOfMemoryError。 方法区方法区（Method Area）与 Java 堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即使编译器编译后的代码等数据。虽然 Java 虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做 Non-Heap（非堆），目的应该是与 Java 堆分开来。 对于习惯在 HotSpot 虚拟机上开发、部署程序的开发者来说，很多人都更愿意把方法区称为“永久代”（Permanent Generation），本质上两者并不等价，仅仅是因为 HotSpot 虚拟机的设计团队选择把 GC 分代收集扩展至方法区，或者说使用永久代来实现方法区而已，这样 HotSpot 的垃圾收集器可以像管理 Java 堆一样管理这部分内存，能够省去专门为方法区编写内存管理代码的工作。对于其他虚拟机（如 BEA JRockit、IBM J9 等）来说是不存在永久代的概念的。原则上，如何实现方法区属于虚拟机实现细节，不受虚拟机规范约束，但使用永久代来实现方法区，现在看来并不是一个好主意，因为这样更容易遇到内存溢出问题。因此，对于 HotSpot 虚拟机，根据官方发布的路线图信息，现在也有放弃永久代并逐步改为采用 NativeMemory 来实现方法区的规划了，在目前已经发布的 JDK 1.7 的 HotSpot 中，已经把原来放在永久代的字符串常量池移出。 Java 虚拟机规范中堆方法区的限制非常宽松，除了和 Java 堆一样不需要连续的内存和可以选择固定大小或者可扩展外，还可以选择不实现垃圾收集。相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入了方法区就如永久代的名字一样 “永久” 存在了。这区域的内存回收目标主要是针对常量池的回收和对类型的卸载，一般来说，这个区域的回收 “成绩” 比较难以令人满意，尤其是类型的卸载，条件相当苛刻，但是这部分区域的回收确实是必要的。 根据 Java 虚拟机规范的规定，当方法区无法满足内存分配需求时，将抛出 OutOfMemoryError 异常。 运行时常量池运行时常量池（Runtime Constant Pool）是方法区的一部分。Class 文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项是常量池（Constant Pool Table），用于存放编译期间生成的各种字面量和符号引用，这部分内容将在类加载后进入方法区的运行时常量池中存放。 Java 虚拟机对 Class 文件每一部分（自然也包括常量池）的格式都有严格规定，每一个字节用于存储哪种数据都必须符合规范上的要求才会被虚拟机认可、装载和执行，但对于运行时常量池，Java 虚拟机规范没有做任何细节的要求，不同的提供商实现的虚拟机可以按照自己的需求来实现这个内存区域。不过，一般来说，除了保存 Class 文件中描述的符号引用外，还会把翻译出来的直接引用也存储运行时常量池中。 运行时常量池相对于 Class 文件常量池的另外一个重要特征是具备动态性，Java 语言并不要求常量一定只有编译期才能产生，也就是并非预置入 Class 文件中常量池的内容才能进入方法区运行时常量池，运行期间也可能将新的常量放入池中，这种特性被开发人员利用得较多的便是 String 类的 intern()方法。 既然运行时常量池是方法区的一部分，自然受到方法区内存的限制，当常量池无法在申请到内存时会抛出 OutOfMemoryError 异常。 直接内存直接内存（Direct Memory）并不是虚拟机运行时数据区的一部分，也不是 Java 虚拟机规范中定义的内存区域。但是这部分内存也被频繁地使用，而且也可能导致 OutOfMemoryError 异常出现。 在 JDK 1.4 中新加入了 NIO（New Input/Output）类，引入了一种基于通道（Channel）与缓存区（Buffer）的 I/O 方式，它可以使用 Native 函数库直接分配堆外内存，然后通过一个存储在 Java 堆中的 DirectByteBuffer 对象作为这块内存的引用进行操作。这样能在一些场景中显著提高性能，因为避免了在 Java 堆和 Native 堆中来回复制数据。 显然，本机直接内存的分配不会受到 Java 堆大小的限制，但是，既然是内存，肯定还是会受到本机总内存大小以及处理器寻址空间的限制。服务器管理员在配置虚拟机参数时，会根据实际内存设置 - Xmx 等参数信息，但经常忽略直接内存，使得各个内存区域总和大于物理内存限制（包括物理和操作系统级的限制），从而导致动态扩展时出现 OutOfMemoryError 异常出现。 Java 可以通过 java.nio.ByteBuffer.allocateDirect(capacity) 直接使用 non java heap（Java堆外）的内存 。 优点：1. 堆外内存不影响 JVM GC，程序会减少 Full GC 次数；2. IO 操作使用堆外内存比堆内存快。因为堆内在 flush 到远程时，会先复制到直接内存（非堆内存），然后在发送。 缺点：1. 堆外内存难以控制，如果内存泄漏，那么很难排查；2. 堆外内存只能通过序列化和反序列化来存储，保存对象速度比堆内存慢，不适合存储很复杂的对象。 OutOfMemoryError 异常JVM 内存参数设置 -Xms 设置堆的最小空间大小。-Xmx 设置堆的最大空间大小。-XX:NewSize 设置新生代最小空间大小。-XX:MaxNewSize 设置新生代最大空间大小。-XX:PermSize 设置永久代最小空间大小。-XX:MaxPermSize 设置永久代最大空间大小。-Xss 设置每个线程的堆栈大小。 Java 堆溢出Java 堆用于存储对象实例，只要不断地创建对象，并且保证 GC Roots 到对象之间有可达路径来避免垃圾回收机制清除这些对象，那么在对象数量达到最大推的容量限制后就会产生内存溢出异常。 例如 JVM Args: -Xms20m -Xmx20m -XX:+HeapDumpOnOutOfMemoryError将堆的最小值 - Xms 参数与最大值 - Xmx 参数设置为一样即可避免堆自动扩展；通过参数 -XX:+HeapDumpOnOutOfMemoryError 即可让虚拟机在出现内存溢出时 Dump 出当前内存堆转储快照以便事后进行分析。 Java 堆内存的 OOM 异常是实际应用中最常见的内存溢出异常情况。当出现 Java 堆内存溢出时，异常堆栈信息 “java.lang.OutOfMemoryError” 会跟着进一步提示“Java heap space”。 要解决这个区域的异常，一般的手段是先通过内存映像分析工具（如 Eclipse Memory Analyzer）对 Dump 出来的转储快照进行分析，重点是确认内存中的对象是否是必要的，也就是要先分清楚到底是出现了内存泄漏（Memory Leak）还是内存溢出（Memory Overflow）。 如果是内存泄漏，可进一步通过工具查看泄漏对象到 GC Roots 的引用链。于是就能找到泄漏对象是通过怎样的路径与 GC Roots 相关联并导致垃圾收集器无法自动回收它们的。掌握了泄漏对象的类型信息以及 GC Roots 引用链的信息，就可以比较准确地定位出泄漏代码的位置。 如果不存在泄漏，换句话说，就是内存中的对象确实都还必须存活着，那就应当检查虚拟机的堆参数（-Xmx 与 - Xms），与机器物理内存对比看是否还可以调大，从代码上检查是否存在某些对象生命周期过长、持有状态时间过长的情况，尝试减少程序运行期的内存消耗。 虚拟机栈和本地方法栈溢出由于在 HotSpot 虚拟机中并不区分虚拟机栈和本地方法栈，因此，对于 HotSpot 来说，虽然 - Xoss 参数（设置本地方法栈大小）存在，但实际上是无效的，栈容量只有 - Xss 参数设定。关于虚拟机栈和本地方法栈，在 Java 虚拟机规范中描述了两种异常： 如果线程请求的栈深度大于虚拟机所允许的最大深度，将抛出 StackOverflowError 异常。 如果虚拟机在扩展栈时无法申请到足够的内存空间，则抛出 OutOfMemoryError 异常。 这里把异常分成两种情况，看似更加严谨，但却存在着一些互相重叠的地方：当栈空间无法继续分配时，到底是内存太小，还是已使用的栈空间太大，其本质上只是对同一件事情的两种描述而已。 若将实现范围限制于单线程中的操作。 使用 - Xss 参数减少栈内存容量。结果：抛出 StackOverflowError 异常，异常出现时输出的堆栈深度相应缩小。 定义了大量的本地变量，增大此方法帧中本地变量表的长度。结果：抛出 StackOverflowError 异常，异常出现时输出的堆栈深度相应缩小。 实现结果表明：在单个线程下，无论是由于栈帧太大还是虚拟机栈容量太小，当内存无法分配的时候，虚拟机抛出的都是 StackOverflowError 异常。 方法区和运行时常量池溢出由于运行时常量池是方法区的一部分，因此这两个区域的溢出测试就放在一起进行。 String.intern() 是一个 Native 方法，它的作用是：如果字符串常量池中已经包含一个等于此 String 对象的字符串，则返回代表池中这个字符串的 String 对象；否则，将此 String 对象包含的字符串添加到常量池中，并且返回此 String 对象的引用。在 JDK 1.6 及之前的版本中，由于常量池分配在永久代内，我们可以通过 -XX:PermSize 和 -XX:MaxPermSize 限制方法区大小，从而间接限制其中常量池的容量。 从运行结果中可以看到，运行时常量池溢出，在 OutOfMemoryError 后面跟随的提示信息是“PermGen space”，说明运行时常量池属于方法区（HotSpot 虚拟机中的永久代）的一部分。 而使用 JDK 1.7 运行这段程序就不会得到相同的结果，while 循环将一直进行下去。 这段代码在 JDK 1.6 中运行，会得到两个 false，而在 JDK 1.7 中运行，会得到一个 true 和一个 false。产生差异的原因是：在 JDK 1.6 中，intern() 方法会把首次遇到的字符串实例复制到永久代中，返回的也是永久代中这个字符串实例的引用，而由 StringBuilder 创建的字符串实例在 Java 堆上，所以必然不是同一个引用，将返回 false。而 JDK 1.7（以及部分其他虚拟机，例如 JRockit）的 intern() 返回的引用和由 StringBuilder 创建的那个字符串实例是同一个。对于 str2 比较返回 false 是因为 “java” 这个字符串在执行 StringBuilder.toString() 之前已经出现过，字符串常量池中已经有它的引用了，不符合 “首次出现” 的原则，而 “计算机软件” 这个字符串则是首次首次出现的，因此返回 true。 方法区溢出也是一种常见的内存溢出异常，一个类要被垃圾收集器回收掉，判定条件是比较苛刻的。在经常动态生成大量 Class 的应用中，需要特别注意类的回收状态。这类场景除了程序使用了 CGLib 字节码增强和动态语言之外，常见的还有：大量 JSP 或动态产生 JSP 文件的应用（JSP 第一次运行时需要编译为 Java 类）、基于 OSGi 的应用（即使是同一个类文件，被不同的加载器也会视为不同的类）等。 本机直接内存溢出DirectMemory 容量可通过 -XX:MaxDirectMemorySize 指定，如果不指定，则默认与 Java 堆最大值（-Xmx 指定）一样，代码清单 2-9 越过了 DirectByteBuffer 类，直接通过反射获取 Unsafe 实例进行内存分配（Unsafe 类的 getUnsafe() 方法限制了只有引导类加载器才会返回实例，也就是设计者希望只有 rt.jar 中的类才能使用 Unsafe 的功能）。因为，虽然使用 DirectByteBuffer 分配内存也会抛出内存溢出异常，但它抛出异常时并没有真正向操作系统申请分配内存，而是通过计算得知内存无法分配，于是手动抛出异常，真正申请分配内存的方法是 unsafe.allocateMemory()。 由 DirectMemory 导致的内存溢出，一个明显的特征是 HeapDump 文件中不会看见明显的异常，如果读者发现 OOM 之后的 Dump 文件很小，而程序中又直接或间接使用了 NIO，那就可以考虑检查一下是不是这方面的问题。 延伸阅读再见永久代 PermGen，你好元空间 Metaspace永久代：对于习惯在 HotSpot 虚拟机上开发、部署程序的开发者来说，很多人都更愿意把方法区称为 “永久代”（Permanent Generation），本质上两者并不等价，仅仅是因为 HotSpot 虚拟机的设计团队选择把 GC 分代收集扩展至方法区，或者说使用永久代来实现方法区而已，这样 HotSpot 的垃圾收集器可以像管理 Java 堆一样管理这部分内存，能够省去专门为方法区编写内存管理代码的工作。（通过 -XX:PermSize 设置永久代初始大小；通过 -XX:MaxPermSize 设置永久代最大大小） 元空间：JDK 1.8 中 HotSpot 虚拟机设计团队使用元空间（Metaspace）代替永久代（PermGen），使用本地内存来存储类元数据，被称为 Metaspace。元空间的本质和永久代类似，都是对 JVM 规范中方法区的实现。不过元空间与永久代之间最大的区别在于：元空间并不在虚拟机中，而是使用本地内存。（通过 -XX:MetaspaceSize 设置元空间初始大小；通过 -XX:MaxMetaspaceSize 设置元空间最大大小，默认没有限制） 为什么废弃永久代（PermGen） 移除永久代是为融合 HotSpot JVM 与 JRockit VM 而做出的努力，因为 JRockit 没有永久代，不需要配置永久代。 Permgen 空间的具体多大很难预测。指定小了会造成 java.lang.OutOfMemoryError: Permgen size 错误，设置多了又造成浪费。 当使用元空间时，可以加载多少类元数据就不再由 MaxPermSize 控制, 而由系统的实际可用空间来控制。 永久代会为 GC 带来不必要的复杂度，并且回收效率偏低。 PermGen 到 Metaspace 数据转移从 JDK 1.7 开始永久代的移除工作，贮存在永久代的一部分数据已经转移到了 Java Heap 或者是 Native Heap。但永久代仍然存在于 JDK 1.7，并没有完全的移除：符号引用（Symbols）转移到了 Native Heap；字面量（interned strings）转移到了 Java Heap；类的静态变量（class statics）转移到了 Java Heap。因此，升级到 JDK 1.8 之后，会发现 Java 堆空间有所增长。我们可以通过一段程序来比较 JDK 1.6 与 JDK 1.7 及 JDK 1.8 的区别，以字符串常量为例： 12345678910111213141516package com.paddx.test.memory; import java.util.ArrayList;import java.util.List; public class StringOomMock &#123; static String base = "string"; public static void main(String[] args) &#123; List&lt;String&gt; list = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; Integer.MAX_VALUE; i++) &#123; String str = base + base; base = str; list.add(str.intern()); &#125; &#125;&#125; 我们通过 JDK 1.6、JDK 1.7 和 JDK 1.8 分别运行： 从上述结果可以看出，JDK 1.6 下，会出现 “PermGen Space” 的内存溢出，而在 JDK 1.7 和 JDK 1.8 中，会出现堆内存溢出，并且 JDK 1.8 中 PermSize 和 MaxPermGen 已经无效。因此，可以大致验证 JDK 1.7 和 1.8 将字符串常量由永久代转移到堆中，并且 JDK 1.8 中已经不存在永久代的结论。 逃逸分析前面说到 Java 堆对象分配时候提到：随着 JIT 编译器的发展以及逃逸分析技术逐渐成熟，栈上分配、标量替换优化技术将会导致一些微妙的变化大声，所有的对象都分配在堆上也渐渐变得不是那么“绝对”了。那么什么是逃逸分析呢？ 逃逸分析（Escape Analusis）是目前 Java 虚拟机中比较前言的优化技术，是为其他优化手段提供依据的分析技术。逃逸分析的基本行为就是分析对象动态作用域：当一个对象在方法中被定义后，它可能被外部方法所引用，例如作为调用参数传递到其他方法中，称为方法逃逸。甚至还可能被外部线程访问到，譬如赋值给类变量或者可以在其他线程中访问的实例变量，称为线程逃逸。如果能证明一个对象不会逃逸到方法或者线程之外，也就是别的方法或者线程无法通过任何途径访问到这个对象，则可能为这个变量进行一些高效的优化。 栈上分配（Stack Allocation）Java 虚拟机中，在 Java 堆上分配创建对象的内存空间几乎是 Java 程序员都清楚的常识了，Java堆中的对象对于各个线程都是共享和可见的，只要持有这个对象的引用，就可以访问堆中存储的对象数据。虚拟机的垃圾收集系统可以回收堆中不再使用的对象，但是回收动作无论是筛选可回收对象，还是回收和整理内存都需要耗费时间。如果确定一个对象不会逃逸出方法之外，那让这个对象在栈上分配内存将会是一个很不错的注意，对象所占用的内存空间就可以随栈帧出栈而销毁。在一般应用中，不会逃逸的局部对象所占的比例很大，如果能使用栈上分配，那大量的对象就会随着方法的结束而自动销毁了，垃圾收集系统的压力将会小很多。 同步消除（Synchronization Elimination）线程同步本身是一个相对耗时的过程，如果逃逸分析能够确定一个变量不会逃逸出线程，无法被其线程访问，那这个变量的读写肯定就不会有竞争，对这个变量实施的同步措施也就可以消除掉。 标量替换（Scalar Replacement）标量（Scalar）是指一个数据已经无法在分解成更小的数据来表示了，Java 虚拟机中的基本数据类型以及 reference 类型都不能再进一步分解，它们就可以成为标量。相对的，如果一个数据可以继续分解，那它就称作聚合量（Aggregate），Java 中的对象就是最典型的聚合量。如果把一个 Java 对象拆散，根据程序访问的情况，将其使用到的成员变量恢复原始类型来访问就叫做标量替换。如果逃逸分析证明一个对象不会被外部访问，并且这个对象可以被拆散的话，那程序真正执行的时候将可能不创建这个对象，而改为直接创建它的若干个被这个方法使用到的成员变量来替代。将对象拆分后，除了可以让对象的成员变量在栈上分配和读写之外，还可以为后续进一步的优化手段创建条件。 本地线程分配缓冲（Thread Local Allocation Buffer，TLAB）虚拟机为新生对象分配内存的分配方式我们知道虚拟机为新生对象分配内存的任务等同于把一块确定大小的内存从 Java 堆中划分出来，其分配方法主要分为以下两种：假设 Java 堆中的内存是绝对规整的，所有用过的内存都放在一边，空间的内存放在另外一边，中间放着一个指定作为分界点的指示器，那所分配内存就仅仅是把那个指针向空闲那边挪动一段与对象大小相等的距离，这种分配方法称为“指针膨胀”（Bump the Pointer）；如果 Java 堆中的内存并不是规整的，已使用的内存和空闲的内存相互交错，那么虚拟机就必须维护一个列表，记录上哪些内存块是可用的，在分配的时候从列表中找到一块足够大的空间划分给对象实例，并更新列表上的记录，这种分配方式称为“空间列表”（Free List）。选择哪种分配方式由 Java 堆是否规整决定，而 Java 堆是否规整又由所采用的的垃圾收集器是否带有压缩整理功能决定。因此，在使用 Serial、ParNer 等带 Compact 过程的收集器时，系统才用的分配算法是指针碰撞，而使用 CMS 这种基于 Mark-Sweep 算法的收集器时，通常采用空闲列表。 TLAB 与对象的创建除如何划分可用空间之外，还有另外一个需要考虑的问题是对象创建在虚拟机中是非常频繁的行为，即使是仅仅修改一个指针所指向的位置，在并发情况下也并不是线程安全的，可能出现正在给对象 A 分配内存，指针还没来得及修改，对象 B 又同时使用了原来的指针来分配内存的情况。解决这个问题有两种方案，一种是分配内存空间的动作进行同步处理——实际上虚拟机采用 CAS 配上失败重试的方式保证更新操作的原子性；另一种是把内存分配的动作按照线程划分在不同的空间之中进行，即每个线程在 Java 堆中预先分配一小怀内存，称为本地线程分配缓冲（Thread Local Allocation Buffer，TLAB）。哪个线程分配内存，就在哪个线程的 TLAB 上分配，只有 TLAB 用完并分配新的 TLAB 时，才需要同步锁定。虚拟机是否使用 TLAB，可以通过 -XX:+/-UseTLAB 参数来决定。 参考博文[1]. OpenJDK 和 Sun/OracleJDK 区别 与联系[2].《深入理解 Java 虚拟机：JVM 高级特效与最佳实现》，第 2 章[3]. Java 8: 从永久代（PermGen）到元空间（Metaspace）[4]. Java对象分配简要流程 注脚[1]. SunJDK 和 OpenJDK: 在 2006 年的 Java One 大会上，Sun 公司宣布最终会把 Java 开源，并在随后的一年，陆续将 JDK 的各个部分（其中当然也包括了 HotSpot VM）在 GPL 协议下公开了源码，并在此基础上建立了 OpenJDK。这样，HotSpot VM 便成为了 SunJDK 和 Open JDK 两个实现极度接近的 JDK 项目的共同虚拟机。 深入理解 Java 虚拟机系列 深入理解 Java 虚拟机（一）：Java 内存区域与内存溢出异常 深入理解 Java 虚拟机（二）：JVM 垃圾收集器 深入理解 Java 虚拟机（三）：内存分配与回收策略 深入理解 Java 虚拟机（四）：Jvm 性能监控与调优]]></content>
      <categories>
        <category>Jvm</category>
      </categories>
      <tags>
        <tag>Jvm</tag>
        <tag>内存区域</tag>
        <tag>内存溢出</tag>
        <tag>JDK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初识 Nginx（一）：理解原理和功能]]></title>
    <url>%2Farchives%2Fb9ada66e.html</url>
    <content type="text"><![CDATA[前言在工作中常常会接触到 Nginx，而本人却对它一知半解，便想在空余时间详细的了解一下 Nginx。本篇是我学习 Nginx 系列的开篇，主要内容讲述 Nginx 的基本概念，然后介绍一下 Nginx 的主要功能，最后探讨一下 Nginx 的模块化的组织架构，以及各个模块的分类、工作方式、职责和提供的相关指令。 什么是 Nginx？Nginx（”enginex”）是一款是由俄罗斯的程序设计师 Igor Sysoev 所开发高性能的 Web 和反向代理服务器，也是一个 IMAP/POP3/SMTP 代理服务器，主要用于提供网上信息浏览服务，为高并发网站的应用场景而设计。Nginx 最初是作为一个 Web 服务器创建的，用于解决 C10K[1] 的问题。 在高连接并发的情况下，Nginx 是 Apache 服务器不错的替代品。它的设计充分使用异步事件模型，削减上下文调度的开销，提高服务器并发能力。采用了模块化设计，提供了丰富模块的第三方模块。所以关于 Nginx，有这些标签：「异步」「事件」「模块化」「高性能」「高并发」「反向代理」「负载均衡」。 Nginx 的各种功能以模块（module）的形式提供，只有在编译安装时可以选择安装或不安装哪些模块，在源码编译后，或通过 Linux 软件管理包工具安装 Nginx，不能再加载或去除模块。 Nginx 能做什么？Nginx 在不依赖第三方模块能处理的事情包括：反向代理、正向代理、负载均衡、HTTP 服务器。 反向代理什么是反向代理?反向代理：指以代理服务器来接受 internet 上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给 internet 上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。 简单来说，反向代理对于客户端而言就是目标服务器，客户端向反向代理服务器发送请求后，反向代理服务器将该请求转发给内部网络上的后端服务器，并从后端服务器上得到的响应结果返回给客户端。 反向代理即是服务端代理，代理服务端，客户端不知道实际提供服务的服务端。 代码实现123456789server &#123; listen 80; server_name blog.maoning.pro; location / &#123; proxy_pass http://118.25.39.41:4000; proxy_set_header Host $host:$server_port; &#125;&#125; 保存配置文件后，重启 Nginx，访问 blog.maoning.pro，相当于访问 118.25.39.41:4000。 正向代理什么是正向代理?正向代理：是一个位于客户端和原始服务器之间的服务器，为了从原始服务器取得内容，客户端向代理发送一个请求并指定目标（原始服务器），然后代理向原始服务器转交请求并将获得的内容返回给客户端。客户端才能使用正向代理。 正向代理即是客户端代理，代理客户端，服务端不知道实际发起请求的客户端。 反向代理和正向代理区别？前面已经分别介绍了反向代理和正向代理，那么二者到底有什么区别呢？此处借用一下知乎网友的一张图片来表达。 负载均衡什么是负载均衡？当一台服务器的性能达到极限时，我们可以使用服务器集群来提高网站的整体性能。那么，在服务器集群中，需要有一台服务器充当调度者的角色，用户的所有请求都会首先由它接收，调度者再根据每台服务器的负载情况将请求分配给某一台后端服务器去处理。 那么在这个过程中，调度者如何合理分配任务，保证所有后端服务器都将性能充分发挥，从而保持服务器集群的整体性能最优，这就是负载均衡问题。 Nginx 负载均衡Nginx 目前支持自带 3 种负载均衡策略，还有 2 种常用的第三方策略（fair 和 url_hash 需要安装第三方模块才能使用）。下面分别介绍 Nginx 的负载均衡策略。 RR 方式（默认）负载均衡默认设置方式，每个请求按照时间顺序逐一分配至不同的后端服务器急性处理，如果有服务器宕机，会自动剔除。 12345678910111213141516# 设定负载均衡服务器列表upstream test &#123; server 192.168.0.1:8080; server 192.168.0.2:8080;&#125;server &#123; listen 80; server_name localhost; client_max_body_size 1024M; location / &#123; proxy_pass http://test; proxy_set_header Host $host:$server_port; &#125;&#125; weight 方式利用 weight 指定轮训的权重比率，与访问率成正比，用于后端服务器性能不均的情况。 123456789101112131415upstream test &#123; server 192.168.0.1:8080 weight=9; server 192.168.0.2:8080 weight=1;&#125;server &#123; listen 80; server_name localhost; client_max_body_size 1024M; location / &#123; proxy_pass http://test; proxy_set_header Host $host:$server_port; &#125;&#125; ip_hash 方式每个请求按访问 IP 的 hash 结果分配，这样可以使每个访客固定访问一个后端服务器，可以解决 Session 共享的问题。 12345678910111213141516upstream test &#123; ip_hash; server 192.168.0.1:8080; server 192.168.0.2:8080;&#125;server &#123; listen 80; server_name localhost; client_max_body_size 1024M; location / &#123; proxy_pass http://test; proxy_set_header Host $host:$server_port; &#125;&#125; fair 方式（第三方）按照每台服务器的影响时间分配请求，响应时间短的优先分配。 12345678910111213141516upstream test &#123; fair; server 192.168.0.1:8080; server 192.168.0.2:8080;&#125;server &#123; listen 80; server_name localhost; client_max_body_size 1024M; location / &#123; proxy_pass http://test; proxy_set_header Host $host:$server_port; &#125;&#125; url_hash 方式（第三方）按访问 url 的 hash 结果来分配请求，使每个 url 定向到同一个后端服务器，后端服务器为缓存时比较有效。P.S. 在 upstream 中加入 hash 语句，server 语句中不能写入 weight 等其他的参数，hash_method 是使用的 hash 算法。 1234567891011121314151617upstream test &#123; hash $request_uri; hash_method crc32; server 192.168.0.1:8080; server 192.168.0.2:8080;&#125;server &#123; listen 80; server_name localhost; client_max_body_size 1024M; location / &#123; proxy_pass http://test; proxy_set_header Host $host:$server_port; &#125;&#125; HTTP 服务器HTTP 服务器Nginx 本身也是一个静态资源的服务器，当只有静态资源的时候，就可以使用 Nginx 来做服务器。 12345678910server &#123; listen 80; server_name localhost; client_max_body_size 1024M; location / &#123; root /home/www/maoning/public; index index.html; &#125;&#125; 动静分离动静分离是将网站静态资源（Html，JavaScript，Css，Img 等文件）与后台应用分开部署，提高用户访问静态代码的速度，降低对后台应用访问。 动静分离是让动态网站里的动态网页根据一定规则把不变的资源和经常变的资源区分开来，动静资源做好了拆分以后，我们就可以根据静态资源的特点将其做缓存操作，这就是网站静态化处理的核心思路。 1234567891011121314151617181920212223242526server &#123; listen 80; server_name localhost; client_max_body_size 1024M; location / &#123; root /home/www/maoning/public; index index.html; &#125; # 所有静态请求由 nginx 处理 location ~ \.(gif|jpg|jpeg|png|bmp|swf|css|js)$ &#123; root /home/www/maoning/public; &#125; # 所有动态请求转发给后端服务器处理 location ~ \.(jsp|do)$ &#123; proxy_pass http://test; &#125; # 错误页面 error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root /home/www/maoning/public; &#125; &#125; Nginx 核心进程模型Nginx 整体架构Nginx 分为 Single 和 Master 两种进程模型，Single 模型即为单进程方式工作，具有较差的容错能力，常常在开发环境调试时候使用，不适合生产之用。Master 模型即为一个 master 进程 + N 个 worker 进程的工作方式，生产环境都是用 master-worker 模型来工作。 多进程网络模型Nginx 在启动后，会有一个 master 进程和多个 worker 进程。Nginx 的主进程将充当监控进程，而由主进程 fork()出来的子进程则充当工作进程。Nginx 包括一个 master 进程和数个 worker 进程，master 进程用于读取、解析配置文件和管理 worker 进程，worker 进程实际处理请求。Nginx 实现了基于事件的模型和操作系统机制驱动的请求分发。 master 主进程监控进程充当整个进程组与用户的交互接口，同时对进程进行监护。它不需要处理网络事件，不负责业务的执行，只会通过管理 worker 进程来实现重启服务、平滑升级、更换日志文件、配置文件实时生效等功能。 主进程启动以后首先初始化系统相应的信号量标志位，然后根据配置参数 （子进程个数、最大连接数等） 通过 fork 复制创建工作进程，worker 进程与 master 进程具有相同的上下文环境，接下来主进程和工作进程进入不同的循环，主进程保存子进程返回的 pid 写入文件，接着主进程进入信号处理的循环，监听系统接收到的 （例如 nginx-reload） 信号并进行相关的处理。master 进程在接收到信号后，会先重新加载配置，然后再启动新进程开始接收新请求，并向所有老进程发送信号告知不再接收新请求并在处理完所有未处理完的请求后自动退出，这就是 Nginx 的从容重启。 master 进程主要用来管理 worker 进程，具体包括如下 4 个主要功能：（1）接收来自外界的信号。（2）向各 worker 进程发送信号。（3）监控 woker 进程的运行状态。（4）当 woker 进程退出后（异常情况下），会自动重新启动新的 woker 进程。 worker 工作进程worker 进程的主要任务是完成具体的任务逻辑。通过主进程 fork 复制出 worker 进程，worker 进程环境变量 (监听 socket、文件描述符等) 都一样，因此各个 worker 进程完成等同，在相同的 socket 端口监听，请求到来时，每个 worker 工作进程都会监听到，但最终只会有一个 worker 进程会接受并处理（Nginx 提供了一把共享锁 accept_mutex 来保证同一时刻只有一个 work 进程在 accept 连接，从而解决惊群问题 [2]）。创建工作进程以后，工作进程进入循环，首先处理退出信号，然后进入事件处理过程 ngx_process_events_and_timers(cycle)，在进程处理函数中，首先处理定时任务，然后处理读取任务再处理写任务。 Nginx 是如何处理一个请求?首先，Nginx 在启动时，会解析配置文件，得到需要监听的端口与 ip 地址，然后在 Nginx 的 master 进程里面，先初始化好这个监控的 socket（创建 socket，设置 addrreuse 等选项，绑定到指定的 ip 地址端口，再 listen），然后再 fork（一个现有进程可以调用 fork 函数创建一个新进程。由 fork 创建的新进程被称为子进程）出多个子进程出来，然后子进程会竞争 accept 新的连接。此时，客户端就可以向 Nginx 发起连接了。当客户端与 Nginx 进行三次握手，与 Nginx 建立好一个连接后，此时，某一个子进程会 accept 成功，得到这个建立好的连接的 socket，然后创建 Nginx 对连接的封装，即 ngx_connection_t 结构体。接着，设置读写事件处理函数并添加读写事件来与客户端进行数据的交换。最后，Nginx 或客户端来主动关掉连接，到此，一个连接就寿终正寝了。 当然，Nginx 也是可以作为客户端来请求其它 server 的数据的（如 upstream 模块），此时，与其它 server 创建的连接，也封装在 ngx_connection_t 中。作为客户端，Nginx 先获取一个 ngx_connection_t 结构体，然后创建 socket，并设置 socket 的属性（比如非阻塞）。然后再通过添加读写事件，调用 connect/read/write 来调用连接，最后关掉连接，并释放 ngx_connection_t。 参考博文[1]. 认识 Nginx，理解原理和功能[2]. nginx 的五种负载算法模式[3]. 全面了解 Nginx 到底能做什么[4]. Nginx 详解 - 服务器集群 注脚[1]. C10K：随着互联网的普及，应用的用户群体几何倍增长，此时服务器性能问题就出现。最初的服务器是基于进程 / 线程模型。假如有 C10K（即单机 1 万个并发连接），就需要创建 1W 个进程，可想而知单机是无法承受的。那么如何突破单机性能是高性能网络编程必须要面对的问题，进而这些局限和问题就统称为 C10K 问题。[2]. 惊群问题： 惊群问题是由于系统中有多个进程在等待同一个资源，当资源可用的时候，系统会唤醒所有或部分处于休眠状态的进程去争抢资源，但是最终只会有一个进程能够成功的响应请求并获得资源，但在这个过程中由于系统要对全部的进程唤醒，导致了需要对这些进程进行不必要的切换，从而会产生系统资源的浪费。 初识 Nginx 系列 初识 Nginx（一）：理解原理和功能 初识 Nginx（二）：安装及配置说明]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>nginx</tag>
        <tag>负载均衡</tag>
        <tag>反向代理</tag>
        <tag>进程模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 工匠精神（一）：Systemd 命令完全指南]]></title>
    <url>%2Farchives%2Ff22a09ab.html</url>
    <content type="text"><![CDATA[理解 Linux 启动过程在我们打开 Linux 电脑的电源后第一个启动的进程就是 init。分配给 init 进程的 PID 是 1。它是系统其他所有进程的父进程。当一台 Linux 电脑启动后，处理器会先在系统存储中查找 BIOS，之后 BIOS 会检测系统资源然后找到第一个引导设备，通常为硬盘，然后会查找硬盘的主引导记录（MBR），然后加载到内存中并把控制权交给它，以后的启动过程就由 MBR 控制。 主引导记录会初始化引导程序（Linux 上有两个著名的引导程序，GRUB 和 LILO，80% 的 Linux 系统在用 GRUB 引导程序），这个时候 GRUB 或 LILO 会加载内核模块。内核会马上查找 / sbin 下的 “init” 程序并执行它。从这里开始 init 成为了 Linux 系统的父进程。init 读取的第一个文件是 / etc/inittab，通过它 init 会确定我们 Linux 操作系统的运行级别。它会从文件 / etc/fstab 里查找分区表信息然后做相应的挂载。然后 init 会启动 / etc/init.d 里指定的默认启动级别的所有服务 / 脚本。所有服务在这里通过 init 一个一个被初始化。在这个过程里，init 每次只启动一个服务，所有服务 / 守护进程都在后台执行并由 init 来管理。 关机过程差不多是相反的过程，首先 init 停止所有服务，最后阶段会卸载文件系统。 以上提到的启动过程有一些不足的地方：一是启动时间长。init 进程是串行启动，只有前一个进程启动完，才会启动下一个进程；二是启动脚本复杂。init 进程只是执行启动脚本，不管其他事情。脚本需要自己处理各种情况，这往往使得脚本变得很长。而用一种更好的方式来替代传统 init 的需求已经存在很长时间了。也产生了许多替代方案。其中比较著名的有 Upstart，Epoch，Muda 和 Systemd。而 Systemd 获得最多关注并被认为是目前最佳的方案。 Systemd 概述Systemd 就是为了解决这些问题而诞生的。它的设计目标是，为系统的启动和管理提供一套完整的解决方案。根据 Linux 惯例，字母 d 是守护进程（daemon）的缩写。Systemd 这个名字的含义，就是它要守护整个系统。 使用了 Systemd，就不需要再用 init 了。Systemd 取代了 initd，成为系统的第一个进程（PID 等于 1），其他进程都是它的子进程。 Systemctl 是一个 systemd 工具，主要负责控制 systemd 系统和服务管理器。systemctl 命令有两大类功能：一是控制 systemd 系统；二是管理系统上运行的服务。 Systemd 初体验Systemd 并不是一个命令，而是一组命令，涉及到系统管理的方方面面。 1234567891. 检查 systemd 安装的版本# systemctl --version2. 检查 systemd 和 systemctl 的二进制文件和库文件的安装位置# whereis systemd # whereis systemctl3. 检查 systemd 是否运行[systemd 是作为父进程（PID=1）运行的。在上面带（-e）参数的 ps 命令输出中，选择所有进程，（-a）选择除会话前导外的所有进程，并使用（-f）参数输出完整格式列表（即 -eaf）]# ps -eaf | grep [s]ystemd systemctlsystemctl 是 Systemd 的主命令，用于管理系统。 1234567891011121314151617181920# 重启系统$ sudo systemctl reboot# 关闭系统，切断电源$ sudo systemctl poweroff# CPU 停止工作$ sudo systemctl halt# 暂停系统$ sudo systemctl suspend# 让系统进入冬眠状态$ sudo systemctl hibernate# 让系统进入交互式休眠状态$ sudo systemctl hybrid-sleep# 启动进入救援状态（单用户状态）$ sudo systemctl rescue systemd-analyzesystemd-analyze 命令用于查看启动耗时。 12345678910# 查看启动耗时$ systemd-analyze # 查看每个服务的启动耗时$ systemd-analyze blame# 显示瀑布状的启动过程流$ systemd-analyze critical-chain# 显示指定服务的启动流$ systemd-analyze critical-chain atd.service hostnamectlhostnamectl 命令用于查看当前主机的信息。 12345# 显示当前主机的信息$ hostnamectl# 设置主机名$ sudo hostnamectl set-hostname rhel7 localectllocalectl 命令用于查看本地化设置。 123456# 查看本地化设置$ localectl# 设置本地化参数$ sudo localectl set-locale LANG=en_GB.utf8$ sudo localectl set-keymap en_GB timedatectltimedatectl 命令用于查看当前时区设置。 12345678910# 查看当前时区设置$ timedatectl# 显示所有可用的时区$ timedatectl list-timezones # 设置当前时区$ sudo timedatectl set-timezone America/New_York$ sudo timedatectl set-time YYYY-MM-DD$ sudo timedatectl set-time HH:MM:SS loginctlloginctl 命令用于查看当前登录的用户。 12345678# 列出当前 session$ loginctl list-sessions# 列出当前登录用户$ loginctl list-users# 列出显示指定用户的信息$ loginctl show-user ruanyf Unit含义Systemd 可以管理所有系统资源。不同的资源统称为 Unit（单位）。Unit 一共分成 12 种。 123456789101112Service unit：系统服务Target unit：多个 Unit 构成的一个组Device Unit：硬件设备Mount Unit：文件系统的挂载点Automount Unit：自动挂载点Path Unit：文件或路径Scope Unit：不是由 Systemd 启动的外部进程Slice Unit：进程组Snapshot Unit：Systemd 快照，可以切回某个快照Socket Unit：进程间通信的 socketSwap Unit：swap 文件Timer Unit：定时器 systemctl list-units 命令可以查看当前系统的所有 Unit。 1234567891011121314# 列出正在运行的 Unit$ systemctl list-units# 列出所有 Unit，包括没有找到配置文件的或者启动失败的$ systemctl list-units --all# 列出所有没有运行的 Unit$ systemctl list-units --all --state=inactive# 列出所有加载失败的 Unit$ systemctl list-units --failed# 列出所有正在运行的、类型为 service 的 Unit$ systemctl list-units --type=service Unit 的状态systemctl status 命令用于查看系统状态和单个 Unit 的状态。 12345678# 显示系统状态$ systemctl status# 显示单个 Unit 的状态$ sysystemctl status bluetooth.service# 显示远程主机的某个 Unit 的状态$ systemctl -H root@rhel7.example.com status httpd.service 除了 status 命令，systemctl 还提供了三个查询状态的简单方法，主要供脚本内部的判断语句使用。 12345678# 显示某个 Unit 是否正在运行$ systemctl is-active application.service# 显示某个 Unit 是否处于启动失败状态$ systemctl is-failed application.service# 显示某个 Unit 服务是否建立了启动链接$ systemctl is-enabled application.service Unit 管理对于用户来说，最常用的是下面这些命令，用于启动和停止 Unit（主要是 service）。 1234567891011121314151617181920212223242526# 立即启动一个服务$ sudo systemctl start apache.service# 立即停止一个服务$ sudo systemctl stop apache.service# 重启一个服务$ sudo systemctl restart apache.service# 杀死一个服务的所有子进程$ sudo systemctl kill apache.service# 重新加载一个服务的配置文件$ sudo systemctl reload apache.service# 重载所有修改过的配置文件$ sudo systemctl daemon-reload# 显示某个 Unit 的所有底层参数$ systemctl show httpd.service# 显示某个 Unit 的指定属性的值$ systemctl show -p CPUShares httpd.service# 设置某个 Unit 的指定属性$ sudo systemctl set-property httpd.service CPUShares=500 依赖关系Unit 之间存在依赖关系：A 依赖于 B，就意味着 Systemd 在启动 A 的时候，同时会去启动 B。 12345# systemctl list-dependencies 命令列出一个 Unit 的所有依赖。$ systemctl list-dependencies nginx.service# 上面命令的输出结果之中，有些依赖是 Target 类型（详见下文），默认不会展开显示。如果要展开 Target，就需要使用 --all 参数。$ systemctl list-dependencies --all nginx.service Unit 的配置文件概述每一个 Unit 都有一个配置文件，告诉 Systemd 怎么启动这个 Unit。 Systemd 默认从目录 / etc/systemd/system / 读取配置文件。但是，里面存放的大部分文件都是符号链接，指向目录 / usr/lib/systemd/system/，真正的配置文件存放在那个目录。 systemctl enable 命令用于在上面两个目录之间，建立符号链接关系。 123$ sudo systemctl enable clamd@scan.service# 等同于$ sudo ln -s '/usr/lib/systemd/system/clamd@scan.service' '/etc/systemd/system/multi-user.target.wants/clamd@scan.service' 如果配置文件里面设置了开机启动，systemctl enable 命令相当于激活开机启动。 与之对应的，systemctl disable 命令用于在两个目录之间，撤销符号链接关系，相当于撤销开机启动。 1$ sudo systemctl disable clamd@scan.service 配置文件的后缀名，就是该 Unit 的种类，比如 sshd.socket。如果省略，Systemd 默认后缀名为. service，所以 sshd 会被理解成 sshd.service。 12345678// 在开机时启用一个服务$ sudo systemctl enable firewalld.service// 在开机时禁用一个服务$ sudo systemctl disable firewalld.service// 查看服务是否开机启动$ sudo systemctl is-enabled firewalld.service// 查看开机启动的服务列表$ sudo systemctl list-unit-files|grep enabled 配置文件的状态systemctl list-unit-files 命令用于列出所有配置文件。 12345678# 列出所有配置文件$ systemctl list-unit-files# 列出指定类型的配置文件$ systemctl list-unit-files --type=service# 列出已建立启动链接的配置文件$ sudo systemctl list-unit-files|grep enabled 这个列表显示每个配置文件的状态，一共有四种。 1234enabled：已建立启动链接disabled：没建立启动链接static：该配置文件没有 [Install] 部分（无法执行），只能作为其他配置文件的依赖masked：该配置文件被禁止建立启动链接 一旦修改配置文件，就要让 SystemD 重新加载配置文件，然后重新启动，否则修改不会生效。 12$ sudo systemctl daemon-reload$ sudo systemctl restart httpd.service 配置文件的格式配置文件就是普通的文本文件，可以用文本编辑器打开。 systemctl cat 命令可以查看配置文件的内容。 [Unit]区块通常是配置文件的第一个区块，用来定义 Unit 的元数据，以及配置与其他 Unit 的关系。它的主要字段如下。 12345678910Description：简短描述Documentation：文档地址Requires：当前 Unit 依赖的其他 Unit，如果它们没有运行，当前 Unit 会启动失败Wants：与当前 Unit 配合的其他 Unit，如果它们没有运行，当前 Unit 不会启动失败BindsTo：与 Requires 类似，它指定的 Unit 如果退出，会导致当前 Unit 停止运行Before：如果该字段指定的 Unit 也要启动，那么必须在当前 Unit 之后启动After：如果该字段指定的 Unit 也要启动，那么必须在当前 Unit 之前启动Conflicts：这里指定的 Unit 不能与当前 Unit 同时运行Condition...：当前 Unit 运行必须满足的条件，否则不会运行Assert...：当前 Unit 运行必须满足的条件，否则会报启动失败 [Install]通常是配置文件的最后一个区块，用来定义如何启动，以及是否开机启动。它的主要字段如下。 1234WantedBy：它的值是一个或多个 Target，当前 Unit 激活时（enable）符号链接会放入 / etc/systemd/system 目录下面以 Target 名 + .wants 后缀构成的子目录中[Target 的含义是服务组，表示一组服务。WantedBy=multi-user.target 指的是，sshd 所在的 Target 是 multi-user.target。]RequiredBy：它的值是一个或多个 Target，当前 Unit 激活时，符号链接会放入 / etc/systemd/system 目录下面以 Target 名 + .required 后缀构成的子目录中Alias：当前 Unit 可用于启动的别名Also：当前 Unit 激活（enable）时，会被同时激活的其他 Unit [Service]区块用来 Service 的配置，只有 Service 类型的 Unit 才有这个区块。它的主要字段如下。 1234567891011121314151617Type：定义启动时的进程行为。它有以下几种值。Type=simple：默认值，执行 ExecStart 指定的命令，启动主进程Type=forking：以 fork 方式从父进程创建子进程，创建后父进程会立即退出Type=oneshot：一次性进程，Systemd 会等当前服务退出，再继续往下执行Type=dbus：当前服务通过 D-Bus 启动Type=notify：当前服务启动完毕，会通知 Systemd，再继续往下执行Type=idle：若有其他任务执行完毕，当前服务才会运行ExecStart：启动当前服务的命令ExecStartPre：启动当前服务之前执行的命令ExecStartPost：启动当前服务之后执行的命令ExecReload：重启当前服务时执行的命令ExecStop：停止当前服务时执行的命令ExecStopPost：停止当其服务之后执行的命令RestartSec：自动重启当前服务间隔的秒数Restart：定义何种情况 Systemd 会自动重启当前服务，可能的值包括 always（总是重启）、on-success、on-failure、on-abnormal、on-abort、on-watchdogTimeoutSec：定义 Systemd 停止当前服务之前等待的秒数Environment：指定环境变量 Systemd 实战关闭防火墙 firewallCentos7.x 中取消了 iptables, 用 firewall 取而代之。要关闭防火墙并禁止开机启动服务使用下面的命令: 12$ systemctl stop firewalld.service$ systemctl disable firewalld.service CentOS7 利用 systemctl 添加自定义系统服务 实例：服务用于开机运行 tomcat 项目: 123456789101112131415161718#vim /usr/lib/systemd/system/tomcat.service [Unit]Description=java tomcat projectAfter=tomcat.service [Service]Type=forkingUser=usersGroup=usersPIDFile=/usr/local/tomcat/tomcat.pidExecStart=/usr/local/tomcat/bin/startup.shExecReload=ExecStop=/usr/local/tomcat/bin/shutdown.shPrivateTmp=true [Install]WantedBy=multi-user.target 添加可执行权限： 1$ chmod 754 /usr/lib/systemd/system/tomcat.service 设置为开机自启动： 1$ systemctl enable tomcat.service 参考博文[1]. Systemd 入门教程：命令篇[2]. Systemd 入门教程：实战篇[3]. systemctl 命令完全指南 Linux 工匠精神系列 Linux 工匠精神（一）：Systemd 命令完全指南 Linux 工匠精神（二）：理解 Linux 的处理器负载均值]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Systemd</tag>
        <tag>systemctl</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论编码重要性（一）：你所不了解的字符编码]]></title>
    <url>%2Farchives%2F3b38de0f.html</url>
    <content type="text"><![CDATA[前言在我的工作中，常常会遇到形形色色的字符编码，对于各种编码技术本人了解的也不是很多。本篇是我了解编码系列的开篇，主要内容讲述字符编码的基本概念，然后介绍一下常见的字符编码，最后说明一下 Java 中如何编解码? 什么是字符编码？字符编码也称字集码，是把字符集中的字符编码为指定集合中某一对象（例如：比特模式、自然数序列、8 位组或者电脉冲），以便文本在计算机中存储和通过通信网络的传递。 编码及编码格式编码是用预先规定的方法将文字、数字或其它对象编成数码，或将信息、数据转换成规定的电脉冲信号。为保证编码的正确性，编码要规范化、标准化，即需有标准的编码格式。常见的编码格式有 ASCII、ISO-8859-1、GB2312、GBK、GB18030、UTF-8、UTF-16 等。 常见的字符编码ASCII/EASCIIASCII（American Standard Code for Information Interchange，美国标准信息交换码）是基于拉丁字母的一套电脑编码系统，主要用于显示现代英语和其他西欧语言，是现今最通用的单字节编码系统。 EASCII(Extended ASCII，延伸美国标准信息交换码)是将 ASCII 码由 7 位扩充为 8 位（增加了 128 个）而成。EASCII 的内码是由 0 到 255 共有 256 个字符组成。EASCII 码比 ASCII 码扩充出来的符号包括表格符号、计算符号、希腊字母和特殊的拉丁符号。 ASCII 码使用指定的 7 位或 8 位二进制数组合来表示 128 或 256 种可能的字符。标准 ASCII 码也叫基础 ASCII 码，使用 7 位二进制数（剩下的 1 位二进制为 0）来表示所有的大写和小写字母，数字 0 到 9、标点符号，以及在美式英语中使用的特殊控制字符。32～126(共 95 个)是字符(32 是空格），其中 48～57 为 0 到 9 十个阿拉伯数字，65～90 为 26 个大写英文字母，97～122 号为 26 个小写英文字母，其余为一些标点符号、运算符号等。 ISO-8859-1ISO-8859（拉丁码表，欧洲码表）是国际标准化组织（ISO）及国际电工委员会（IEC）联合制定的一系列 8 位字符集的标准。 ISO-8859-1 编码是单字节编码，向下兼容 ASCII，其编码范围是 0x00-0xFF，0x00-0x7F 之间完全和 ASCII 一致，0x80-0x9F 之间是控制字符，0xA0-0xFF 之间是文字符号。 GB2312/GBK/GB18030GB2312GB2312《信息交换用汉字编码字符集》是由中国国家标准总局 1980 年发布，GB 是 “国标” 二字的汉语拼音缩写，GB2312 编码适用于汉字处理、汉字通信等系统之间的信息交换，基本集共收入汉字 6763 个（从 B0-F7 是汉字区）和非汉字图形字符 682 个（其中从 A1-A9 是符号区）。整个字符集分成 94 个区(A1-FE)，每区有 94 个位，总的编码范围是 A1-F7。每个区位上只有一个字符，因此可用所在的区和位来对汉字进行编码，称为区位码。 GB2312 简体中文编码表，GB2312 只是编码表，在计算机中通常都是用 “EUC-CN” 表示法，即在每个区位加上 0xA0 来表示。区和位分别占用一个字节。 举例来说，“啊”字是 GB2312 之中的第一个汉字，它的区位码就是 1601。字节编码，通常采用 EUC 储存方法，以便兼容于 ASCII。每个汉字及符号以两个字节来表示。第一个字节称为 “高位字节”，第二个字节称为“低位字节”。“高位字节” 使用了 0xA1-0xF7(把 01-87 区的区号加上 0xA0)，“低位字节”使用了 0xA1-0xFE(把 01-94 加上 0xA0)。例如 “啊” 字在大多数程序中，会以 0xB0A1 储存（与区位码对比：0xB0=0xA0+16,0xA1=0xA0+1）。 GBKGBK 全称《汉字内码扩展规范》（GBK 即 “国标”、“扩展” 汉语拼音的第一个字母）。GBK 编码，是在 GB2312-80 标准基础上的内码扩展规范，使用了双字节编码方案。 GBK 亦采用双字节表示，总体编码范围为 8140-FEFE，首字节在 81-FE 之间，尾字节在 40-FE 之间，剔除 xx7F 一条线。总计 23940 个码位，共收入 21886 个汉字和图形符号，其中汉字（包括部首和构件）21003 个，图形符号 883 个。 GB18030GB18030 编码采用单字节、双字节、四字节分段编码方案，具体码位见下文。GB18030 向下兼容 GBK 和 GB2312 编码。 GB18030-2005 收录了 70244 个汉字 一图弄懂 ASCII、GB2312、GBK、GB18030 编码 UTF-8/UTF-16UnicodeUnicode（统一码、万国码、单一码），Unicode 是为了解决传统的字符编码方案的局限而产生的，它为每种语言中的每个字符设定了统一并且唯一的二进制编码，以满足跨语言、跨平台进行文本转换、处理的要求。Unicode 通常用两个字节表示一个字符，原有的英文编码从单字节变成双字节，只需要把高字节全部填为 0 就可以。 Unicode 是国际组织制定的可以容纳世界上所有文字和符号的字符编码方案。目前的 Unicode 字符分为 17 组编排，0x0000 至 0x10FFFF，每组称为平面（Plane），而每平面拥有 65536 个码位，共 1114112 个。然而目前只用了少数平面。UTF-8、UTF-16、UTF-32 都是将数字转换到程序数据的编码方案。 最初的 unicode 编码是固定长度的，16 位，也就是 2 两个字节代表一个字符，这样一共可以表示 65536 个字符(即 0 号平面，基本多文种平面)。显然，这样要表示各种语言中所有的字符是远远不够的。Unicode4.0 规范考虑到了这种情况，定义了一组附加字符编码，附加字符编码采用 2 个 16 位来表示，这样最多可以定义 1048576 个附加字符，目前 unicode4.0 只定义了 45960 个附加字符。 Unicode 编码方案之前提到，Unicode 没有规定字符对应的二进制码如何存储。以汉字 “汉” 为例，它的 Unicode 码点是 0x6c49，对应的二进制数是 110110001001001，二进制数有 15 位，这也就说明了它至少需要 2 个字节来表示。可以想象，在 Unicode 字典中往后的字符可能就需要 3 个字节或者 4 个字节，甚至更多字节来表示了。这就导致了一些问题，计算机怎么知道你这个 2 个字节表示的是一个字符，而不是分别表示两个字符呢？这里我们可能会想到，那就取个最大的，假如 Unicode 中最大的字符用 4 字节就可以表示了，那么我们就将所有的字符都用 4 个字节来表示，不够的就往前面补 0。这样确实可以解决编码问题，但是却造成了空间的极大浪费，如果是一个英文文档，那文件大小就大出了 3 倍，这显然是无法接受的。于是，为了较好的解决 Unicode 的编码问题，UTF-8 和 UTF-16 两种当前比较流行的编码方式诞生了。 UTF-8UTF-8 是一种针对 Unicode 的可变长度字符编码，是目前互联网上使用最广泛的一种 Unicode 编码方式，它的最大特点就是可变长。它可以使用 1-4 个字节表示一个字符，根据字符的不同变换长度。编码规则如下： 对于单个字节的字符，第一位设为 0，后面的 7 位对应这个字符的 Unicode 码点。因此，对于英文中的 0-127 号字符，与 ASCII 码完全相同。这意味着 ASCII 码那个年代的文档用 UTF-8 编码打开完全没有问题。 对于需要使用 N 个字节来表示的字符（N&gt;1），第一个字节的前 N 位都设为 1，第 N+1 位设为 0，剩余的 N-1 个字节的前两位都设位 10，剩下的二进制位则使用这个字符的 Unicode 码点来填充。编码规则如下： Unicode 十六进制码点范围 UTF-8 二进制 0000 0000 - 0000 007F 0xxxxxxx 0000 0080 - 0000 07FF 110xxxxx 10xxxxxx 0000 0800 - 0000 FFFF 1110xxxx 10xxxxxx 10xxxxxx 0001 0000 - 0010 FFFF 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx 下面以汉字 “汉” 为利，具体说明如何进行 UTF-8 编码和解码。 “汉”的 Unicode 码点是 0x6c49（110 1100 0100 1001），通过上面的对照表可以发现，0x0000 6c49 位于第三行的范围，那么得出其格式为 1110xxxx 10xxxxxx 10xxxxxx。接着，从 “汉” 的二进制数最后一位开始，从后向前依次填充对应格式中的 x，多出的 x 用 0 补上。这样，就得到了 “汉” 的 UTF-8 编码为 11100110 10110001 10001001，转换成十六进制就是 0xE6 0xB7 0x89。解码的过程也十分简单：如果一个字节的第一位是 0，则说明这个字节对应一个字符；如果一个字节的第一位 1，那么连续有多少个 1，就表示该字符占用多少个字节。 UTF-16UTF-16 编码介于 UTF-32 与 UTF-8 之间，同时结合了定长和变长两种编码方法的特点。它的编码规则很简单：基本平面的字符占用 2 个字节，辅助平面的字符占用 4 个字节。也就是说，UTF-16 的编码长度要么是 2 个字节（U+0000 到 U+FFFF），要么是 4 个字节（U+010000 到 U+10FFFF）。 那么问题来了，当我们遇到两个字节时，到底是把这两个字节当作一个字符还是与后面的两个字节一起当作一个字符呢？这里有一个很巧妙的地方，在基本平面内，从 U+D800 到 U+DFFF 是一个空段，即这些码点不对应任何字符。因此，这个空段可以用来映射辅助平面的字符。辅助平面的字符位共有 2^20 个，因此表示这些字符至少需要 20 个二进制位。UTF-16 将这 20 个二进制位分成两半，前 10 位映射在 U+D800 到 U+DBFF，称为高位（H），后 10 位映射在 U+DC00 到 U+DFFF，称为低位（L）。这意味着，一个辅助平面的字符，被拆成两个基本平面的字符表示。因此，当我们遇到两个字节，发现它的码点在 U+D800 到 U+DBFF 之间，就可以断定，紧跟在后面的两个字节的码点，应该在 U+DC00 到 U+DFFF 之间，这四个字节必须放在一起解读。 接下来，以汉字 “𠮷” 为例，说明 UTF-16 编码方式是如何工作的。 汉字 “𠮷” 的 Unicode 码点为 0x20BB7，该码点显然超出了基本平面的范围（0x0000-0xFFFF），因此需要使用四个字节表示。首先用 0x20BB7-0x10000 计算出超出的部分，然后将其用 20 个二进制位表示（不足前面补 0），结果为 00010000101110110111。接着，将前 10 位映射到 U+D800 到 U+DBFF 之间，后 10 位映射到 U+DC00 到 U+DFFF 即可。U+D800 对应的二进制数为 1101100000000000，直接填充后面的 10 个二进制位即可，得到 1101100001000010，转成 16 进制数则为 0xD842。同理可得，低位为 0xDFB7。因此得出汉字 “𠮷” 的 UTF-16 编码为 0xD8420xDFB7。 Java 中如何编解码? 下面我们以 “I am 君山” 这个字符串为例介绍 Java 中如何把它以 ISO-8859-1、GB2312、GBK、UTF-16、UTF-8 编码格式进行编码的。 123456String name = "I am 君山";byte[] iso8859 = name.getBytes("ISO-8859-1");byte[] gb2312 = name.getBytes("GB2312");byte[] gbk = name.getBytes("GBK");byte[] utf16 = name.getBytes("UTF-16");byte[] utf8 = name.getBytes("UTF-8"); ISO-8859-1 编码 ISO-8859-1 是单字节编码，中文 “君山” 被转化成值是 3f 的 byte。3f 也就是 “？” 字符，所以经常会出现中文变成 “？” 很可能就是错误的使用了 ISO-8859-1 这个编码导致的。中文字符经过 ISO-8859-1 编码会丢失信息，通常我们称之为“黑洞”，它会把不认识的字符吸收掉。 GB2312 编码 GB2312 字符集有一个 char 到 byte 的码表，不同的字符编码就是查这个码表找到与每个字符的对应的字节，然后拼装成 byte 数组。 GBK 编码 UTF-16 编码 用 UTF-16 编码将 char 数组放大了一倍，单字节范围内的字符，在高位补 0 变成两个字节，中文字符也变成两个字节。从 UTF-16 编码规则来看，仅仅将字符的高位和地位进行拆分变成两个字节。 UTF-8 编码 UTF-16 虽然编码效率很高，但是对单字节范围内字符也放大了一倍，这无形也浪费了存储空间，另外 UTF-16 采用顺序编码，不能对单个字符的编码值进行校验，如果中间的一个字符码值损坏，后面的所有码值都将受影响。而 UTF-8 这些问题都不存在，UTF-8 对单字节范围内字符仍然用一个字节表示，对汉字采用三个字节表示。UTF-8 编码与 GBK 和 GB2312 不同，不用查码表，所以在编码效率上 UTF-8 的效率会更好。 小知识点[1].\uxxxx: 其中 xxxx 表示一个 16 进制数字，这种格式是 unicode 码的写法。[2].0xf: 表示十进制数 15，在 java 中以 0x 开头的数表示十六进制数(如 0x1,0xa)。[3].03: 表示八进制数 3，在 java 中以 0 开头的数表示八进制数(如 012,03)。 参考博文[1]. GB2312 简体中文编码表[2]. Unicode 编码原理[3]. 彻底弄懂 Unicode 编码[4]. 深入分析 Java 中的中文编码问题 论编码重要性系列 论编码重要性（一）：你所不了解的字符编码 论编码重要性（二）：你所不了解的编码解码技术]]></content>
      <categories>
        <category>编码</category>
      </categories>
      <tags>
        <tag>编码</tag>
        <tag>字符编码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[跟上 Java7：你忽略了的新特性]]></title>
    <url>%2Farchives%2Fa61692cf.html</url>
    <content type="text"><![CDATA[前言Java7，代号「海豚(Dolphin)」，是 Java 历史上一次非常重大的版本更新，同时也是我入门学习 Java 所用的版本。本篇主要介绍几个很实用的 Java7 特性，文中若有用词不当或专业术语不准的现象，望见谅！ Java7 新特性： 1．二进制形式的字面值表示2．在数值类型的字面值中使用下划线分隔符联接3．创建泛型实例时自动类型推断4．switch-case 语句支持字符串类型5．新增 try-with-resources 语句6．单个 catch 子句同时捕获多种异常类型7．引入 java.util.Objects 工具类 二进制形式的字面值表示由于继承 C 语言，Java 代码在传统上迫使程序员只能使用十进制，八进制或十六进制来表示数 (numbers)。Java SE 7 中，整数类型(byte、short、int 以及 long) 也可以使用二进制数系来表示。要指定一个二进制字面量，可以给二进制数字添加前缀 0b 或者 0B。 12345678910111213// 一个 8 位的'byte'值:byte aByte = (byte) 0b00100001;// 一个 16 位的'short'值:short aShort = (short) 0b1010000101000101;// 几个 32 位的'int'值:[B 可以是大写或者小写]int anInt1 = 0b10100001010001011010000101000101;int anInt2 = 0b101;int anInt3 = 0B101;// 一个 64 位的'long'值. 注意 "L" 后缀:long aLong = 0b1010000101000101101000010100010110100001010001011010000101000101L; 支持的数字字面量表示： 十进制：默认的八进制：整数之前加数字 0 来表示十六进制：整数之前加 “0x” 或“0X”二进制（新加的）：整数之前加 “0b” 或“0B” 在数值类型的字面值中使用下划线分隔符联接如果 Java 源代码中有一个很长的数值字面量，开发人员在阅读这段代码时需要很费力地去分辨数字的位数，以知道其所代表的数值大小。在现实生活中，当遇到很长的数字的时候，我们采取的是分段分隔的方式。比如数字 500000，我们通常会写成 500,000，即每三位数字用逗号分隔。利用这种方式就可以很快知道数值的大小。这种做法的理念被加入到了 Java7 中，不过用的不是逗号，而是下划线“_”。 123456// 输出 56.34System.out.println(5_6.3_4);// 输出 8931System.out.println(89_3___1);// 输出 1500000System.out.println(1_500_000); 注意：下划线只能出现在数字中间，前后必须是数字。所以“_100”、“0b_101“是不合法的，无法通过编译。 创建泛型实例时自动类型推断只要编译器从上下文中能够推断出类型参数，你就可以使用一个空的类型参数集合 (&lt;&gt;) 代替调用一个泛型类的构造器所需要的类型参数。这对尖括号通常叫做 diamond。 12345// 举个例子, 考虑下面的变量声明:Map&lt;String, List&lt;String&gt;&gt; myMap = new HashMap&lt;String, List&lt;String&gt;&gt;();// 在 Java SE 7 中, 你可以使用一个空的类型参数集合 (&lt;&gt;) 代替构造器的参数化类型:Map&lt;String, List&lt;String&gt;&gt; myMap = new HashMap&lt;&gt;(); 注意：想要在泛型类初始化期间利用自动类型推断，你必须要指定 diamond。这个 &lt;&gt; 被叫做 diamond（钻石）运算符，Java 7 后这个运算符从引用的声明中推断类型。 switch-case 语句支持字符串类型switch 语句可以使用原始类型或枚举类型。Java 引入了另一种类型，我们可以在 switch 语句中使用：字符串类型。 123456789101112131415161718192021222324252627282930String input = "Monday";String output;switch (input) &#123; case "Monday": output = "星期一"; break; case "Tuesday": output = "星期二"; break; case "Wednesday": output = "星期三"; break; case "Thursday": output = "星期四"; break; case "Friday": output = "星期五"; break; case "Saturday": output = "星期六"; break; case "Sunday": output = "星期日"; break; default: throw new IllegalArgumentException("无效的输入参数：" + input);&#125;// 输出: 星期一System.out.println(output); 新增 try-with-resources 语句Java 中某些资源是需要手动关闭的，如 InputStream，Writes，Sockets，Sqlclasses 等。这个新的语言特性允许 try 语句本身申请更多的资源，这些资源作用于 try 代码块，并自动关闭。 1234567891011121314151617181920212223// 传统的资源关闭方式[为了确保外部资源一定要被关闭, 通常关闭代码被写入 finally 代码块中, 当然我们还必须注意到关闭资源时可能抛出的异常]FileInputStream inputStream = null;try &#123; inputStream = new FileInputStream(new File("test")); System.out.println(inputStream.read());&#125; catch (IOException e) &#123; throw new RuntimeException(e.getMessage(), e);&#125; finally &#123; if (inputStream != null) &#123; try &#123; inputStream.close(); &#125; catch (IOException e) &#123; throw new RuntimeException(e.getMessage(), e); &#125; &#125;&#125;// try-with-resource 语法try (FileInputStream inputStreamForResource = new FileInputStream(new File("test"))) &#123; System.out.println(inputStreamForResource.read());&#125; catch (IOException e) &#123; throw new RuntimeException(e.getMessage(), e);&#125; 那什么是 try-with-resource 呢？简而言之，当一个外部资源的句柄对象（比如 FileInputStream 对象）实现了 AutoCloseable 接口，将外部资源的句柄对象的创建放在 try 关键字后面的括号中，当这个 try-catch 代码块执行完毕后，Java 会确保外部资源的 close 方法被调用。 单个 catch 子句同时捕获多种异常类型在 Java7 中，catch 代码块得到了升级，用以在单个 catch 块中处理多个异常。如果你要捕获多个异常并且它们包含相似的代码，使用这一特性将会减少代码重复度。 1234567891011121314151617// Java 7 之前的版本:try &#123; // 具体的处理代码&#125; catch (InstantiationException e) &#123; // 捕获异常的处理方式&#125; catch (IllegalAccessException e) &#123; // 捕获异常的处理方式&#125; catch (ClassNotFoundException e) &#123; // 捕获异常的处理方式&#125;// 在 Java 7 中，我们可以用一个 catch 块捕获所有这些异常:try &#123; // 具体的处理代码&#125; catch (InstantiationException | IllegalAccessException | ClassNotFoundException e) &#123; // 捕获异常的处理方式&#125; 引入 java.util.Objects 工具类引入 java.util.Objects 工具类，用于封装一些平时使用频度很高或容易出错的操作。 （1）Objects.equals(Object a, Object b)：有别于 Object.equals(Object a, Object b)，这个方法可以避免空指针异常。 123public static boolean equals(Object a, Object b) &#123; return (a == b) || (a != null &amp;&amp; a.equals(b));&#125; （2）Objects.deepEquals(Object a, Object b)：Object.equals(Object a, Object b) 用于比较两个对象的引用是否相同，而 deepEquals() 却扩展成了可以支持数组。 12345678public static boolean deepEquals(Object a, Object b) &#123; if (a == b) return true; else if (a == null || b == null) return false; else return Arrays.deepEquals0(a, b);&#125; （3）Objects.hashCode(Object o)：和 Object.hashCode(Object o) 类似，只是在对象为 null 时返回的散列值为 0 而已。 123public static int hashCode(Object o) &#123; return o != null ? o.hashCode() : 0;&#125; （4）Objects.hash(Object… values)：生成对象的散列值，包括数组。 123public static int hash(Object... values) &#123; return Arrays.hashCode(values);&#125; （5）Objects.toString(Object o)：归根结底，其内部最终调用了对象的 toString() 方法。只是额外多了空指针判断而已。 1234567public static String toString(Object o) &#123; return String.valueOf(o);&#125;public static String valueOf(Object obj) &#123; return (obj == null) ? "null" : obj.toString();&#125; （6）Objects.compare(T a, T b, Comparator&lt; ? super T&gt; c)：用于比较两个对象。 123public static &lt;T&gt; int compare(T a, T b, Comparator&lt;? super T&gt; c) &#123; return (a == b) ? 0 : c.compare(a, b);&#125; （7）Objects.requireNonNull(T obj)：在对象为空指针时，抛出特定 message 的空指针异常。 1234567891011121314151617public static &lt;T&gt; T requireNonNull(T obj) &#123; if (obj == null) throw new NullPointerException(); return obj;&#125;public static &lt;T&gt; T requireNonNull(T obj, String message) &#123; if (obj == null) throw new NullPointerException(message); return obj;&#125;public static &lt;T&gt; T requireNonNull(T obj, Supplier&lt;String&gt; messageSupplier) &#123; if (obj == null) throw new NullPointerException(messageSupplier.get()); return obj;&#125; （8）Objects.isNull(Object obj) / Objects.nonNull(Object obj)：这两个方法用于判断对象为 null 和对象不为 null。通常情况下，我们不会直接使用这两个方法，而是使用比较操作符 == 和 !=。这两个方法主要用在 jdk8 开始支持的流计算里面。 1234567891011121314151617public static &lt;T&gt; T requireNonNull(T obj) &#123; if (obj == null) throw new NullPointerException(); return obj;&#125;public static &lt;T&gt; T requireNonNull(T obj, String message) &#123; if (obj == null) throw new NullPointerException(message); return obj;&#125;public static &lt;T&gt; T requireNonNull(T obj, Supplier&lt;String&gt; messageSupplier) &#123; if (obj == null) throw new NullPointerException(messageSupplier.get()); return obj;&#125; 参考博文[1]. A look at Java 7’s new features]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java7</tag>
        <tag>海豚</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Farchives%2F4a17b156.html</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>Hello</category>
      </categories>
      <tags>
        <tag>Hello</tag>
        <tag>World</tag>
      </tags>
  </entry>
</search>
